{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch_geometric\n",
    "\n",
    "from  utils import run_experiment\n",
    "from models import GCN, iterativeGCN\n",
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Cora():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "Training sample:  140\n",
      "Validation sample:  500\n",
      "Test sample:  1000\n",
      "===========================================================================================================\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Number of training nodes: 140\n",
      "Training node label rate: 0.05\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print(\"Training sample: \", data.train_mask.sum().item())\n",
    "print(\"Validation sample: \", data.val_mask.sum().item())\n",
    "print(\"Test sample: \", data.test_mask.sum().item())\n",
    "print('===========================================================================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = np.arange(0.001, 0.01, 0.0005)\n",
    "SMOOTH_FAC = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_exp = 1\n",
    "for lr in LR:\n",
    "    for smooth_fac in SMOOTH_FAC:\n",
    "        model = iterativeGCN(input_dim=dataset.num_features,\n",
    "                                output_dim=dataset.num_classes,\n",
    "                                hidden_dim=32,\n",
    "                                num_train_iter=2,\n",
    "                                num_eval_iter=2,\n",
    "                                smooth_fac=smooth_fac,\n",
    "                                dropout=0.5)\n",
    "        loss_test, acc_test, training_time = run_experiment(model=model, \n",
    "                                                            data=data, \n",
    "                                                            lr=lr, \n",
    "                                                            weight_decay=2e-4,\n",
    "                                                            model_name=str(LR) + \",\" + str(smooth_fac),\n",
    "                                                            run=1,\n",
    "                                                            num_epochs=200)\n",
    "        print(\"Experiment \", curr_exp, \", training time: \", training_time)\n",
    "        print(\"Learning rate: \", lr, \", smoothing factor: \", smooth_fac, \" loss: \", loss_test, \", accuracy: \", acc_test)\n",
    "        curr_exp += 1\n",
    "        del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_layer_GCN = GCN(input_dim=dataset.num_features,\n",
    "                    output_dim=dataset.num_classes,\n",
    "                    hidden_dim=16,\n",
    "                    num_layers=2,\n",
    "                    dropout=0.5)\n",
    "run_experiment(model=two_layer_GCN,\n",
    "               data=data,\n",
    "               lr=0.01,\n",
    "               weight_decay=5e-4,\n",
    "               model_name=\"2l\",\n",
    "               run=1,\n",
    "               num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(two_layer_GCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (gc1): GCNConv(1433, 16)\n",
      "  (gc2): GCNConv(16, 7)\n",
      ")\n",
      "Epoch: 0001 loss_train: 1.9448 acc_train: 0.1357 loss_val: 1.9432 acc_val: 0.1240 time: 0.0165s\n",
      "Epoch: 0002 loss_train: 1.9374 acc_train: 0.3571 loss_val: 1.9364 acc_val: 0.2980 time: 0.0115s\n",
      "Epoch: 0003 loss_train: 1.9307 acc_train: 0.4500 loss_val: 1.9296 acc_val: 0.4920 time: 0.0106s\n",
      "Epoch: 0004 loss_train: 1.9133 acc_train: 0.6571 loss_val: 1.9238 acc_val: 0.4400 time: 0.0109s\n",
      "Epoch: 0005 loss_train: 1.9073 acc_train: 0.5286 loss_val: 1.9185 acc_val: 0.4360 time: 0.0105s\n",
      "Epoch: 0006 loss_train: 1.8958 acc_train: 0.5929 loss_val: 1.9130 acc_val: 0.4360 time: 0.0119s\n",
      "Epoch: 0007 loss_train: 1.8761 acc_train: 0.6000 loss_val: 1.9073 acc_val: 0.4280 time: 0.0119s\n",
      "Epoch: 0008 loss_train: 1.8616 acc_train: 0.6357 loss_val: 1.9009 acc_val: 0.4300 time: 0.0127s\n",
      "Epoch: 0009 loss_train: 1.8550 acc_train: 0.6071 loss_val: 1.8941 acc_val: 0.4340 time: 0.0126s\n",
      "Epoch: 0010 loss_train: 1.8395 acc_train: 0.6214 loss_val: 1.8864 acc_val: 0.4440 time: 0.0127s\n",
      "Epoch: 0011 loss_train: 1.8308 acc_train: 0.6786 loss_val: 1.8780 acc_val: 0.4880 time: 0.0117s\n",
      "Epoch: 0012 loss_train: 1.8164 acc_train: 0.6286 loss_val: 1.8694 acc_val: 0.5080 time: 0.0132s\n",
      "Epoch: 0013 loss_train: 1.7991 acc_train: 0.6857 loss_val: 1.8608 acc_val: 0.5280 time: 0.0126s\n",
      "Epoch: 0014 loss_train: 1.7832 acc_train: 0.7143 loss_val: 1.8523 acc_val: 0.5540 time: 0.0135s\n",
      "Epoch: 0015 loss_train: 1.7631 acc_train: 0.7857 loss_val: 1.8436 acc_val: 0.5780 time: 0.0148s\n",
      "Epoch: 0016 loss_train: 1.7499 acc_train: 0.7643 loss_val: 1.8348 acc_val: 0.5980 time: 0.0152s\n",
      "Epoch: 0017 loss_train: 1.7226 acc_train: 0.7643 loss_val: 1.8261 acc_val: 0.6040 time: 0.0589s\n",
      "Epoch: 0018 loss_train: 1.7100 acc_train: 0.8000 loss_val: 1.8168 acc_val: 0.6060 time: 0.0268s\n",
      "Epoch: 0019 loss_train: 1.6928 acc_train: 0.7786 loss_val: 1.8076 acc_val: 0.6120 time: 0.0124s\n",
      "Epoch: 0020 loss_train: 1.6793 acc_train: 0.7786 loss_val: 1.7981 acc_val: 0.6240 time: 0.0119s\n",
      "Epoch: 0021 loss_train: 1.6539 acc_train: 0.8071 loss_val: 1.7875 acc_val: 0.6380 time: 0.0098s\n",
      "Epoch: 0022 loss_train: 1.6348 acc_train: 0.7571 loss_val: 1.7767 acc_val: 0.6440 time: 0.0107s\n",
      "Epoch: 0023 loss_train: 1.6211 acc_train: 0.7929 loss_val: 1.7656 acc_val: 0.6520 time: 0.0098s\n",
      "Epoch: 0024 loss_train: 1.6078 acc_train: 0.8286 loss_val: 1.7547 acc_val: 0.6500 time: 0.0106s\n",
      "Epoch: 0025 loss_train: 1.5746 acc_train: 0.8143 loss_val: 1.7439 acc_val: 0.6540 time: 0.0117s\n",
      "Epoch: 0026 loss_train: 1.5605 acc_train: 0.8500 loss_val: 1.7333 acc_val: 0.6480 time: 0.0113s\n",
      "Epoch: 0027 loss_train: 1.5516 acc_train: 0.8143 loss_val: 1.7223 acc_val: 0.6440 time: 0.0106s\n",
      "Epoch: 0028 loss_train: 1.5229 acc_train: 0.8286 loss_val: 1.7108 acc_val: 0.6480 time: 0.0106s\n",
      "Epoch: 0029 loss_train: 1.4786 acc_train: 0.9071 loss_val: 1.6991 acc_val: 0.6460 time: 0.0107s\n",
      "Epoch: 0030 loss_train: 1.4825 acc_train: 0.8071 loss_val: 1.6871 acc_val: 0.6560 time: 0.0124s\n",
      "Epoch: 0031 loss_train: 1.4883 acc_train: 0.8000 loss_val: 1.6739 acc_val: 0.6720 time: 0.0142s\n",
      "Epoch: 0032 loss_train: 1.4484 acc_train: 0.8500 loss_val: 1.6599 acc_val: 0.6960 time: 0.0116s\n",
      "Epoch: 0033 loss_train: 1.4294 acc_train: 0.8357 loss_val: 1.6457 acc_val: 0.7140 time: 0.0094s\n",
      "Epoch: 0034 loss_train: 1.4184 acc_train: 0.8286 loss_val: 1.6312 acc_val: 0.7300 time: 0.0133s\n",
      "Epoch: 0035 loss_train: 1.3902 acc_train: 0.8214 loss_val: 1.6162 acc_val: 0.7360 time: 0.0142s\n",
      "Epoch: 0036 loss_train: 1.3553 acc_train: 0.9357 loss_val: 1.6018 acc_val: 0.7540 time: 0.0142s\n",
      "Epoch: 0037 loss_train: 1.3179 acc_train: 0.8929 loss_val: 1.5888 acc_val: 0.7500 time: 0.0147s\n",
      "Epoch: 0038 loss_train: 1.3415 acc_train: 0.8857 loss_val: 1.5771 acc_val: 0.7400 time: 0.0153s\n",
      "Epoch: 0039 loss_train: 1.2912 acc_train: 0.8786 loss_val: 1.5665 acc_val: 0.7380 time: 0.0144s\n",
      "Epoch: 0040 loss_train: 1.3052 acc_train: 0.8786 loss_val: 1.5568 acc_val: 0.7380 time: 0.0144s\n",
      "Epoch: 0041 loss_train: 1.2456 acc_train: 0.9000 loss_val: 1.5466 acc_val: 0.7300 time: 0.0156s\n",
      "Epoch: 0042 loss_train: 1.2394 acc_train: 0.9071 loss_val: 1.5359 acc_val: 0.7260 time: 0.0175s\n",
      "Epoch: 0043 loss_train: 1.2259 acc_train: 0.8929 loss_val: 1.5242 acc_val: 0.7300 time: 0.0150s\n",
      "Epoch: 0044 loss_train: 1.1969 acc_train: 0.8643 loss_val: 1.5119 acc_val: 0.7340 time: 0.0155s\n",
      "Epoch: 0045 loss_train: 1.1802 acc_train: 0.8643 loss_val: 1.4985 acc_val: 0.7440 time: 0.0162s\n",
      "Epoch: 0046 loss_train: 1.1392 acc_train: 0.9000 loss_val: 1.4843 acc_val: 0.7500 time: 0.0148s\n",
      "Epoch: 0047 loss_train: 1.1298 acc_train: 0.8786 loss_val: 1.4705 acc_val: 0.7540 time: 0.0151s\n",
      "Epoch: 0048 loss_train: 1.1100 acc_train: 0.9071 loss_val: 1.4562 acc_val: 0.7580 time: 0.0141s\n",
      "Epoch: 0049 loss_train: 1.1056 acc_train: 0.8857 loss_val: 1.4417 acc_val: 0.7660 time: 0.0141s\n",
      "Epoch: 0050 loss_train: 1.0747 acc_train: 0.9000 loss_val: 1.4279 acc_val: 0.7640 time: 0.0123s\n",
      "Epoch: 0051 loss_train: 1.0236 acc_train: 0.9357 loss_val: 1.4148 acc_val: 0.7640 time: 0.0125s\n",
      "Epoch: 0052 loss_train: 1.0109 acc_train: 0.9000 loss_val: 1.4024 acc_val: 0.7640 time: 0.0145s\n",
      "Epoch: 0053 loss_train: 1.0384 acc_train: 0.8857 loss_val: 1.3915 acc_val: 0.7660 time: 0.0150s\n",
      "Epoch: 0054 loss_train: 1.0126 acc_train: 0.8786 loss_val: 1.3812 acc_val: 0.7640 time: 0.0158s\n",
      "Epoch: 0055 loss_train: 0.9888 acc_train: 0.9071 loss_val: 1.3695 acc_val: 0.7640 time: 0.0132s\n",
      "Epoch: 0056 loss_train: 0.9521 acc_train: 0.9000 loss_val: 1.3573 acc_val: 0.7640 time: 0.0129s\n",
      "Epoch: 0057 loss_train: 0.9505 acc_train: 0.9071 loss_val: 1.3448 acc_val: 0.7660 time: 0.0111s\n",
      "Epoch: 0058 loss_train: 1.0130 acc_train: 0.8786 loss_val: 1.3335 acc_val: 0.7680 time: 0.0177s\n",
      "Epoch: 0059 loss_train: 0.8937 acc_train: 0.9286 loss_val: 1.3221 acc_val: 0.7700 time: 0.0142s\n",
      "Epoch: 0060 loss_train: 0.9297 acc_train: 0.9143 loss_val: 1.3114 acc_val: 0.7740 time: 0.0138s\n",
      "Epoch: 0061 loss_train: 0.9091 acc_train: 0.9000 loss_val: 1.2994 acc_val: 0.7800 time: 0.0133s\n",
      "Epoch: 0062 loss_train: 0.8658 acc_train: 0.9214 loss_val: 1.2893 acc_val: 0.7820 time: 0.0119s\n",
      "Epoch: 0063 loss_train: 0.8308 acc_train: 0.9643 loss_val: 1.2773 acc_val: 0.7920 time: 0.0094s\n",
      "Epoch: 0064 loss_train: 0.8302 acc_train: 0.9214 loss_val: 1.2638 acc_val: 0.7960 time: 0.0103s\n",
      "Epoch: 0065 loss_train: 0.8689 acc_train: 0.9214 loss_val: 1.2513 acc_val: 0.8020 time: 0.0106s\n",
      "Epoch: 0066 loss_train: 0.8163 acc_train: 0.9357 loss_val: 1.2404 acc_val: 0.8020 time: 0.0117s\n",
      "Epoch: 0067 loss_train: 0.8000 acc_train: 0.9286 loss_val: 1.2295 acc_val: 0.8000 time: 0.0109s\n",
      "Epoch: 0068 loss_train: 0.8099 acc_train: 0.9214 loss_val: 1.2209 acc_val: 0.7920 time: 0.0104s\n",
      "Epoch: 0069 loss_train: 0.7834 acc_train: 0.9500 loss_val: 1.2141 acc_val: 0.7940 time: 0.0113s\n",
      "Epoch: 0070 loss_train: 0.7380 acc_train: 0.9571 loss_val: 1.2091 acc_val: 0.7860 time: 0.0115s\n",
      "Epoch: 0071 loss_train: 0.7725 acc_train: 0.9429 loss_val: 1.2043 acc_val: 0.7820 time: 0.0108s\n",
      "Epoch: 0072 loss_train: 0.7710 acc_train: 0.9214 loss_val: 1.1986 acc_val: 0.7820 time: 0.0118s\n",
      "Epoch: 0073 loss_train: 0.7742 acc_train: 0.9214 loss_val: 1.1890 acc_val: 0.7840 time: 0.0106s\n",
      "Epoch: 0074 loss_train: 0.7333 acc_train: 0.9214 loss_val: 1.1786 acc_val: 0.7900 time: 0.0112s\n",
      "Epoch: 0075 loss_train: 0.7220 acc_train: 0.9571 loss_val: 1.1671 acc_val: 0.7980 time: 0.0104s\n",
      "Epoch: 0076 loss_train: 0.7410 acc_train: 0.9286 loss_val: 1.1566 acc_val: 0.7980 time: 0.0113s\n",
      "Epoch: 0077 loss_train: 0.7017 acc_train: 0.9214 loss_val: 1.1477 acc_val: 0.7980 time: 0.0113s\n",
      "Epoch: 0078 loss_train: 0.6905 acc_train: 0.9357 loss_val: 1.1391 acc_val: 0.7940 time: 0.0103s\n",
      "Epoch: 0079 loss_train: 0.6587 acc_train: 0.9286 loss_val: 1.1309 acc_val: 0.7940 time: 0.0140s\n",
      "Epoch: 0080 loss_train: 0.7036 acc_train: 0.9357 loss_val: 1.1268 acc_val: 0.7920 time: 0.0136s\n",
      "Epoch: 0081 loss_train: 0.6285 acc_train: 0.9571 loss_val: 1.1246 acc_val: 0.7860 time: 0.0132s\n",
      "Epoch: 0082 loss_train: 0.6771 acc_train: 0.9571 loss_val: 1.1226 acc_val: 0.7840 time: 0.0123s\n",
      "Epoch: 0083 loss_train: 0.6419 acc_train: 0.9571 loss_val: 1.1190 acc_val: 0.7820 time: 0.0135s\n",
      "Epoch: 0084 loss_train: 0.6384 acc_train: 0.9429 loss_val: 1.1132 acc_val: 0.7840 time: 0.0143s\n",
      "Epoch: 0085 loss_train: 0.6074 acc_train: 0.9357 loss_val: 1.1059 acc_val: 0.7840 time: 0.0136s\n",
      "Epoch: 0086 loss_train: 0.6314 acc_train: 0.9571 loss_val: 1.0990 acc_val: 0.7880 time: 0.0120s\n",
      "Epoch: 0087 loss_train: 0.6223 acc_train: 0.9643 loss_val: 1.0925 acc_val: 0.7900 time: 0.0128s\n",
      "Epoch: 0088 loss_train: 0.6070 acc_train: 0.9286 loss_val: 1.0836 acc_val: 0.7880 time: 0.0138s\n",
      "Epoch: 0089 loss_train: 0.5765 acc_train: 0.9571 loss_val: 1.0748 acc_val: 0.7880 time: 0.0139s\n",
      "Epoch: 0090 loss_train: 0.5906 acc_train: 0.9571 loss_val: 1.0662 acc_val: 0.7900 time: 0.0135s\n",
      "Epoch: 0091 loss_train: 0.5862 acc_train: 0.9714 loss_val: 1.0578 acc_val: 0.8020 time: 0.0116s\n",
      "Epoch: 0092 loss_train: 0.5732 acc_train: 0.9643 loss_val: 1.0502 acc_val: 0.7960 time: 0.0110s\n",
      "Epoch: 0093 loss_train: 0.5630 acc_train: 0.9643 loss_val: 1.0454 acc_val: 0.8080 time: 0.0119s\n",
      "Epoch: 0094 loss_train: 0.5710 acc_train: 0.9357 loss_val: 1.0405 acc_val: 0.7980 time: 0.0127s\n",
      "Epoch: 0095 loss_train: 0.5731 acc_train: 0.9500 loss_val: 1.0389 acc_val: 0.7860 time: 0.0140s\n",
      "Epoch: 0096 loss_train: 0.5720 acc_train: 0.9571 loss_val: 1.0399 acc_val: 0.7800 time: 0.0109s\n",
      "Epoch: 0097 loss_train: 0.5554 acc_train: 0.9429 loss_val: 1.0411 acc_val: 0.7800 time: 0.0105s\n",
      "Epoch: 0098 loss_train: 0.5384 acc_train: 0.9714 loss_val: 1.0401 acc_val: 0.7780 time: 0.0101s\n",
      "Epoch: 0099 loss_train: 0.6048 acc_train: 0.9286 loss_val: 1.0347 acc_val: 0.7780 time: 0.0097s\n",
      "Epoch: 0100 loss_train: 0.5557 acc_train: 0.9357 loss_val: 1.0257 acc_val: 0.7820 time: 0.0101s\n",
      "Epoch: 0101 loss_train: 0.5582 acc_train: 0.9571 loss_val: 1.0155 acc_val: 0.7900 time: 0.0104s\n",
      "Epoch: 0102 loss_train: 0.5101 acc_train: 0.9643 loss_val: 1.0062 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0103 loss_train: 0.5550 acc_train: 0.9500 loss_val: 1.0005 acc_val: 0.7960 time: 0.0100s\n",
      "Epoch: 0104 loss_train: 0.5715 acc_train: 0.9429 loss_val: 0.9996 acc_val: 0.7980 time: 0.0109s\n",
      "Epoch: 0105 loss_train: 0.5357 acc_train: 0.9143 loss_val: 1.0006 acc_val: 0.7960 time: 0.0119s\n",
      "Epoch: 0106 loss_train: 0.5269 acc_train: 0.9571 loss_val: 1.0008 acc_val: 0.7880 time: 0.0107s\n",
      "Epoch: 0107 loss_train: 0.4927 acc_train: 0.9643 loss_val: 1.0004 acc_val: 0.7840 time: 0.0116s\n",
      "Epoch: 0108 loss_train: 0.5228 acc_train: 0.9714 loss_val: 0.9964 acc_val: 0.7780 time: 0.0123s\n",
      "Epoch: 0109 loss_train: 0.4960 acc_train: 0.9643 loss_val: 0.9914 acc_val: 0.7820 time: 0.0114s\n",
      "Epoch: 0110 loss_train: 0.4851 acc_train: 0.9786 loss_val: 0.9866 acc_val: 0.7800 time: 0.0101s\n",
      "Epoch: 0111 loss_train: 0.5011 acc_train: 0.9643 loss_val: 0.9825 acc_val: 0.7840 time: 0.0121s\n",
      "Epoch: 0112 loss_train: 0.4970 acc_train: 0.9429 loss_val: 0.9797 acc_val: 0.7840 time: 0.0120s\n",
      "Epoch: 0113 loss_train: 0.4817 acc_train: 0.9643 loss_val: 0.9752 acc_val: 0.7860 time: 0.0131s\n",
      "Epoch: 0114 loss_train: 0.4766 acc_train: 0.9786 loss_val: 0.9711 acc_val: 0.7900 time: 0.0112s\n",
      "Epoch: 0115 loss_train: 0.4679 acc_train: 0.9857 loss_val: 0.9696 acc_val: 0.7920 time: 0.0111s\n",
      "Epoch: 0116 loss_train: 0.4902 acc_train: 0.9429 loss_val: 0.9680 acc_val: 0.7900 time: 0.0112s\n",
      "Epoch: 0117 loss_train: 0.5108 acc_train: 0.9429 loss_val: 0.9662 acc_val: 0.7860 time: 0.0097s\n",
      "Epoch: 0118 loss_train: 0.4756 acc_train: 0.9857 loss_val: 0.9634 acc_val: 0.7900 time: 0.0110s\n",
      "Epoch: 0119 loss_train: 0.4333 acc_train: 0.9500 loss_val: 0.9606 acc_val: 0.7840 time: 0.0106s\n",
      "Epoch: 0120 loss_train: 0.4717 acc_train: 0.9500 loss_val: 0.9582 acc_val: 0.7840 time: 0.0096s\n",
      "Epoch: 0121 loss_train: 0.4823 acc_train: 0.9786 loss_val: 0.9560 acc_val: 0.7840 time: 0.0100s\n",
      "Epoch: 0122 loss_train: 0.4474 acc_train: 0.9714 loss_val: 0.9541 acc_val: 0.7840 time: 0.0103s\n",
      "Epoch: 0123 loss_train: 0.4940 acc_train: 0.9571 loss_val: 0.9512 acc_val: 0.7820 time: 0.0122s\n",
      "Epoch: 0124 loss_train: 0.4559 acc_train: 0.9500 loss_val: 0.9477 acc_val: 0.7840 time: 0.0121s\n",
      "Epoch: 0125 loss_train: 0.4858 acc_train: 0.9571 loss_val: 0.9445 acc_val: 0.7880 time: 0.0090s\n",
      "Epoch: 0126 loss_train: 0.4542 acc_train: 0.9571 loss_val: 0.9402 acc_val: 0.7880 time: 0.0110s\n",
      "Epoch: 0127 loss_train: 0.4447 acc_train: 0.9571 loss_val: 0.9362 acc_val: 0.7900 time: 0.0106s\n",
      "Epoch: 0128 loss_train: 0.4365 acc_train: 0.9643 loss_val: 0.9320 acc_val: 0.7880 time: 0.0104s\n",
      "Epoch: 0129 loss_train: 0.4232 acc_train: 0.9571 loss_val: 0.9292 acc_val: 0.7920 time: 0.0110s\n",
      "Epoch: 0130 loss_train: 0.4187 acc_train: 0.9643 loss_val: 0.9265 acc_val: 0.7940 time: 0.0102s\n",
      "Epoch: 0131 loss_train: 0.4759 acc_train: 0.9429 loss_val: 0.9235 acc_val: 0.7900 time: 0.0105s\n",
      "Epoch: 0132 loss_train: 0.4001 acc_train: 0.9643 loss_val: 0.9207 acc_val: 0.7900 time: 0.0125s\n",
      "Epoch: 0133 loss_train: 0.4034 acc_train: 0.9786 loss_val: 0.9156 acc_val: 0.8000 time: 0.0122s\n",
      "Epoch: 0134 loss_train: 0.4467 acc_train: 0.9714 loss_val: 0.9117 acc_val: 0.8020 time: 0.0113s\n",
      "Epoch: 0135 loss_train: 0.4150 acc_train: 0.9643 loss_val: 0.9076 acc_val: 0.8040 time: 0.0110s\n",
      "Epoch: 0136 loss_train: 0.4146 acc_train: 0.9786 loss_val: 0.9059 acc_val: 0.8020 time: 0.0111s\n",
      "Epoch: 0137 loss_train: 0.3990 acc_train: 0.9500 loss_val: 0.9063 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0138 loss_train: 0.4269 acc_train: 0.9500 loss_val: 0.9068 acc_val: 0.8000 time: 0.0108s\n",
      "Epoch: 0139 loss_train: 0.3829 acc_train: 0.9857 loss_val: 0.9070 acc_val: 0.7960 time: 0.0102s\n",
      "Epoch: 0140 loss_train: 0.3986 acc_train: 0.9643 loss_val: 0.9062 acc_val: 0.7940 time: 0.0097s\n",
      "Epoch: 0141 loss_train: 0.3717 acc_train: 0.9857 loss_val: 0.9057 acc_val: 0.7920 time: 0.0108s\n",
      "Epoch: 0142 loss_train: 0.3966 acc_train: 0.9786 loss_val: 0.9046 acc_val: 0.7900 time: 0.0114s\n",
      "Epoch: 0143 loss_train: 0.4217 acc_train: 0.9643 loss_val: 0.9037 acc_val: 0.7900 time: 0.0098s\n",
      "Epoch: 0144 loss_train: 0.3651 acc_train: 0.9786 loss_val: 0.8993 acc_val: 0.7920 time: 0.0106s\n",
      "Epoch: 0145 loss_train: 0.3829 acc_train: 0.9857 loss_val: 0.8942 acc_val: 0.7920 time: 0.0095s\n",
      "Epoch: 0146 loss_train: 0.4093 acc_train: 0.9429 loss_val: 0.8895 acc_val: 0.7900 time: 0.0108s\n",
      "Epoch: 0147 loss_train: 0.4124 acc_train: 0.9714 loss_val: 0.8882 acc_val: 0.7940 time: 0.0100s\n",
      "Epoch: 0148 loss_train: 0.3881 acc_train: 0.9571 loss_val: 0.8881 acc_val: 0.7960 time: 0.0096s\n",
      "Epoch: 0149 loss_train: 0.3904 acc_train: 0.9571 loss_val: 0.8885 acc_val: 0.7940 time: 0.0115s\n",
      "Epoch: 0150 loss_train: 0.3578 acc_train: 0.9786 loss_val: 0.8887 acc_val: 0.7920 time: 0.0112s\n",
      "Epoch: 0151 loss_train: 0.3921 acc_train: 0.9643 loss_val: 0.8912 acc_val: 0.7860 time: 0.0124s\n",
      "Epoch: 0152 loss_train: 0.4434 acc_train: 0.9500 loss_val: 0.8923 acc_val: 0.7840 time: 0.0121s\n",
      "Epoch: 0153 loss_train: 0.3880 acc_train: 0.9786 loss_val: 0.8919 acc_val: 0.7860 time: 0.0100s\n",
      "Epoch: 0154 loss_train: 0.3807 acc_train: 0.9500 loss_val: 0.8920 acc_val: 0.7800 time: 0.0111s\n",
      "Epoch: 0155 loss_train: 0.4002 acc_train: 0.9643 loss_val: 0.8908 acc_val: 0.7780 time: 0.0111s\n",
      "Epoch: 0156 loss_train: 0.3809 acc_train: 0.9714 loss_val: 0.8863 acc_val: 0.7780 time: 0.0101s\n",
      "Epoch: 0157 loss_train: 0.3795 acc_train: 0.9857 loss_val: 0.8798 acc_val: 0.7980 time: 0.0095s\n",
      "Epoch: 0158 loss_train: 0.3720 acc_train: 0.9643 loss_val: 0.8739 acc_val: 0.8020 time: 0.0109s\n",
      "Epoch: 0159 loss_train: 0.3553 acc_train: 0.9571 loss_val: 0.8702 acc_val: 0.7980 time: 0.0116s\n",
      "Epoch: 0160 loss_train: 0.3570 acc_train: 0.9857 loss_val: 0.8676 acc_val: 0.8000 time: 0.0126s\n",
      "Epoch: 0161 loss_train: 0.3818 acc_train: 0.9714 loss_val: 0.8647 acc_val: 0.7920 time: 0.0106s\n",
      "Epoch: 0162 loss_train: 0.3649 acc_train: 0.9643 loss_val: 0.8647 acc_val: 0.7900 time: 0.0117s\n",
      "Epoch: 0163 loss_train: 0.3530 acc_train: 0.9857 loss_val: 0.8660 acc_val: 0.7860 time: 0.0101s\n",
      "Epoch: 0164 loss_train: 0.3607 acc_train: 0.9857 loss_val: 0.8670 acc_val: 0.7860 time: 0.0113s\n",
      "Epoch: 0165 loss_train: 0.3835 acc_train: 0.9429 loss_val: 0.8664 acc_val: 0.7900 time: 0.0103s\n",
      "Epoch: 0166 loss_train: 0.3902 acc_train: 0.9500 loss_val: 0.8642 acc_val: 0.7900 time: 0.0100s\n",
      "Epoch: 0167 loss_train: 0.3694 acc_train: 0.9714 loss_val: 0.8644 acc_val: 0.7900 time: 0.0107s\n",
      "Epoch: 0168 loss_train: 0.3620 acc_train: 0.9714 loss_val: 0.8651 acc_val: 0.7860 time: 0.0096s\n",
      "Epoch: 0169 loss_train: 0.3942 acc_train: 0.9571 loss_val: 0.8640 acc_val: 0.7880 time: 0.0123s\n",
      "Epoch: 0170 loss_train: 0.3684 acc_train: 0.9643 loss_val: 0.8628 acc_val: 0.7900 time: 0.0137s\n",
      "Epoch: 0171 loss_train: 0.3520 acc_train: 0.9857 loss_val: 0.8604 acc_val: 0.8000 time: 0.0102s\n",
      "Epoch: 0172 loss_train: 0.3422 acc_train: 0.9643 loss_val: 0.8604 acc_val: 0.7920 time: 0.0107s\n",
      "Epoch: 0173 loss_train: 0.3314 acc_train: 0.9714 loss_val: 0.8570 acc_val: 0.7920 time: 0.0115s\n",
      "Epoch: 0174 loss_train: 0.3732 acc_train: 0.9786 loss_val: 0.8537 acc_val: 0.7900 time: 0.0107s\n",
      "Epoch: 0175 loss_train: 0.3835 acc_train: 0.9571 loss_val: 0.8511 acc_val: 0.7880 time: 0.0104s\n",
      "Epoch: 0176 loss_train: 0.3577 acc_train: 0.9500 loss_val: 0.8498 acc_val: 0.7900 time: 0.0118s\n",
      "Epoch: 0177 loss_train: 0.3383 acc_train: 0.9643 loss_val: 0.8508 acc_val: 0.7920 time: 0.0103s\n",
      "Epoch: 0178 loss_train: 0.3636 acc_train: 0.9714 loss_val: 0.8527 acc_val: 0.7880 time: 0.0110s\n",
      "Epoch: 0179 loss_train: 0.3185 acc_train: 0.9786 loss_val: 0.8543 acc_val: 0.7840 time: 0.0117s\n",
      "Epoch: 0180 loss_train: 0.3399 acc_train: 0.9786 loss_val: 0.8548 acc_val: 0.7840 time: 0.0102s\n",
      "Epoch: 0181 loss_train: 0.3591 acc_train: 0.9643 loss_val: 0.8592 acc_val: 0.7760 time: 0.0103s\n",
      "Epoch: 0182 loss_train: 0.3281 acc_train: 0.9714 loss_val: 0.8616 acc_val: 0.7740 time: 0.0100s\n",
      "Epoch: 0183 loss_train: 0.3188 acc_train: 0.9786 loss_val: 0.8588 acc_val: 0.7760 time: 0.0100s\n",
      "Epoch: 0184 loss_train: 0.3446 acc_train: 0.9786 loss_val: 0.8535 acc_val: 0.7760 time: 0.0112s\n",
      "Epoch: 0185 loss_train: 0.3016 acc_train: 0.9857 loss_val: 0.8464 acc_val: 0.7860 time: 0.0119s\n",
      "Epoch: 0186 loss_train: 0.3115 acc_train: 0.9786 loss_val: 0.8380 acc_val: 0.7940 time: 0.0117s\n",
      "Epoch: 0187 loss_train: 0.2858 acc_train: 0.9643 loss_val: 0.8317 acc_val: 0.8020 time: 0.0141s\n",
      "Epoch: 0188 loss_train: 0.3326 acc_train: 0.9786 loss_val: 0.8260 acc_val: 0.8060 time: 0.0138s\n",
      "Epoch: 0189 loss_train: 0.3157 acc_train: 0.9786 loss_val: 0.8231 acc_val: 0.8040 time: 0.0150s\n",
      "Epoch: 0190 loss_train: 0.3419 acc_train: 0.9714 loss_val: 0.8227 acc_val: 0.8040 time: 0.0167s\n",
      "Epoch: 0191 loss_train: 0.3537 acc_train: 0.9714 loss_val: 0.8252 acc_val: 0.8000 time: 0.0143s\n",
      "Epoch: 0192 loss_train: 0.3001 acc_train: 0.9786 loss_val: 0.8316 acc_val: 0.7880 time: 0.0134s\n",
      "Epoch: 0193 loss_train: 0.3155 acc_train: 0.9929 loss_val: 0.8382 acc_val: 0.7840 time: 0.0135s\n",
      "Epoch: 0194 loss_train: 0.3547 acc_train: 0.9500 loss_val: 0.8437 acc_val: 0.7820 time: 0.0113s\n",
      "Epoch: 0195 loss_train: 0.3397 acc_train: 0.9786 loss_val: 0.8456 acc_val: 0.7840 time: 0.0107s\n",
      "Epoch: 0196 loss_train: 0.3108 acc_train: 0.9857 loss_val: 0.8439 acc_val: 0.7780 time: 0.0103s\n",
      "Epoch: 0197 loss_train: 0.3139 acc_train: 0.9857 loss_val: 0.8411 acc_val: 0.7800 time: 0.0108s\n",
      "Epoch: 0198 loss_train: 0.3473 acc_train: 0.9786 loss_val: 0.8384 acc_val: 0.7800 time: 0.0101s\n",
      "Epoch: 0199 loss_train: 0.3199 acc_train: 0.9643 loss_val: 0.8373 acc_val: 0.7880 time: 0.0101s\n",
      "Epoch: 0200 loss_train: 0.2826 acc_train: 0.9786 loss_val: 0.8322 acc_val: 0.7900 time: 0.0104s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7862623929977417, 0.805, 2.441358804702759)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dim = dataset.num_features\n",
    "num_class = dataset.num_classes\n",
    "dropout = 0.5\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, hidden_dim, feature_dim, num_class, dropout=0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.gc1 = GCNConv(feature_dim, hidden_dim)\n",
    "        self.gc2 = GCNConv(hidden_dim, num_class)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.gc1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.gc2(x, edge_index)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model22 = GCN(hidden_dim=16,\n",
    "            feature_dim=feature_dim,\n",
    "            num_class=num_class,\n",
    "            dropout=0.5\n",
    "            )\n",
    "print(model22)\n",
    "\n",
    "run_experiment(model=model22,\n",
    "               data=data,\n",
    "               lr=0.01,\n",
    "               weight_decay=5e-4,\n",
    "               model_name=\"2l\",\n",
    "               run=1,\n",
    "               num_epochs=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iterENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
