{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m  \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m run_experiment\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m GCN, iterativeGCN\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m Planetoid\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch_geometric\n",
    "\n",
    "from  tils import run_experiment\n",
    "from models import GCN, iterativeGCN\n",
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('===========================================================================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features + torch.normal(mean=0, std=noise_std, size=features.shape)\n",
    "print(\"Total data mean: \", torch.mean(features))\n",
    "print(\"Mean of feature variances: \", torch.std(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = range(140)\n",
    "idx_val = range(200, 500)\n",
    "idx_test = range(500, 1500)\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 16\n",
    "dropout = 0.5\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "num_epochs = 200\n",
    "smooth_fac = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ite_GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nclass, dropout, train_nite, eval_nite=0, allow_grad=True, smooth_fac=0):\n",
    "        '''     \n",
    "        - This model is a 1-layer GCN with nite iterations, followed by a linear layer and a log_softmax\n",
    "            - GC layer:     nfeat to nfeat\n",
    "            - linear layer: nfeat to nclass, (to cast hidden representations of nodes to a dimension of nclass)\n",
    "        - Activation: ReLu\n",
    "        - Input:\n",
    "            - nfeat:        the number of features of each node\n",
    "            - nclass:       the number of target classes (we are doing a node classification task here)\n",
    "            - dropout:      dropout rate\n",
    "            - train_nite:   the number of iterations during training\n",
    "            - eval_nite:    the number of iterations during evaluation, \n",
    "                            if not specified (or invalid), intialize to the same as train_nite\n",
    "            - allow_grad:   (bool) defaulted to True. \n",
    "                            whether or nor allow gradients to flow through all GC iterations, \n",
    "                            if False, gradients will only flow to the last iteration\n",
    "            - smooth_fac:   a number in [0,1], smoothing factor, controls how much of the OLD iteration result is\n",
    "                            counted in the skip connection in each iteration\n",
    "                            for example, smooth_fac = x means y_{i+1} = x * y_i + (1-x) * y_{i+1}\n",
    "                            Invalid inputs will be treated as 0.\n",
    "        - Output:\n",
    "            - A probability vector of length nclass, by log_softmax\n",
    "        '''\n",
    "        super(ite_GCN, self).__init__()\n",
    "\n",
    "        self.gc = GraphConvolution(nfeat, nfeat)\n",
    "        self.linear_no_bias = nn.Linear(nfeat, nclass, bias=False)\n",
    "        self.dropout = dropout\n",
    "        self.train_nite = train_nite\n",
    "        self.allow_grad = allow_grad\n",
    "        self.smooth_fac = smooth_fac\n",
    "        self.eval_nite = eval_nite\n",
    "        \n",
    "        if (smooth_fac > 1) or (smooth_fac < 0):\n",
    "            print(\"Invalid smoothing factor. Treat as 0.\")\n",
    "            self.smooth_fac = 0\n",
    "        if (eval_nite <= 0):\n",
    "            print(\"Unspecified or invalid number of iterations for inference. Treat as the same as training iterations.\")\n",
    "            self.eval_nite = self.train_nite\n",
    "        \n",
    "        print(\"Initialize a 1-layer GCN with \", self.train_nite, \"iterations\")\n",
    "        print(\"Gradient flows to all iterations: \", allow_grad)\n",
    "\n",
    "    def run_one_layer(self, x, adj):\n",
    "        x_old = x\n",
    "        x_new = self.gc(x, adj)\n",
    "        x = F.relu(self.smooth_fac * x_old + (1 - self.smooth_fac) * x_new)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        if self.training:\n",
    "            for i in range(self.train_nite):\n",
    "                if not self.allow_grad:\n",
    "                    # print(\"no no no! new new\")\n",
    "                    x = x.detach()\n",
    "                    x = self.run_one_layer(x, adj)\n",
    "                else:\n",
    "                    # print(\"yea yea yea\")\n",
    "                    x = self.run_one_layer(x, adj)\n",
    "        else:\n",
    "            for i in range(self.eval_nite):\n",
    "                x = self.run_one_layer(x, adj)\n",
    "\n",
    "        \n",
    "        x = self.linear_no_bias(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, model_dict, patience=1, activation_iteration=100):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.min_acc_val = -np.inf\n",
    "        self.saved_model = model_dict\n",
    "        self.activation_iteration = activation_iteration\n",
    "\n",
    "    def early_stop(self, acc_val, model_dict, iteration):\n",
    "        if (acc_val >= self.min_acc_val) or (iteration < self.activation_iteration):\n",
    "            self.min_acc_val = acc_val\n",
    "            self.saved_model = model_dict\n",
    "            self.counter = 0\n",
    "            print(\"keep\")\n",
    "        else:\n",
    "            print(\"ohohoh\")\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.saved_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(num_epochs, model, lr, weight_decay, features, adj, idx_train, idx_val, idx_test, labels, patience=3):\n",
    "    print(\"runrunrun!\")\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "    t_total = time.time()\n",
    "    loss_TRAIN = []\n",
    "    acc_TRAIN = []\n",
    "    loss_VAL = []\n",
    "    acc_VAL = []\n",
    "    # early_stopper = EarlyStopper(model.state_dict().copy(), patience=patience)\n",
    "    for epoch in range(num_epochs):\n",
    "        t = time.time()\n",
    "    \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(features, adj)\n",
    "        \n",
    "        loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "        loss_TRAIN.append(loss_train)\n",
    "        acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "        acc_TRAIN.append(acc_train)\n",
    "\n",
    "        t3 = time.time()\n",
    "        loss_train.backward()\n",
    "        t4 = time.time()\n",
    "        print(\"backward: \", t4-t3)\n",
    "        # print(\"before step: \", model.gc.weight)\n",
    "        optimizer.step()\n",
    "        # print(\"after step: \", model.gc.weight)\n",
    "\n",
    "        \n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        # t1 = time.time()\n",
    "        output = model(features, adj)\n",
    "        # print(\"eval output: \", output)\n",
    "        # t2 = time.time()\n",
    "        # print(\"forward time: \", t2-t1)\n",
    "\n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "        loss_VAL.append(loss_val)\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "        acc_VAL.append(acc_val)\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "            'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "            'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "            'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "            'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "            'time: {:.4f}s'.format(time.time() - t))\n",
    "        # if early_stopper.early_stop(acc_val=acc_val, model_dict=model.state_dict().copy(), iteration=epoch):\n",
    "        #     model.load_state_dict(early_stopper.get_model())\n",
    "        #     break\n",
    "        \n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "    # Testing\n",
    "    test(model, features, adj, idx_test, labels)\n",
    "    return loss_TRAIN, acc_TRAIN, loss_VAL, acc_VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "weight_decay = 1e-4\n",
    "num_epochs=200\n",
    "model3 = ite_GCN(num_feat=features.shape[1],\n",
    "            num_class=labels.max().item() + 1,\n",
    "            dropout=dropout,\n",
    "            latent_dim=16,\n",
    "            train_nite= 2,\n",
    "            eval_nite= 0,\n",
    "            allow_grad=True,\n",
    "            smooth_fac=smooth_fac)\n",
    "run_experiment(num_epochs=num_epochs, model=model3, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels, model_name=\"lalala\", run=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 16\n",
    "dropout = 0.5\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "num_epochs = 200\n",
    "smooth_fac = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = GCN_5(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout\n",
    ")\n",
    "run_experiment(num_epochs=num_epochs, model=model0, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# totally messed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_t = []\n",
    "for ten in loss_TRAIN:\n",
    "    l_t.append(ten.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(l_t, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_t = []\n",
    "for ten in acc_TRAIN:\n",
    "    a_t.append(ten.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a_t, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_v = []\n",
    "for ten in loss_VAL:\n",
    "    l_v.append(ten.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(l_v, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_v = []\n",
    "for ten in acc_VAL:\n",
    "    a_v.append(ten.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a_v, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = ite_GCN(nfeat=features.shape[1],\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout,\n",
    "            train_nite = 3,\n",
    "            allow_grad=False,\n",
    "            smooth_fac=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(num_epochs=400, model=model4, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model3.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(name, param.grad.abs().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = GCN_3(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ite_GCN(nfeat=features.shape[1],\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0,\n",
    "            train_nite= 2,\n",
    "            eval_nite= 0,\n",
    "            allow_grad=True,\n",
    "            smooth_fac=smooth_fac)\n",
    "run_experiment(num_epochs, model2, lr, weight_decay, features, adj, idx_train, idx_val, idx_test, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for i in range(1, 30):\n",
    "    model = ite_GCN(nfeat=features.shape[1],\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0,\n",
    "            train_nite= 3,\n",
    "            eval_nite= i,\n",
    "            allow_grad=True,\n",
    "            smooth_fac=smooth_fac)\n",
    "    model.load_state_dict(model2.state_dict().copy())\n",
    "    loss_test, acc_test = test(model, features, adj, idx_test, labels)\n",
    "    test_losses.append(loss_test.item())\n",
    "    test_accuracies.append(acc_test.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(test_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2)\n",
    "fig.tight_layout(pad=5.0)\n",
    "ax[0, 0].plot(a, 'b') #row=0, col=0\n",
    "ax[0, 0].title.set_text(\"Training loss\")\n",
    "ax[0, 1].plot(a, 'b') #row=0, col=1\n",
    "ax[0, 1].title.set_text(\"Training accuracy\")\n",
    "ax[1, 0].plot(a, 'b') #row=1, col=0\n",
    "ax[1, 0].title.set_text(\"Validation loss\")\n",
    "ax[1, 1].plot(a, 'b') #row=1, col=1\n",
    "ax[1, 1].title.set_text(\"Validation accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.path.dirname('__file__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN_2_loss = []\n",
    "GCN_2_acc = []\n",
    "GCN_2_time = []\n",
    "for i in range(num_runs):\n",
    "    # run 2L, 3L ten times each\n",
    "    model = GCN_2(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "    loss, acc, train_time = run_experiment(num_epochs=num_epochs, model=model, lr=0.01, \n",
    "                   weight_decay=weight_decay, features=features, adj=adj, \n",
    "                   idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels, model_name=\"2L\", run=i)\n",
    "    GCN_2_loss.append(loss)\n",
    "    GCN_2_acc.append(acc)\n",
    "    GCN_2_time.append(train_time)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import print_stats\n",
    "print_stats(model_name=\"Non-iterative 2 layer\", acc_test=GCN_2_acc, training_time=GCN_2_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = GraphConvolution(features.shape[1], hidden)\n",
    "layer2 = GraphConvolution(features.shape[1], features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "layer1(features, adj)\n",
    "t2 = time.time()\n",
    "t3 = time.time()\n",
    "layer2(features, adj)\n",
    "t4 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = t4-t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt/t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import load_data, test, train, accuracy, run_experiment\n",
    "from models import GCN_2, GCN_3, GCN_5, ite_GCN\n",
    "from layers import GraphConvolution\n",
    "\n",
    "adj, features, labels = load_data(path=\"../data/cora/\", dataset=\"cora\")\n",
    "idx_train = range(140)\n",
    "idx_val = range(200, 500)\n",
    "idx_test = range(500, 1500)\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ite_GCN(nn.Module):\n",
    "    def __init__(self, num_feat, num_class, dropout, latent_dim,train_nite, eval_nite=0, allow_grad=True, smooth_fac=0):\n",
    "        '''     \n",
    "        - The current architecture of the model is:\n",
    "            - Encoder:      encode features into the latent space\n",
    "            - GC:           iterations in the latent space\n",
    "            - Decoder:      decode to the output space\n",
    "        - Activation: ReLu\n",
    "        - Input:\n",
    "            - num_feat:        the number of features of each node\n",
    "            - num_class:       the number of target classes (we are doing a node classification task here)\n",
    "            - dropout:      dropout rate\n",
    "            - latent_dim:   the dimension of the latent space\n",
    "            - train_nite:   the number of iterations during training\n",
    "            - eval_nite:    the number of iterations during evaluation, \n",
    "                            if not specified (or invalid), intialize to the same as train_nite\n",
    "            - allow_grad:   (bool) defaulted to True. \n",
    "                            whether or nor allow gradients to flow through all GC iterations, \n",
    "                            if False, gradients will only flow to the last iteration\n",
    "            - smooth_fac:   a number in [0,1], smoothing factor, controls how much of the OLD iteration result is\n",
    "                            counted in the skip connection in each iteration\n",
    "                            for example, smooth_fac = x means y_{i+1} = x * y_i + (1-x) * y_{i+1}\n",
    "                            Invalid inputs will be treated as 0.\n",
    "        - Output:\n",
    "            - A probability vector of length nclass, by log_softmax\n",
    "        '''\n",
    "        super(ite_GCN, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(num_feat, latent_dim)\n",
    "        self.gc = GraphConvolution(latent_dim, latent_dim)\n",
    "        self.decoder = nn.Linear(latent_dim, num_class, bias=False)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.train_nite = train_nite\n",
    "        self.allow_grad = allow_grad\n",
    "        self.smooth_fac = smooth_fac\n",
    "        self.eval_nite = eval_nite\n",
    "        \n",
    "        if (smooth_fac > 1) or (smooth_fac < 0):\n",
    "            # print(\"Invalid smoothing factor. Treat as 0.\")\n",
    "            self.smooth_fac = 0\n",
    "        if (eval_nite <= 0):\n",
    "            # print(\"Unspecified or invalid number of iterations for inference. Treat as the same as training iterations.\")\n",
    "            self.eval_nite = self.train_nite\n",
    "        \n",
    "        print(\"Initialize a 1-layer GCN with \", self.train_nite, \"iterations and latent dimension: \", latent_dim)\n",
    "        print(\"Gradient flows to all iterations: \", allow_grad)\n",
    "\n",
    "    def run_one_layer(self, x, adj):\n",
    "        x_old = x\n",
    "        x_new = self.gc(x, adj)\n",
    "        x = F.relu(self.smooth_fac * x_old + (1 - self.smooth_fac) * x_new)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        print(\"Inpute features shape: \", x.shape)\n",
    "        print(\"Adjcency matrix shape: \", adj.shape)\n",
    "        x = F.relu(self.encoder(x))\n",
    "        print(\"Encoded features shape: \", x.shape)\n",
    "        # x = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        if self.training:\n",
    "            num_iterations = self.train_nite\n",
    "        else:\n",
    "            num_iterations = self.eval_nite\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            x = self.run_one_layer(x, adj)\n",
    "\n",
    "        print(\"Feature shape after iterations: \", x.shape)\n",
    "        x = self.decoder(x)\n",
    "        print(\"Decoded shape: \", x.shape)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 16\n",
    "dropout = 0.5\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "num_epochs = 200\n",
    "smooth_fac = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = GCN_2(nfeat=features.shape[1],\n",
    "            nhid=16,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "run_experiment(num_epochs=num_epochs, model=model1, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels, model_name=\"lalalagua\", run=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.007\n",
    "weight_decay = 1e-4\n",
    "num_epochs = 200\n",
    "model3 = ite_GCN(num_feat=features.shape[1],\n",
    "            num_class=labels.max().item() + 1,\n",
    "            dropout=dropout,\n",
    "            latent_dim= 16,\n",
    "            train_nite= 2,\n",
    "            eval_nite= 0,\n",
    "            allow_grad=True,\n",
    "            smooth_fac=0.5)\n",
    "run_experiment(num_epochs=num_epochs, model=model3, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels, model_name=\"lalala\", run=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ite_GCN(num_feat=features.shape[1],\n",
    "            num_class=labels.max().item() + 1,\n",
    "            dropout=dropout,\n",
    "            latent_dim= 16,\n",
    "            train_nite= 2,\n",
    "            eval_nite= 0,\n",
    "            allow_grad=True,\n",
    "            smooth_fac=0.5)\n",
    "aa = model(features, adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1433, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 16)\n",
    "        )\n",
    "          \n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(16, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 1433),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        return decoded\n",
    "    \n",
    "AE_model = AutoEncoder()\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "AE_epochs = 200\n",
    "AE_lr = 0.0001\n",
    "optimizer = torch.optim.Adam(AE_model.parameters(), lr=AE_lr)\n",
    "AE_train_loss = []\n",
    "\n",
    "for i in range(AE_epochs):\n",
    "    output = AE_model(features)\n",
    "    loss = criterion(output, features)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"Epoch \", i, \": \", \"loss: \", loss.item())\n",
    "    AE_train_loss.append(loss.item())\n",
    "\n",
    "plt.plot(AE_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_new = AE_model.encoder(features).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ite_GCN_try(nn.Module):\n",
    "    def __init__(self, num_class, dropout, latent_dim, train_nite, eval_nite=0, allow_grad=True, smooth_fac=0):\n",
    "        '''     \n",
    "        - The current architecture of the model is:\n",
    "           In this one, I separated encoder out of the model\n",
    "            - GC:           iterations in the latent space\n",
    "            - Decoder:      decode to the output space\n",
    "        - Activation: ReLu\n",
    "        - Input:\n",
    "            - num_feat:        the number of features of each node\n",
    "            - num_class:       the number of target classes (we are doing a node classification task here)\n",
    "            - dropout:      dropout rate\n",
    "            - latent_dim:   the dimension of the latent space\n",
    "            - train_nite:   the number of iterations during training\n",
    "            - eval_nite:    the number of iterations during evaluation, \n",
    "                            if not specified (or invalid), intialize to the same as train_nite\n",
    "            - allow_grad:   (bool) defaulted to True. \n",
    "                            whether or nor allow gradients to flow through all GC iterations, \n",
    "                            if False, gradients will only flow to the last iteration\n",
    "            - smooth_fac:   a number in [0,1], smoothing factor, controls how much of the OLD iteration result is\n",
    "                            counted in the skip connection in each iteration\n",
    "                            for example, smooth_fac = x means y_{i+1} = x * y_i + (1-x) * y_{i+1}\n",
    "                            Invalid inputs will be treated as 0.\n",
    "        - Output:\n",
    "            - A probability vector of length nclass, by log_softmax\n",
    "        '''\n",
    "        super(ite_GCN_try, self).__init__()\n",
    "\n",
    "        self.gc = GraphConvolution(latent_dim, latent_dim)\n",
    "        self.decoder = nn.Linear(latent_dim, num_class, bias=False)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.train_nite = train_nite\n",
    "        self.allow_grad = allow_grad\n",
    "        self.smooth_fac = smooth_fac\n",
    "        self.eval_nite = eval_nite\n",
    "        \n",
    "        if (smooth_fac > 1) or (smooth_fac < 0):\n",
    "            print(\"Invalid smoothing factor. Treat as 0.\")\n",
    "            self.smooth_fac = 0\n",
    "        if (eval_nite <= 0):\n",
    "            print(\"Unspecified or invalid number of iterations for inference. Treat as the same as training iterations.\")\n",
    "            self.eval_nite = self.train_nite\n",
    "        \n",
    "        print(\"Initialize a 1-layer GCN with \", self.train_nite, \"iterations\")\n",
    "        print(\"Gradient flows to all iterations: \", allow_grad)\n",
    "\n",
    "    def run_one_layer(self, x, adj):\n",
    "        x_old = x\n",
    "        x_new = self.gc(x, adj)\n",
    "        x = F.relu(self.smooth_fac * x_old + (1 - self.smooth_fac) * x_new)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        if self.training:\n",
    "            for i in range(self.train_nite):\n",
    "                if not self.allow_grad:\n",
    "                    x = x.detach()\n",
    "                    x = self.run_one_layer(x, adj)\n",
    "                else:\n",
    "                    x = self.run_one_layer(x, adj)\n",
    "        else:\n",
    "            for i in range(self.eval_nite):\n",
    "                x = self.run_one_layer(x, adj)\n",
    "\n",
    "        x = self.decoder(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "weight_decay = 1e-4\n",
    "num_epochs = 200\n",
    "dropout = 0.5\n",
    "model = ite_GCN_try(\n",
    "            num_class=labels.max().item() + 1,\n",
    "            dropout=dropout,\n",
    "            latent_dim= 16,\n",
    "            train_nite= 2,\n",
    "            eval_nite= 0,\n",
    "            allow_grad=True,\n",
    "            smooth_fac=0.5)\n",
    "run_experiment(num_epochs=num_epochs, model=model, lr=lr, weight_decay=weight_decay, features=features_new, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels, model_name=\"lalala\", run=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ite_GCN_try1(nn.Module):\n",
    "    def __init__(self, num_feat, num_class, dropout, latent_dim,train_nite, eval_nite=0, allow_grad=True, smooth_fac=0):\n",
    "        '''     \n",
    "        - The current architecture of the model is:\n",
    "            - Encoder:      encode features into the latent space\n",
    "            - GC:           iterations in the latent space\n",
    "            - Decoder:      decode to the output space\n",
    "        - Activation: ReLu\n",
    "        - Input:\n",
    "            - num_feat:        the number of features of each node\n",
    "            - num_class:       the number of target classes (we are doing a node classification task here)\n",
    "            - dropout:      dropout rate\n",
    "            - latent_dim:   the dimension of the latent space\n",
    "            - train_nite:   the number of iterations during training\n",
    "            - eval_nite:    the number of iterations during evaluation, \n",
    "                            if not specified (or invalid), intialize to the same as train_nite\n",
    "            - allow_grad:   (bool) defaulted to True. \n",
    "                            whether or nor allow gradients to flow through all GC iterations, \n",
    "                            if False, gradients will only flow to the last iteration\n",
    "            - smooth_fac:   a number in [0,1], smoothing factor, controls how much of the OLD iteration result is\n",
    "                            counted in the skip connection in each iteration\n",
    "                            for example, smooth_fac = x means y_{i+1} = x * y_i + (1-x) * y_{i+1}\n",
    "                            Invalid inputs will be treated as 0.\n",
    "        - Output:\n",
    "            - A probability vector of length nclass, by log_softmax\n",
    "        '''\n",
    "        super(ite_GCN_try1, self).__init__()\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1433, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 16)\n",
    "        )\n",
    "        self.gc = GraphConvolution(latent_dim, latent_dim)\n",
    "        self.decoder = nn.Linear(latent_dim, num_class, bias=False)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.train_nite = train_nite\n",
    "        self.allow_grad = allow_grad\n",
    "        self.smooth_fac = smooth_fac\n",
    "        self.eval_nite = eval_nite\n",
    "        \n",
    "        if (smooth_fac > 1) or (smooth_fac < 0):\n",
    "            print(\"Invalid smoothing factor. Treat as 0.\")\n",
    "            self.smooth_fac = 0\n",
    "        if (eval_nite <= 0):\n",
    "            print(\"Unspecified or invalid number of iterations for inference. Treat as the same as training iterations.\")\n",
    "            self.eval_nite = self.train_nite\n",
    "        \n",
    "        print(\"Initialize a 1-layer GCN with \", self.train_nite, \"iterations\")\n",
    "        print(\"Gradient flows to all iterations: \", allow_grad)\n",
    "\n",
    "    def run_one_layer(self, x, adj):\n",
    "        x_old = x\n",
    "        x_new = self.gc(x, adj)\n",
    "        x = F.relu(self.smooth_fac * x_old + (1 - self.smooth_fac) * x_new)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        print(\"Input dimension: \", x.shape)\n",
    "        x = self.encoder(x)\n",
    "        print(\"Encoded dimension: \", x.shape)\n",
    "\n",
    "        if self.training:\n",
    "            for i in range(self.train_nite):\n",
    "                if not self.allow_grad:\n",
    "                    x = x.detach()\n",
    "                    x = self.run_one_layer(x, adj)\n",
    "                else:\n",
    "                    x = self.run_one_layer(x, adj)\n",
    "        else:\n",
    "            for i in range(self.eval_nite):\n",
    "                x = self.run_one_layer(x, adj)\n",
    "\n",
    "        x = self.decoder(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ite_GCN_try1(num_feat=1433,\n",
    "            num_class=labels.max().item() + 1,\n",
    "            dropout=dropout,\n",
    "            latent_dim= 16,\n",
    "            train_nite= 2,\n",
    "            eval_nite= 0,\n",
    "            allow_grad=True,\n",
    "            smooth_fac=0.5)\n",
    "run_experiment(num_epochs=num_epochs, model=model, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels, model_name=\"lalala\", run=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iterENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
