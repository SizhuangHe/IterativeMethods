{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "BASE_PATH = globals()['_dh'][0].parent.parent.parent.parent.absolute()\n",
    "sys.path.insert(1, str(BASE_PATH))\n",
    "# import torch\n",
    "from src.utils.utils import exp_vocsp, make_uniform_schedule\n",
    "# from src.models.iterativeModels import iterativeGCN_vocsp\n",
    "# from src.utils.metrics import MAD\n",
    "from torch_geometric.datasets import LRGBDataset\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# from torch.optim import AdamW\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR\n",
    "\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "train_dataset = LRGBDataset(root=\"data/\", name=\"Peptides-func\", split=\"train\")\n",
    "val_dataset = LRGBDataset(root=\"data/\", name=\"Peptides-func\", split=\"val\")\n",
    "test_dataset = LRGBDataset(root=\"data/\", name=\"Peptides-func\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "def eval_ap(y_true, y_pred):\n",
    "        '''\n",
    "            compute Average Precision (AP) averaged across tasks\n",
    "        '''\n",
    "\n",
    "        ap_list = []\n",
    "\n",
    "        for i in range(y_true.shape[1]):\n",
    "            #AUC is only defined when there is at least one positive data.\n",
    "            if np.sum(y_true[:,i] == 1) > 0 and np.sum(y_true[:,i] == 0) > 0:\n",
    "                # ignore nan values\n",
    "                is_labeled = y_true[:,i] == y_true[:,i]\n",
    "                ap = average_precision_score(y_true[is_labeled,i], y_pred[is_labeled,i])\n",
    "                print(\"ap\",ap)\n",
    "                ap_list.append(ap)\n",
    "\n",
    "        if len(ap_list) == 0:\n",
    "            raise RuntimeError('No positively labeled data available. Cannot compute Average Precision.')\n",
    "\n",
    "        return sum(ap_list)/len(ap_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.iterativeModels import iterativeGCN_peptides\n",
    "model = iterativeGCN_peptides(out_dim=train_dataset.num_classes,\n",
    "                              hidden_dim=32,\n",
    "                              train_schedule=[0.5, 0.5]\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2331, 10])\n",
      "torch.Size([2331, 10])\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "preds = []\n",
    "for batched_data in val_loader:\n",
    "    y_true.append(batched_data.y.detach())\n",
    "    pred = model(batched_data.x, batched_data.edge_index, batched_data.edge_attr,batched_data.batch)\n",
    "    preds.append(pred)\n",
    "y_true = torch.concat(y_true)\n",
    "preds = torch.concat(preds)\n",
    "print(y_true.shape)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true [[1. 0. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "pred [[-0.08293815  0.08614053  0.14155483 ... -0.45164433 -0.31345788\n",
      "   0.09716818]\n",
      " [-0.0948697   0.02251315  0.13115355 ... -0.42408094 -0.37856725\n",
      "   0.11396343]\n",
      " [-0.04704923  0.19769663  0.25790745 ... -0.39948735 -0.45777595\n",
      "   0.10346394]\n",
      " ...\n",
      " [-0.08958013  0.06446217  0.10771471 ... -0.41582912 -0.26132357\n",
      "   0.12289611]\n",
      " [-0.06635819  0.10537678  0.12020095 ... -0.4801312  -0.26969352\n",
      "   0.04356698]\n",
      " [-0.01072649  0.00812696 -0.09428862 ... -0.4327428  -0.17169869\n",
      "   0.21387328]]\n",
      "10\n",
      "ap 0.09341149143819545\n",
      "ap 0.025261973430720123\n",
      "ap 0.07245924881817692\n",
      "ap 0.08782007913893959\n",
      "ap 0.5905330246901417\n",
      "ap 0.23832441643906108\n",
      "ap 0.26279590846862066\n",
      "ap 0.150919427048299\n",
      "ap 0.01720393030304569\n",
      "ap 0.26957900745843066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18083085072336308"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ap(y_true.detach().numpy(), preds.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2331,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_true = []\n",
    "y_pred = []\n",
    "preds = []\n",
    "\n",
    "for step, batched_data in enumerate(val_loader):  # Iterate in batches over the training dataset.\n",
    "        \n",
    "    pred = model(batched_data.x, batched_data.edge_index, batched_data.edge_attr,batched_data.batch) # size of pred is [number of nodes, number of features]\n",
    "    true = batched_data.y\n",
    "\n",
    "    pred_val = pred.max(dim=1)[1] # pred_val contains actually class predictions\n",
    "    y_pred.append(pred_val.detach())\n",
    "    y_true.append(true.detach())\n",
    "    preds.append(pred.detach())\n",
    "\n",
    "    \n",
    "y_true = torch.cat(y_true, dim = 0).cpu().numpy()\n",
    "y_pred = torch.cat(y_pred, dim = 0).cpu().numpy()\n",
    "preds = torch.cat(preds, dim=0).cpu().numpy()\n",
    "print(y_pred.shape)\n",
    "# val_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "# val_ap = eval_ap(y_true, preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.rand(batch.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred [[0.4729731  0.48898375 0.00603795 0.40174198 0.33372813 0.14399242\n",
      "  0.7643688  0.7350955  0.01468372 0.07714814]]\n",
      "10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No positively labeled data available. Cannot compute Average Precision.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m eval_ap(batch\u001b[39m.\u001b[39;49my\u001b[39m.\u001b[39;49mnumpy(), pred\u001b[39m.\u001b[39;49mnumpy())\n",
      "Cell \u001b[0;32mIn[74], line 22\u001b[0m, in \u001b[0;36meval_ap\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     19\u001b[0m         ap_list\u001b[39m.\u001b[39mappend(ap)\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(ap_list) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mNo positively labeled data available. Cannot compute Average Precision.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m(ap_list)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(ap_list)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No positively labeled data available. Cannot compute Average Precision."
     ]
    }
   ],
   "source": [
    "eval_ap(batch.y.numpy(), pred.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iterENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
