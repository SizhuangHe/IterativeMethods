{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sizhuang/Desktop/GitHubRepos/IterativeMethods/iterENV/lib/python3.8/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/sizhuang/Desktop/GitHubRepos/IterativeMethods/iterENV/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from ogb.graphproppred import Evaluator\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import sys\n",
    "BASE_PATH = globals()['_dh'][0].parent.parent.parent.parent.absolute()\n",
    "sys.path.insert(1, str(BASE_PATH))\n",
    "from src.models.models import GCN\n",
    "from src.utils.utils import make_uniform_schedule, count_parameters\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import LRGBDataset\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import MLP, Linear\n",
    "from sklearn.metrics import f1_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.dropbox.com/s/8x722ai272wqwl4/pascalvocsp.zip?dl=1\n",
      "Extracting data/LRGB/pascalvocsp.zip\n",
      "Processing...\n",
      "Processing train dataset: 100%|██████████| 8498/8498 [00:00<00:00, 10813.73it/s]\n",
      "Processing val dataset: 100%|██████████| 1428/1428 [00:00<00:00, 9368.10it/s]\n",
      "Processing test dataset: 100%|██████████| 1429/1429 [00:00<00:00, 14948.65it/s]\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 8498\n",
      "val: 1428\n",
      "test: 1429\n"
     ]
    }
   ],
   "source": [
    "train_dataset = LRGBDataset(root=\"data/LRGB\",\n",
    "                      name=\"PascalVOC-SP\",\n",
    "                      split=\"train\")\n",
    "val_dataset = LRGBDataset(root=\"data/LRGB\",\n",
    "                      name=\"PascalVOC-SP\",\n",
    "                      split=\"val\")\n",
    "test_dataset = LRGBDataset(root=\"data/LRGB\",\n",
    "                      name=\"PascalVOC-SP\",\n",
    "                      split=\"test\")\n",
    "print(\"train:\", len(train_dataset))\n",
    "print(\"val:\", len(val_dataset))\n",
    "print(\"test:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03909621650257977"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "y_pred = np.zeros(val_dataset.y.size()[0])\n",
    "f1_score(val_dataset.y.numpy(), y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 1\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_dataset(train_len, val_len, test_len):\n",
    "    train_idx = random.sample(range(len(train_dataset)), train_len)\n",
    "    val_idx = random.sample(range(len(val_dataset)), val_len)\n",
    "    test_idx = random.sample(range(len(test_dataset)), test_len)\n",
    "    small_train_set = torch.utils.data.Subset(train_dataset, train_idx)\n",
    "    small_val_set = torch.utils.data.Subset(val_dataset, val_idx)\n",
    "    small_test_set = torch.utils.data.Subset(test_dataset, test_idx)\n",
    "\n",
    "    return small_train_set, small_val_set, small_test_set\n",
    "    \n",
    "train_small_set, val_small_set, test_small_set = small_dataset(85, 14, 14)\n",
    "print(\"train:\", len(train_small_set))\n",
    "print(\"val:\", len(val_small_set))\n",
    "print(\"test:\", len(test_small_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_small_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_small_set, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_small_set, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "=== Description of the VOCSuperpixels dataset === \n",
    "Each graph is a tuple (x, edge_attr, edge_index, y)\n",
    "Shape of x : [num_nodes, 14]\n",
    "Shape of edge_attr : [num_edges, 1] or [num_edges, 2]\n",
    "Shape of edge_index : [2, num_edges]\n",
    "Shape of y : [num_nodes]\n",
    "\"\"\"\n",
    "\n",
    "VOC_node_input_dim = 14\n",
    "# VOC_edge_input_dim = 1 or 2; defined in class VOCEdgeEncoder\n",
    "\n",
    "class VOCNodeEncoder(torch.nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = torch.nn.Linear(VOC_node_input_dim, emb_dim)\n",
    "        # torch.nn.init.xavier_uniform_(self.encoder.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNInductiveNodeHead(nn.Module):\n",
    "    \"\"\"\n",
    "    GNN prediction head for inductive node prediction tasks.\n",
    "\n",
    "    Args:\n",
    "        dim_in (int): Input dimension\n",
    "        dim_out (int): Output dimension. For binary prediction, dim_out=1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, num_layers):\n",
    "        super(GNNInductiveNodeHead, self).__init__()\n",
    "        layers = []\n",
    "        if num_layers > 1:\n",
    "            layers.append(MLP(in_channels=in_dim,\n",
    "                                 hidden_channels=hid_dim,\n",
    "                                 out_channels=hid_dim,\n",
    "                                 num_layers=num_layers - 1,\n",
    "                                 bias=True))\n",
    "            layers.append(Linear(in_channels=hid_dim, out_channels=out_dim, bias=True))\n",
    "        else:\n",
    "            layers.append(Linear(in_channels=in_dim, out_channels=out_dim, bias=True))\n",
    "\n",
    "        self.layer_post_mp = nn.Sequential(*layers)\n",
    "                          \n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_post_mp(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iterativeGCN_vocsp(nn.Module):\n",
    "    '''\n",
    "    This iterative version of GCN is for inductive tasks on the ogbg-mol* datasets.\n",
    "    Apart from most basic ingredients of iterativeGCNs, it uses:\n",
    "        - the AtomEncoder provided by the OGB team\n",
    "        - the BondEncoder provided by the OGB team\n",
    "        - a slightly different implementation of the GCNConv layer provided by the OGB team\n",
    "            - It differs from the PyG version by adding BondEncoder to edge_attr\n",
    "        - a global mean pooling over the batch, since it's doing an inductive task\n",
    "    '''\n",
    "    def __init__(self,  \n",
    "                 out_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 train_schedule,\n",
    "                 MLP_layers=3,\n",
    "                 dropout=0.5,\n",
    "                 eval_schedule=None,\n",
    "                 xavier_init=False\n",
    "                 ):\n",
    "        super().__init__() \n",
    "        self.out_dim = out_dim\n",
    "        self.dropout = dropout\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.train_schedule = train_schedule\n",
    "        if eval_schedule is not None:\n",
    "            self.eval_schedule = eval_schedule\n",
    "        else:\n",
    "            self.eval_schedule = self.train_schedule\n",
    "\n",
    "        self.atom_encoder = VOCNodeEncoder(hidden_dim)\n",
    "        self.graph_conv = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        self.graph_pred_linear = GNNInductiveNodeHead(in_dim=hidden_dim, hid_dim=hidden_dim, out_dim=out_dim, num_layers=MLP_layers)\n",
    "    \n",
    "    def _init_xavier(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear): # GCNConv layers are already Xavier initilized\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "      \n",
    "    def _next_x(self, old_x, new_x, smooth_fac):\n",
    "        next_x = smooth_fac * old_x + (1 - smooth_fac) * new_x\n",
    "        return next_x\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        if self.training:\n",
    "            schedule = self.train_schedule\n",
    "        else:\n",
    "            schedule = self.eval_schedule\n",
    "        \n",
    "        x = self.atom_encoder(x)\n",
    "\n",
    "        for smooth_fac in range(len(schedule)):      \n",
    "            old_x = x\n",
    "            x = self.graph_conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = self.batch_norm(x)\n",
    "            new_x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = self._next_x(old_x, new_x, smooth_fac) \n",
    "        x = self.graph_pred_linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_cross_entropy(pred, true):\n",
    "    \"\"\"Weighted cross-entropy for unbalanced classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculating label weights for weighted loss computation\n",
    "    V = true.size(0)\n",
    "    \n",
    "    n_classes = pred.shape[1] if pred.ndim > 1 else 2\n",
    "    label_count = torch.bincount(true)\n",
    "    label_count = label_count[label_count.nonzero(as_tuple=True)].squeeze()\n",
    "    cluster_sizes = torch.zeros(n_classes, device=pred.device).long()\n",
    "    cluster_sizes[torch.unique(true)] = label_count\n",
    "    weight = (V - cluster_sizes).float() / V\n",
    "    weight *= (cluster_sizes > 0).float()\n",
    "\n",
    "    # multiclass\n",
    "    if pred.ndim > 1:\n",
    "        pred = F.log_softmax(pred, dim=-1)\n",
    "        loss = F.nll_loss(pred, true, weight=weight)\n",
    "        \n",
    "        return loss\n",
    "    # binary\n",
    "    else:\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, true.float(),\n",
    "                                                    weight=weight[true])\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_vocsp_epoch(model, loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    criterion = weighted_cross_entropy\n",
    "    epoch_loss = 0\n",
    "    for step, batched_data in enumerate(loader):  # Iterate in batches over the training dataset.\n",
    "        batched_data = batched_data.to(device)\n",
    "        pred = model(batched_data.x, batched_data.edge_index, batched_data.edge_attr,batched_data.batch) # size of pred is [number of nodes, number of features]\n",
    "        true = batched_data.y\n",
    "        loss = criterion(pred, true)\n",
    "        epoch_loss += loss.item() * batched_data.y.size()[0]\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "    return epoch_loss\n",
    "\n",
    "def eval_vocsp(model, loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    criterion = weighted_cross_entropy\n",
    "    val_loss = 0\n",
    "    for step, batched_data in enumerate(loader):  # Iterate in batches over the training dataset.\n",
    "        batched_data = batched_data.to(device)\n",
    "        pred = model(batched_data.x, batched_data.edge_index, batched_data.edge_attr,batched_data.batch) # size of pred is [number of nodes, number of features]\n",
    "        true = batched_data.y\n",
    "        loss = criterion(pred, true)\n",
    "        val_loss += loss.item() * batched_data.y.size()[0]\n",
    "\n",
    "        pred_val = pred.max(dim=1)[1] # pred_val contains actually class predictions\n",
    "\n",
    "        print(batched_data)\n",
    "        print(\"true:\", len(true))\n",
    "\n",
    "        y_pred.append(pred_val.detach())\n",
    "        y_true.append(true.detach())\n",
    "    \n",
    "    y_true = torch.cat(y_true, dim = 0).cpu().numpy()\n",
    "    y_pred = torch.cat(y_pred, dim = 0).cpu().numpy()\n",
    "    val_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    val_acc = accuracy_score(y_true, y_pred)\n",
    "    print(\"y_true:\")\n",
    "    print(len(y_true))\n",
    "    print(\"y_pred:\")\n",
    "    print(len(y_pred))\n",
    "        \n",
    "    return val_loss, val_f1, val_acc\n",
    "\n",
    "def train_vocsp(model, optimizer, scheduler, train_loader, valid_loader, num_epochs, device):\n",
    "    # wandb.watch(model, log=\"all\", log_freq=10)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_loss = train_vocsp_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "        val_loss, val_f1, val_acc = eval_vocsp(model, valid_loader, device)\n",
    "        \n",
    "        # wandb.log({\n",
    "        #     \"Train loss\": train_loss,\n",
    "        #     \"Validate f1\": val_f1,\n",
    "        #     \"Validate loss\": val_loss,\n",
    "        #     \"epoch\": epoch+1\n",
    "        # })\n",
    "        print(\"Epoch {}: train loss {:.4}, valid loss {:.4}, valid F1 {:.4}, valid accuracy {:.4}\".format(epoch+1, train_loss, val_loss, val_f1, val_acc))\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "def exp_vocsp(model, optimizer, scheduler,train_loader, valid_loader, test_loader, num_epochs,device):\n",
    "    num_params = count_parameters(model)\n",
    "    # wandb.log({ \n",
    "    #         'num_param': num_params\n",
    "    # }) \n",
    "    start = time.time()\n",
    "    train_vocsp(model, optimizer, scheduler,train_loader, valid_loader, num_epochs, device)\n",
    "    test_loss, test_f1, test_acc =eval_vocsp(model, test_loader, device)\n",
    "    # wandb.log({\n",
    "    #     \"Test loss\": test_loss,\n",
    "    #     \"Test f1\": test_f1\n",
    "    # })\n",
    "    end = time.time()\n",
    "    print(\"Experiment ends! Time elasped: {:.2}s, Test loss: {:.4}, Test F1: {:.6}, Test accuracy\".format(end-start, test_loss, test_f1, test_acc))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_schedule = make_uniform_schedule(5, 0.5)\n",
    "iGCN = iterativeGCN_vocsp(out_dim=train_dataset.num_classes,\n",
    "                          hidden_dim=220,\n",
    "                          train_schedule=train_schedule,\n",
    "                          MLP_layers=3,\n",
    "                          dropout=0.5\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_vocsp(iGCN, val_loader, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "# wandb.init(job_type=\"Run\", \n",
    "#                project=\"IterativeMethods\", \n",
    "#                notes=\"try small vocsp\",\n",
    "#                tags=[\"iGCN\"])\n",
    "optimizer = AdamW(iGCN.parameters(), lr=0.0005, weight_decay=0.0)\n",
    "scheduler = ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-5, verbose=True)\n",
    "exp_vocsp(iGCN, optimizer, scheduler, train_loader, val_loader, test_loader, num_epochs=200, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.models import GCN_vocsp\n",
    "gcn = GCN_vocsp(out_dim=train_dataset.num_classes,\n",
    "                          hidden_dim=220,\n",
    "                          MLP_layers=3,\n",
    "                          num_layers=8,\n",
    "                          dropout=0.5)\n",
    "optimizer = AdamW(gcn.parameters(), lr=0.0005, weight_decay=0.0)\n",
    "scheduler = ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-5, verbose=True)\n",
    "exp_vocsp(gcn, optimizer, scheduler, train_loader, val_loader, test_loader, num_epochs=200, device=\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iterENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
