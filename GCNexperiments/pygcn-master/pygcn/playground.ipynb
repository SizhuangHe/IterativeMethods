{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import load_data, accuracy, run_experiment\n",
    "from models import GCN_3, ite_GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(path=\"../data/cora/\", dataset=\"cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 16\n",
    "dropout = 0.5\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intialize a 3-layer GCN\n"
     ]
    }
   ],
   "source": [
    "model1 = GCN_3(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n"
     ]
    }
   ],
   "source": [
    "model2 = ite_GCN(nfeat=features.shape[1],\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout,\n",
    "            nite = 3,\n",
    "            allow_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runrunrun!\n",
      "Epoch: 0001 loss_train: 1.9593 acc_train: 0.1286 loss_val: 1.9487 acc_val: 0.1567 time: 0.0466s\n",
      "Epoch: 0002 loss_train: 1.9468 acc_train: 0.2000 loss_val: 1.9249 acc_val: 0.1567 time: 0.0093s\n",
      "Epoch: 0003 loss_train: 1.9269 acc_train: 0.1857 loss_val: 1.9029 acc_val: 0.1567 time: 0.0088s\n",
      "Epoch: 0004 loss_train: 1.8978 acc_train: 0.2000 loss_val: 1.8827 acc_val: 0.1567 time: 0.0086s\n",
      "Epoch: 0005 loss_train: 1.8773 acc_train: 0.2429 loss_val: 1.8639 acc_val: 0.1900 time: 0.0082s\n",
      "Epoch: 0006 loss_train: 1.8667 acc_train: 0.2714 loss_val: 1.8465 acc_val: 0.3500 time: 0.0096s\n",
      "Epoch: 0007 loss_train: 1.8544 acc_train: 0.2643 loss_val: 1.8308 acc_val: 0.3500 time: 0.0088s\n",
      "Epoch: 0008 loss_train: 1.8250 acc_train: 0.2929 loss_val: 1.8168 acc_val: 0.3500 time: 0.0084s\n",
      "Epoch: 0009 loss_train: 1.8142 acc_train: 0.3143 loss_val: 1.8048 acc_val: 0.3500 time: 0.0093s\n",
      "Epoch: 0010 loss_train: 1.8091 acc_train: 0.2929 loss_val: 1.7951 acc_val: 0.3500 time: 0.0112s\n",
      "Epoch: 0011 loss_train: 1.8011 acc_train: 0.2929 loss_val: 1.7876 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0012 loss_train: 1.8162 acc_train: 0.2929 loss_val: 1.7823 acc_val: 0.3500 time: 0.0107s\n",
      "Epoch: 0013 loss_train: 1.7986 acc_train: 0.2929 loss_val: 1.7787 acc_val: 0.3500 time: 0.0087s\n",
      "Epoch: 0014 loss_train: 1.7847 acc_train: 0.2929 loss_val: 1.7759 acc_val: 0.3500 time: 0.0091s\n",
      "Epoch: 0015 loss_train: 1.8204 acc_train: 0.2929 loss_val: 1.7739 acc_val: 0.3500 time: 0.0163s\n",
      "Epoch: 0016 loss_train: 1.8146 acc_train: 0.2929 loss_val: 1.7720 acc_val: 0.3500 time: 0.0091s\n",
      "Epoch: 0017 loss_train: 1.7959 acc_train: 0.2929 loss_val: 1.7703 acc_val: 0.3500 time: 0.0098s\n",
      "Epoch: 0018 loss_train: 1.7985 acc_train: 0.2929 loss_val: 1.7685 acc_val: 0.3500 time: 0.0098s\n",
      "Epoch: 0019 loss_train: 1.7739 acc_train: 0.3000 loss_val: 1.7667 acc_val: 0.3500 time: 0.0158s\n",
      "Epoch: 0020 loss_train: 1.8090 acc_train: 0.2929 loss_val: 1.7650 acc_val: 0.3500 time: 0.0137s\n",
      "Epoch: 0021 loss_train: 1.7620 acc_train: 0.2929 loss_val: 1.7623 acc_val: 0.3500 time: 0.0141s\n",
      "Epoch: 0022 loss_train: 1.7725 acc_train: 0.2857 loss_val: 1.7590 acc_val: 0.3500 time: 0.0088s\n",
      "Epoch: 0023 loss_train: 1.7584 acc_train: 0.2929 loss_val: 1.7545 acc_val: 0.3500 time: 0.0109s\n",
      "Epoch: 0024 loss_train: 1.7359 acc_train: 0.3000 loss_val: 1.7488 acc_val: 0.3500 time: 0.0104s\n",
      "Epoch: 0025 loss_train: 1.7579 acc_train: 0.3000 loss_val: 1.7424 acc_val: 0.3500 time: 0.0088s\n",
      "Epoch: 0026 loss_train: 1.7446 acc_train: 0.3214 loss_val: 1.7348 acc_val: 0.3500 time: 0.0094s\n",
      "Epoch: 0027 loss_train: 1.7156 acc_train: 0.3357 loss_val: 1.7257 acc_val: 0.3567 time: 0.0093s\n",
      "Epoch: 0028 loss_train: 1.7220 acc_train: 0.3571 loss_val: 1.7155 acc_val: 0.3633 time: 0.0098s\n",
      "Epoch: 0029 loss_train: 1.6691 acc_train: 0.3714 loss_val: 1.7034 acc_val: 0.3633 time: 0.0092s\n",
      "Epoch: 0030 loss_train: 1.7010 acc_train: 0.3643 loss_val: 1.6897 acc_val: 0.3633 time: 0.0110s\n",
      "Epoch: 0031 loss_train: 1.6428 acc_train: 0.3500 loss_val: 1.6745 acc_val: 0.3633 time: 0.0092s\n",
      "Epoch: 0032 loss_train: 1.6536 acc_train: 0.3500 loss_val: 1.6584 acc_val: 0.3633 time: 0.0088s\n",
      "Epoch: 0033 loss_train: 1.6154 acc_train: 0.3929 loss_val: 1.6410 acc_val: 0.3633 time: 0.0088s\n",
      "Epoch: 0034 loss_train: 1.6063 acc_train: 0.3643 loss_val: 1.6223 acc_val: 0.3600 time: 0.0093s\n",
      "Epoch: 0035 loss_train: 1.5771 acc_train: 0.3643 loss_val: 1.6021 acc_val: 0.3633 time: 0.0094s\n",
      "Epoch: 0036 loss_train: 1.5293 acc_train: 0.4500 loss_val: 1.5800 acc_val: 0.3800 time: 0.0085s\n",
      "Epoch: 0037 loss_train: 1.5506 acc_train: 0.4286 loss_val: 1.5563 acc_val: 0.4100 time: 0.0090s\n",
      "Epoch: 0038 loss_train: 1.4647 acc_train: 0.4500 loss_val: 1.5318 acc_val: 0.4433 time: 0.0101s\n",
      "Epoch: 0039 loss_train: 1.4646 acc_train: 0.4500 loss_val: 1.5061 acc_val: 0.4767 time: 0.0109s\n",
      "Epoch: 0040 loss_train: 1.4200 acc_train: 0.4857 loss_val: 1.4794 acc_val: 0.4833 time: 0.0092s\n",
      "Epoch: 0041 loss_train: 1.3884 acc_train: 0.5714 loss_val: 1.4513 acc_val: 0.4933 time: 0.0119s\n",
      "Epoch: 0042 loss_train: 1.3403 acc_train: 0.5214 loss_val: 1.4222 acc_val: 0.5067 time: 0.0091s\n",
      "Epoch: 0043 loss_train: 1.3361 acc_train: 0.5571 loss_val: 1.3927 acc_val: 0.5200 time: 0.0094s\n",
      "Epoch: 0044 loss_train: 1.3084 acc_train: 0.5571 loss_val: 1.3630 acc_val: 0.5333 time: 0.0093s\n",
      "Epoch: 0045 loss_train: 1.2282 acc_train: 0.6071 loss_val: 1.3329 acc_val: 0.5533 time: 0.0099s\n",
      "Epoch: 0046 loss_train: 1.2022 acc_train: 0.6000 loss_val: 1.3025 acc_val: 0.5800 time: 0.0084s\n",
      "Epoch: 0047 loss_train: 1.1593 acc_train: 0.6071 loss_val: 1.2733 acc_val: 0.5900 time: 0.0093s\n",
      "Epoch: 0048 loss_train: 1.1521 acc_train: 0.6214 loss_val: 1.2458 acc_val: 0.5900 time: 0.0078s\n",
      "Epoch: 0049 loss_train: 1.1173 acc_train: 0.6214 loss_val: 1.2208 acc_val: 0.5933 time: 0.0105s\n",
      "Epoch: 0050 loss_train: 1.0789 acc_train: 0.6071 loss_val: 1.1972 acc_val: 0.5900 time: 0.0088s\n",
      "Epoch: 0051 loss_train: 1.1042 acc_train: 0.6071 loss_val: 1.1751 acc_val: 0.5867 time: 0.0084s\n",
      "Epoch: 0052 loss_train: 1.0268 acc_train: 0.6071 loss_val: 1.1474 acc_val: 0.5933 time: 0.0102s\n",
      "Epoch: 0053 loss_train: 1.0088 acc_train: 0.6214 loss_val: 1.1162 acc_val: 0.6067 time: 0.0080s\n",
      "Epoch: 0054 loss_train: 0.9619 acc_train: 0.6643 loss_val: 1.0926 acc_val: 0.6100 time: 0.0090s\n",
      "Epoch: 0055 loss_train: 0.9809 acc_train: 0.6786 loss_val: 1.0732 acc_val: 0.6267 time: 0.0089s\n",
      "Epoch: 0056 loss_train: 0.9438 acc_train: 0.6714 loss_val: 1.0580 acc_val: 0.6300 time: 0.0096s\n",
      "Epoch: 0057 loss_train: 0.9012 acc_train: 0.6786 loss_val: 1.0502 acc_val: 0.6300 time: 0.0088s\n",
      "Epoch: 0058 loss_train: 0.8908 acc_train: 0.7071 loss_val: 1.0456 acc_val: 0.6300 time: 0.0091s\n",
      "Epoch: 0059 loss_train: 0.8729 acc_train: 0.7214 loss_val: 1.0306 acc_val: 0.6500 time: 0.0108s\n",
      "Epoch: 0060 loss_train: 0.8309 acc_train: 0.7143 loss_val: 1.0137 acc_val: 0.6633 time: 0.0113s\n",
      "Epoch: 0061 loss_train: 0.8307 acc_train: 0.6929 loss_val: 0.9981 acc_val: 0.6667 time: 0.0093s\n",
      "Epoch: 0062 loss_train: 0.8141 acc_train: 0.7143 loss_val: 0.9847 acc_val: 0.6767 time: 0.0112s\n",
      "Epoch: 0063 loss_train: 0.8243 acc_train: 0.6857 loss_val: 0.9709 acc_val: 0.6767 time: 0.0092s\n",
      "Epoch: 0064 loss_train: 0.7867 acc_train: 0.7286 loss_val: 0.9638 acc_val: 0.6933 time: 0.0111s\n",
      "Epoch: 0065 loss_train: 0.7912 acc_train: 0.7000 loss_val: 0.9572 acc_val: 0.6900 time: 0.0101s\n",
      "Epoch: 0066 loss_train: 0.7853 acc_train: 0.7143 loss_val: 0.9491 acc_val: 0.7000 time: 0.0097s\n",
      "Epoch: 0067 loss_train: 0.7202 acc_train: 0.7714 loss_val: 0.9433 acc_val: 0.7067 time: 0.0123s\n",
      "Epoch: 0068 loss_train: 0.7249 acc_train: 0.7643 loss_val: 0.9423 acc_val: 0.7033 time: 0.0154s\n",
      "Epoch: 0069 loss_train: 0.7333 acc_train: 0.7571 loss_val: 0.9421 acc_val: 0.7100 time: 0.0212s\n",
      "Epoch: 0070 loss_train: 0.6831 acc_train: 0.7857 loss_val: 0.9408 acc_val: 0.7133 time: 0.0137s\n",
      "Epoch: 0071 loss_train: 0.6929 acc_train: 0.7571 loss_val: 0.9336 acc_val: 0.7100 time: 0.0107s\n",
      "Epoch: 0072 loss_train: 0.6835 acc_train: 0.7429 loss_val: 0.9276 acc_val: 0.7133 time: 0.0128s\n",
      "Epoch: 0073 loss_train: 0.7141 acc_train: 0.7571 loss_val: 0.9194 acc_val: 0.7100 time: 0.0117s\n",
      "Epoch: 0074 loss_train: 0.6198 acc_train: 0.8214 loss_val: 0.9146 acc_val: 0.7100 time: 0.0103s\n",
      "Epoch: 0075 loss_train: 0.6692 acc_train: 0.7929 loss_val: 0.9179 acc_val: 0.7133 time: 0.0106s\n",
      "Epoch: 0076 loss_train: 0.6141 acc_train: 0.8071 loss_val: 0.9157 acc_val: 0.7100 time: 0.0101s\n",
      "Epoch: 0077 loss_train: 0.6245 acc_train: 0.7929 loss_val: 0.9117 acc_val: 0.7067 time: 0.0104s\n",
      "Epoch: 0078 loss_train: 0.5712 acc_train: 0.8000 loss_val: 0.9064 acc_val: 0.7067 time: 0.0127s\n",
      "Epoch: 0079 loss_train: 0.5991 acc_train: 0.8000 loss_val: 0.9062 acc_val: 0.6967 time: 0.0106s\n",
      "Epoch: 0080 loss_train: 0.5703 acc_train: 0.7857 loss_val: 0.8997 acc_val: 0.7033 time: 0.0107s\n",
      "Epoch: 0081 loss_train: 0.5548 acc_train: 0.8286 loss_val: 0.8950 acc_val: 0.7100 time: 0.0102s\n",
      "Epoch: 0082 loss_train: 0.5827 acc_train: 0.8214 loss_val: 0.8881 acc_val: 0.7167 time: 0.0095s\n",
      "Epoch: 0083 loss_train: 0.5461 acc_train: 0.8357 loss_val: 0.8816 acc_val: 0.7167 time: 0.0103s\n",
      "Epoch: 0084 loss_train: 0.5308 acc_train: 0.8429 loss_val: 0.8775 acc_val: 0.7200 time: 0.0095s\n",
      "Epoch: 0085 loss_train: 0.5261 acc_train: 0.8071 loss_val: 0.8754 acc_val: 0.7533 time: 0.0102s\n",
      "Epoch: 0086 loss_train: 0.5223 acc_train: 0.8357 loss_val: 0.8778 acc_val: 0.7567 time: 0.0100s\n",
      "Epoch: 0087 loss_train: 0.5023 acc_train: 0.8714 loss_val: 0.8746 acc_val: 0.7567 time: 0.0101s\n",
      "Epoch: 0088 loss_train: 0.4871 acc_train: 0.8571 loss_val: 0.8637 acc_val: 0.7633 time: 0.0103s\n",
      "Epoch: 0089 loss_train: 0.5142 acc_train: 0.8500 loss_val: 0.8540 acc_val: 0.7567 time: 0.0098s\n",
      "Epoch: 0090 loss_train: 0.4789 acc_train: 0.8643 loss_val: 0.8488 acc_val: 0.7533 time: 0.0097s\n",
      "Epoch: 0091 loss_train: 0.4464 acc_train: 0.8929 loss_val: 0.8458 acc_val: 0.7567 time: 0.0104s\n",
      "Epoch: 0092 loss_train: 0.4607 acc_train: 0.8857 loss_val: 0.8388 acc_val: 0.7600 time: 0.0101s\n",
      "Epoch: 0093 loss_train: 0.4895 acc_train: 0.8500 loss_val: 0.8436 acc_val: 0.7767 time: 0.0116s\n",
      "Epoch: 0094 loss_train: 0.4623 acc_train: 0.8500 loss_val: 0.8494 acc_val: 0.7700 time: 0.0103s\n",
      "Epoch: 0095 loss_train: 0.4675 acc_train: 0.8714 loss_val: 0.8357 acc_val: 0.7767 time: 0.0111s\n",
      "Epoch: 0096 loss_train: 0.3918 acc_train: 0.9071 loss_val: 0.8245 acc_val: 0.7700 time: 0.0114s\n",
      "Epoch: 0097 loss_train: 0.3953 acc_train: 0.8857 loss_val: 0.8339 acc_val: 0.7700 time: 0.0101s\n",
      "Epoch: 0098 loss_train: 0.4048 acc_train: 0.8929 loss_val: 0.8444 acc_val: 0.7700 time: 0.0139s\n",
      "Epoch: 0099 loss_train: 0.4186 acc_train: 0.8857 loss_val: 0.8408 acc_val: 0.7733 time: 0.0112s\n",
      "Epoch: 0100 loss_train: 0.4235 acc_train: 0.8571 loss_val: 0.8360 acc_val: 0.7667 time: 0.0121s\n",
      "Epoch: 0101 loss_train: 0.3976 acc_train: 0.9071 loss_val: 0.8247 acc_val: 0.7767 time: 0.0094s\n",
      "Epoch: 0102 loss_train: 0.3752 acc_train: 0.9071 loss_val: 0.8144 acc_val: 0.7767 time: 0.0099s\n",
      "Epoch: 0103 loss_train: 0.4402 acc_train: 0.8286 loss_val: 0.8137 acc_val: 0.7800 time: 0.0099s\n",
      "Epoch: 0104 loss_train: 0.3675 acc_train: 0.8929 loss_val: 0.8159 acc_val: 0.7767 time: 0.0125s\n",
      "Epoch: 0105 loss_train: 0.3608 acc_train: 0.8857 loss_val: 0.8225 acc_val: 0.7700 time: 0.0112s\n",
      "Epoch: 0106 loss_train: 0.4132 acc_train: 0.8714 loss_val: 0.8247 acc_val: 0.7800 time: 0.0111s\n",
      "Epoch: 0107 loss_train: 0.3335 acc_train: 0.9214 loss_val: 0.8228 acc_val: 0.7833 time: 0.0126s\n",
      "Epoch: 0108 loss_train: 0.3686 acc_train: 0.9000 loss_val: 0.8193 acc_val: 0.7867 time: 0.0128s\n",
      "Epoch: 0109 loss_train: 0.3470 acc_train: 0.8929 loss_val: 0.8223 acc_val: 0.7933 time: 0.0167s\n",
      "Epoch: 0110 loss_train: 0.3520 acc_train: 0.9000 loss_val: 0.8290 acc_val: 0.7867 time: 0.0190s\n",
      "Epoch: 0111 loss_train: 0.3322 acc_train: 0.9071 loss_val: 0.8433 acc_val: 0.7767 time: 0.0140s\n",
      "Epoch: 0112 loss_train: 0.3386 acc_train: 0.9000 loss_val: 0.8499 acc_val: 0.7767 time: 0.0192s\n",
      "Epoch: 0113 loss_train: 0.3325 acc_train: 0.9071 loss_val: 0.8437 acc_val: 0.7833 time: 0.0233s\n",
      "Epoch: 0114 loss_train: 0.3531 acc_train: 0.9286 loss_val: 0.8296 acc_val: 0.8000 time: 0.0202s\n",
      "Epoch: 0115 loss_train: 0.3343 acc_train: 0.9214 loss_val: 0.8168 acc_val: 0.7767 time: 0.0183s\n",
      "Epoch: 0116 loss_train: 0.3520 acc_train: 0.9143 loss_val: 0.8106 acc_val: 0.7833 time: 0.0160s\n",
      "Epoch: 0117 loss_train: 0.3136 acc_train: 0.9000 loss_val: 0.8115 acc_val: 0.7867 time: 0.0156s\n",
      "Epoch: 0118 loss_train: 0.3541 acc_train: 0.8929 loss_val: 0.8109 acc_val: 0.7933 time: 0.0140s\n",
      "Epoch: 0119 loss_train: 0.2778 acc_train: 0.9500 loss_val: 0.8151 acc_val: 0.8000 time: 0.0156s\n",
      "Epoch: 0120 loss_train: 0.2871 acc_train: 0.9214 loss_val: 0.8262 acc_val: 0.7900 time: 0.0110s\n",
      "Epoch: 0121 loss_train: 0.3030 acc_train: 0.9357 loss_val: 0.8393 acc_val: 0.7867 time: 0.0119s\n",
      "Epoch: 0122 loss_train: 0.3285 acc_train: 0.8857 loss_val: 0.8332 acc_val: 0.7800 time: 0.0121s\n",
      "Epoch: 0123 loss_train: 0.2874 acc_train: 0.9000 loss_val: 0.8215 acc_val: 0.7833 time: 0.0101s\n",
      "Epoch: 0124 loss_train: 0.3070 acc_train: 0.9000 loss_val: 0.8149 acc_val: 0.7833 time: 0.0116s\n",
      "Epoch: 0125 loss_train: 0.2792 acc_train: 0.9571 loss_val: 0.8176 acc_val: 0.7767 time: 0.0111s\n",
      "Epoch: 0126 loss_train: 0.3187 acc_train: 0.9286 loss_val: 0.8192 acc_val: 0.7867 time: 0.0093s\n",
      "Epoch: 0127 loss_train: 0.3110 acc_train: 0.9071 loss_val: 0.8274 acc_val: 0.7933 time: 0.0100s\n",
      "Epoch: 0128 loss_train: 0.2718 acc_train: 0.9286 loss_val: 0.8393 acc_val: 0.7900 time: 0.0109s\n",
      "Epoch: 0129 loss_train: 0.2666 acc_train: 0.9357 loss_val: 0.8449 acc_val: 0.7967 time: 0.0101s\n",
      "Epoch: 0130 loss_train: 0.3110 acc_train: 0.9143 loss_val: 0.8420 acc_val: 0.7900 time: 0.0129s\n",
      "Epoch: 0131 loss_train: 0.2758 acc_train: 0.9357 loss_val: 0.8361 acc_val: 0.8000 time: 0.0119s\n",
      "Epoch: 0132 loss_train: 0.2572 acc_train: 0.9500 loss_val: 0.8306 acc_val: 0.8000 time: 0.0089s\n",
      "Epoch: 0133 loss_train: 0.2538 acc_train: 0.9357 loss_val: 0.8280 acc_val: 0.7967 time: 0.0108s\n",
      "Epoch: 0134 loss_train: 0.2794 acc_train: 0.9214 loss_val: 0.8306 acc_val: 0.8067 time: 0.0088s\n",
      "Epoch: 0135 loss_train: 0.2514 acc_train: 0.9500 loss_val: 0.8356 acc_val: 0.8067 time: 0.0099s\n",
      "Epoch: 0136 loss_train: 0.2695 acc_train: 0.9214 loss_val: 0.8398 acc_val: 0.8033 time: 0.0102s\n",
      "Epoch: 0137 loss_train: 0.2707 acc_train: 0.9357 loss_val: 0.8488 acc_val: 0.8033 time: 0.0093s\n",
      "Epoch: 0138 loss_train: 0.2410 acc_train: 0.9429 loss_val: 0.8738 acc_val: 0.8033 time: 0.0098s\n",
      "Epoch: 0139 loss_train: 0.2524 acc_train: 0.9357 loss_val: 0.8794 acc_val: 0.8067 time: 0.0112s\n",
      "Epoch: 0140 loss_train: 0.2864 acc_train: 0.9286 loss_val: 0.8537 acc_val: 0.8033 time: 0.0116s\n",
      "Epoch: 0141 loss_train: 0.2509 acc_train: 0.9500 loss_val: 0.8484 acc_val: 0.7900 time: 0.0106s\n",
      "Epoch: 0142 loss_train: 0.3026 acc_train: 0.9286 loss_val: 0.8511 acc_val: 0.7767 time: 0.0090s\n",
      "Epoch: 0143 loss_train: 0.2427 acc_train: 0.9643 loss_val: 0.8563 acc_val: 0.7800 time: 0.0094s\n",
      "Epoch: 0144 loss_train: 0.2462 acc_train: 0.9286 loss_val: 0.8581 acc_val: 0.7833 time: 0.0090s\n",
      "Epoch: 0145 loss_train: 0.2532 acc_train: 0.9429 loss_val: 0.8705 acc_val: 0.7900 time: 0.0088s\n",
      "Epoch: 0146 loss_train: 0.2308 acc_train: 0.9500 loss_val: 0.8790 acc_val: 0.7967 time: 0.0103s\n",
      "Epoch: 0147 loss_train: 0.2392 acc_train: 0.9286 loss_val: 0.8787 acc_val: 0.8067 time: 0.0079s\n",
      "Epoch: 0148 loss_train: 0.2328 acc_train: 0.9357 loss_val: 0.8751 acc_val: 0.7900 time: 0.0093s\n",
      "Epoch: 0149 loss_train: 0.2472 acc_train: 0.9643 loss_val: 0.8645 acc_val: 0.7900 time: 0.0081s\n",
      "Epoch: 0150 loss_train: 0.2350 acc_train: 0.9643 loss_val: 0.8494 acc_val: 0.7867 time: 0.0105s\n",
      "Epoch: 0151 loss_train: 0.2500 acc_train: 0.9357 loss_val: 0.8479 acc_val: 0.8033 time: 0.0108s\n",
      "Epoch: 0152 loss_train: 0.2323 acc_train: 0.9071 loss_val: 0.8542 acc_val: 0.8033 time: 0.0099s\n",
      "Epoch: 0153 loss_train: 0.2265 acc_train: 0.9571 loss_val: 0.8549 acc_val: 0.8100 time: 0.0105s\n",
      "Epoch: 0154 loss_train: 0.2377 acc_train: 0.9357 loss_val: 0.8583 acc_val: 0.8133 time: 0.0079s\n",
      "Epoch: 0155 loss_train: 0.2282 acc_train: 0.9429 loss_val: 0.8619 acc_val: 0.8067 time: 0.0101s\n",
      "Epoch: 0156 loss_train: 0.2206 acc_train: 0.9571 loss_val: 0.8599 acc_val: 0.8000 time: 0.0084s\n",
      "Epoch: 0157 loss_train: 0.2347 acc_train: 0.9500 loss_val: 0.8532 acc_val: 0.7967 time: 0.0095s\n",
      "Epoch: 0158 loss_train: 0.2775 acc_train: 0.9286 loss_val: 0.8467 acc_val: 0.8133 time: 0.0091s\n",
      "Epoch: 0159 loss_train: 0.2339 acc_train: 0.9357 loss_val: 0.8462 acc_val: 0.8067 time: 0.0090s\n",
      "Epoch: 0160 loss_train: 0.1859 acc_train: 0.9643 loss_val: 0.8449 acc_val: 0.8100 time: 0.0111s\n",
      "Epoch: 0161 loss_train: 0.2605 acc_train: 0.9143 loss_val: 0.8439 acc_val: 0.8067 time: 0.0097s\n",
      "Epoch: 0162 loss_train: 0.2090 acc_train: 0.9500 loss_val: 0.8446 acc_val: 0.8067 time: 0.0114s\n",
      "Epoch: 0163 loss_train: 0.2573 acc_train: 0.9357 loss_val: 0.8515 acc_val: 0.8067 time: 0.0106s\n",
      "Epoch: 0164 loss_train: 0.2337 acc_train: 0.9500 loss_val: 0.8617 acc_val: 0.8033 time: 0.0091s\n",
      "Epoch: 0165 loss_train: 0.1820 acc_train: 0.9643 loss_val: 0.8708 acc_val: 0.8000 time: 0.0103s\n",
      "Epoch: 0166 loss_train: 0.2048 acc_train: 0.9286 loss_val: 0.8668 acc_val: 0.8000 time: 0.0082s\n",
      "Epoch: 0167 loss_train: 0.2226 acc_train: 0.9714 loss_val: 0.8564 acc_val: 0.8067 time: 0.0092s\n",
      "Epoch: 0168 loss_train: 0.1809 acc_train: 0.9571 loss_val: 0.8493 acc_val: 0.8067 time: 0.0086s\n",
      "Epoch: 0169 loss_train: 0.1977 acc_train: 0.9643 loss_val: 0.8467 acc_val: 0.8067 time: 0.0099s\n",
      "Epoch: 0170 loss_train: 0.2383 acc_train: 0.9500 loss_val: 0.8468 acc_val: 0.8067 time: 0.0099s\n",
      "Epoch: 0171 loss_train: 0.1798 acc_train: 0.9500 loss_val: 0.8478 acc_val: 0.8000 time: 0.0104s\n",
      "Epoch: 0172 loss_train: 0.2013 acc_train: 0.9643 loss_val: 0.8544 acc_val: 0.8000 time: 0.0102s\n",
      "Epoch: 0173 loss_train: 0.2097 acc_train: 0.9357 loss_val: 0.8605 acc_val: 0.8000 time: 0.0147s\n",
      "Epoch: 0174 loss_train: 0.2295 acc_train: 0.9286 loss_val: 0.8546 acc_val: 0.8067 time: 0.0095s\n",
      "Epoch: 0175 loss_train: 0.1980 acc_train: 0.9571 loss_val: 0.8511 acc_val: 0.8100 time: 0.0105s\n",
      "Epoch: 0176 loss_train: 0.1982 acc_train: 0.9571 loss_val: 0.8465 acc_val: 0.8067 time: 0.0105s\n",
      "Epoch: 0177 loss_train: 0.1947 acc_train: 0.9429 loss_val: 0.8424 acc_val: 0.8033 time: 0.0088s\n",
      "Epoch: 0178 loss_train: 0.1805 acc_train: 0.9571 loss_val: 0.8406 acc_val: 0.8100 time: 0.0113s\n",
      "Epoch: 0179 loss_train: 0.1862 acc_train: 0.9571 loss_val: 0.8367 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0180 loss_train: 0.1988 acc_train: 0.9643 loss_val: 0.8350 acc_val: 0.8100 time: 0.0115s\n",
      "Epoch: 0181 loss_train: 0.1910 acc_train: 0.9714 loss_val: 0.8456 acc_val: 0.7900 time: 0.0128s\n",
      "Epoch: 0182 loss_train: 0.2237 acc_train: 0.9429 loss_val: 0.8550 acc_val: 0.8000 time: 0.0119s\n",
      "Epoch: 0183 loss_train: 0.1997 acc_train: 0.9429 loss_val: 0.8523 acc_val: 0.8167 time: 0.0133s\n",
      "Epoch: 0184 loss_train: 0.1830 acc_train: 0.9429 loss_val: 0.8562 acc_val: 0.8033 time: 0.0146s\n",
      "Epoch: 0185 loss_train: 0.1918 acc_train: 0.9643 loss_val: 0.8568 acc_val: 0.8067 time: 0.0117s\n",
      "Epoch: 0186 loss_train: 0.1950 acc_train: 0.9500 loss_val: 0.8356 acc_val: 0.8200 time: 0.0121s\n",
      "Epoch: 0187 loss_train: 0.1709 acc_train: 0.9643 loss_val: 0.8251 acc_val: 0.8300 time: 0.0132s\n",
      "Epoch: 0188 loss_train: 0.1588 acc_train: 0.9786 loss_val: 0.8305 acc_val: 0.8067 time: 0.0134s\n",
      "Epoch: 0189 loss_train: 0.2216 acc_train: 0.9500 loss_val: 0.8325 acc_val: 0.8067 time: 0.0140s\n",
      "Epoch: 0190 loss_train: 0.1888 acc_train: 0.9643 loss_val: 0.8330 acc_val: 0.8133 time: 0.0187s\n",
      "Epoch: 0191 loss_train: 0.2177 acc_train: 0.9643 loss_val: 0.8332 acc_val: 0.8167 time: 0.0150s\n",
      "Epoch: 0192 loss_train: 0.1809 acc_train: 0.9714 loss_val: 0.8457 acc_val: 0.8033 time: 0.0141s\n",
      "Epoch: 0193 loss_train: 0.2087 acc_train: 0.9214 loss_val: 0.8697 acc_val: 0.7967 time: 0.0125s\n",
      "Epoch: 0194 loss_train: 0.1781 acc_train: 0.9571 loss_val: 0.8717 acc_val: 0.7967 time: 0.0129s\n",
      "Epoch: 0195 loss_train: 0.1801 acc_train: 0.9571 loss_val: 0.8556 acc_val: 0.7900 time: 0.0131s\n",
      "Epoch: 0196 loss_train: 0.1806 acc_train: 0.9500 loss_val: 0.8441 acc_val: 0.8033 time: 0.0109s\n",
      "Epoch: 0197 loss_train: 0.1477 acc_train: 0.9929 loss_val: 0.8410 acc_val: 0.8033 time: 0.0116s\n",
      "Epoch: 0198 loss_train: 0.1461 acc_train: 0.9714 loss_val: 0.8489 acc_val: 0.8033 time: 0.0102s\n",
      "Epoch: 0199 loss_train: 0.2079 acc_train: 0.9571 loss_val: 0.8495 acc_val: 0.8067 time: 0.0097s\n",
      "Epoch: 0200 loss_train: 0.1811 acc_train: 0.9714 loss_val: 0.8549 acc_val: 0.7933 time: 0.0089s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 2.2312s\n",
      "Test set results: loss= 0.7465 accuracy= 0.7880\n",
      "inference time:  0.0028867721557617188\n"
     ]
    }
   ],
   "source": [
    "run_experiment(num_epochs=num_epochs, model=model1, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runrunrun!\n",
      "Epoch: 0001 loss_train: 1.9433 acc_train: 0.1857 loss_val: 2.3318 acc_val: 0.3500 time: 0.6795s\n",
      "Epoch: 0002 loss_train: 2.4178 acc_train: 0.2929 loss_val: 1.9849 acc_val: 0.1267 time: 0.7625s\n",
      "Epoch: 0003 loss_train: 1.9770 acc_train: 0.1357 loss_val: 1.9384 acc_val: 0.1567 time: 0.6224s\n",
      "Epoch: 0004 loss_train: 1.9376 acc_train: 0.2214 loss_val: 1.9257 acc_val: 0.3467 time: 0.6431s\n",
      "Epoch: 0005 loss_train: 1.9244 acc_train: 0.3071 loss_val: 1.8787 acc_val: 0.3500 time: 0.6035s\n",
      "Epoch: 0006 loss_train: 1.8806 acc_train: 0.2786 loss_val: 1.8154 acc_val: 0.3500 time: 0.6098s\n",
      "Epoch: 0007 loss_train: 1.8079 acc_train: 0.2786 loss_val: 1.8537 acc_val: 0.3500 time: 0.6281s\n",
      "Epoch: 0008 loss_train: 1.8752 acc_train: 0.2929 loss_val: 1.7720 acc_val: 0.3500 time: 0.6449s\n",
      "Epoch: 0009 loss_train: 1.7858 acc_train: 0.2929 loss_val: 1.7980 acc_val: 0.3500 time: 0.6176s\n",
      "Epoch: 0010 loss_train: 1.8074 acc_train: 0.2929 loss_val: 1.8009 acc_val: 0.3500 time: 0.6209s\n",
      "Epoch: 0011 loss_train: 1.8084 acc_train: 0.2929 loss_val: 1.7709 acc_val: 0.3500 time: 0.6708s\n",
      "Epoch: 0012 loss_train: 1.7720 acc_train: 0.2929 loss_val: 1.7027 acc_val: 0.3500 time: 0.7201s\n",
      "Epoch: 0013 loss_train: 1.7104 acc_train: 0.2929 loss_val: 1.6362 acc_val: 0.3500 time: 0.9236s\n",
      "Epoch: 0014 loss_train: 1.6380 acc_train: 0.2929 loss_val: 1.5801 acc_val: 0.3500 time: 0.6019s\n",
      "Epoch: 0015 loss_train: 1.5922 acc_train: 0.2929 loss_val: 1.5200 acc_val: 0.3500 time: 0.6327s\n",
      "Epoch: 0016 loss_train: 1.4974 acc_train: 0.2929 loss_val: 1.5098 acc_val: 0.3433 time: 0.6594s\n",
      "Epoch: 0017 loss_train: 1.4600 acc_train: 0.2857 loss_val: 1.4783 acc_val: 0.4233 time: 0.6907s\n",
      "Epoch: 0018 loss_train: 1.4271 acc_train: 0.4071 loss_val: 1.4216 acc_val: 0.4300 time: 0.7048s\n",
      "Epoch: 0019 loss_train: 1.4001 acc_train: 0.4071 loss_val: 1.4100 acc_val: 0.4267 time: 0.5933s\n",
      "Epoch: 0020 loss_train: 1.4117 acc_train: 0.4071 loss_val: 1.4334 acc_val: 0.4233 time: 0.7838s\n",
      "Epoch: 0021 loss_train: 1.3726 acc_train: 0.4071 loss_val: 1.4341 acc_val: 0.4433 time: 0.8006s\n",
      "Epoch: 0022 loss_train: 1.3432 acc_train: 0.4357 loss_val: 1.4114 acc_val: 0.4367 time: 0.7676s\n",
      "Epoch: 0023 loss_train: 1.3229 acc_train: 0.4357 loss_val: 1.4105 acc_val: 0.4300 time: 0.7095s\n",
      "Epoch: 0024 loss_train: 1.3160 acc_train: 0.4214 loss_val: 1.4422 acc_val: 0.4333 time: 0.6614s\n",
      "Epoch: 0025 loss_train: 1.2983 acc_train: 0.4571 loss_val: 1.4254 acc_val: 0.4267 time: 0.6608s\n",
      "Epoch: 0026 loss_train: 1.2746 acc_train: 0.4214 loss_val: 1.4319 acc_val: 0.4267 time: 0.6670s\n",
      "Epoch: 0027 loss_train: 1.2314 acc_train: 0.4429 loss_val: 1.4113 acc_val: 0.4167 time: 0.6838s\n",
      "Epoch: 0028 loss_train: 1.2239 acc_train: 0.4786 loss_val: 1.3951 acc_val: 0.4233 time: 0.6638s\n",
      "Epoch: 0029 loss_train: 1.1601 acc_train: 0.4571 loss_val: 1.4472 acc_val: 0.4367 time: 0.6686s\n",
      "Epoch: 0030 loss_train: 1.1844 acc_train: 0.4286 loss_val: 1.4447 acc_val: 0.4500 time: 0.7050s\n",
      "Epoch: 0031 loss_train: 1.1007 acc_train: 0.5357 loss_val: 1.4420 acc_val: 0.5100 time: 0.6813s\n",
      "Epoch: 0032 loss_train: 1.0817 acc_train: 0.5429 loss_val: 1.4909 acc_val: 0.4933 time: 0.6681s\n",
      "Epoch: 0033 loss_train: 1.0461 acc_train: 0.5214 loss_val: 1.6830 acc_val: 0.3333 time: 0.6846s\n",
      "Epoch: 0034 loss_train: 1.1965 acc_train: 0.5000 loss_val: 1.4933 acc_val: 0.5067 time: 0.6730s\n",
      "Epoch: 0035 loss_train: 0.9836 acc_train: 0.5500 loss_val: 1.7511 acc_val: 0.4833 time: 0.6729s\n",
      "Epoch: 0036 loss_train: 1.1332 acc_train: 0.5000 loss_val: 1.6666 acc_val: 0.4100 time: 0.7092s\n",
      "Epoch: 0037 loss_train: 1.1073 acc_train: 0.4786 loss_val: 1.7024 acc_val: 0.3800 time: 0.6803s\n",
      "Epoch: 0038 loss_train: 1.0616 acc_train: 0.5214 loss_val: 1.4787 acc_val: 0.5300 time: 0.6951s\n",
      "Epoch: 0039 loss_train: 0.9340 acc_train: 0.5857 loss_val: 1.5329 acc_val: 0.4900 time: 0.7245s\n",
      "Epoch: 0040 loss_train: 0.9718 acc_train: 0.5929 loss_val: 1.5019 acc_val: 0.5100 time: 0.8857s\n",
      "Epoch: 0041 loss_train: 0.9303 acc_train: 0.5857 loss_val: 1.5836 acc_val: 0.4333 time: 0.6695s\n",
      "Epoch: 0042 loss_train: 0.9338 acc_train: 0.5571 loss_val: 1.5786 acc_val: 0.4400 time: 0.7866s\n",
      "Epoch: 0043 loss_train: 0.8999 acc_train: 0.5714 loss_val: 1.4663 acc_val: 0.5067 time: 0.6158s\n",
      "Epoch: 0044 loss_train: 0.8059 acc_train: 0.6143 loss_val: 1.4527 acc_val: 0.5167 time: 0.6448s\n",
      "Epoch: 0045 loss_train: 0.8109 acc_train: 0.6571 loss_val: 1.4625 acc_val: 0.5267 time: 0.7600s\n",
      "Epoch: 0046 loss_train: 0.7841 acc_train: 0.6143 loss_val: 1.4272 acc_val: 0.5933 time: 0.6757s\n",
      "Epoch: 0047 loss_train: 0.7137 acc_train: 0.6786 loss_val: 1.4524 acc_val: 0.5900 time: 0.6453s\n",
      "Epoch: 0048 loss_train: 0.7217 acc_train: 0.7071 loss_val: 1.4794 acc_val: 0.6233 time: 0.6970s\n",
      "Epoch: 0049 loss_train: 0.6684 acc_train: 0.7357 loss_val: 1.4641 acc_val: 0.6200 time: 0.6303s\n",
      "Epoch: 0050 loss_train: 0.6274 acc_train: 0.7500 loss_val: 1.4786 acc_val: 0.6133 time: 0.6358s\n",
      "Epoch: 0051 loss_train: 0.6586 acc_train: 0.7214 loss_val: 1.4626 acc_val: 0.5867 time: 0.7050s\n",
      "Epoch: 0052 loss_train: 0.5901 acc_train: 0.7071 loss_val: 1.4360 acc_val: 0.6267 time: 0.6797s\n",
      "Epoch: 0053 loss_train: 0.5904 acc_train: 0.7071 loss_val: 1.4813 acc_val: 0.6200 time: 0.6416s\n",
      "Epoch: 0054 loss_train: 0.5874 acc_train: 0.8000 loss_val: 1.4603 acc_val: 0.6433 time: 0.6602s\n",
      "Epoch: 0055 loss_train: 0.5483 acc_train: 0.7929 loss_val: 1.7346 acc_val: 0.6133 time: 0.7493s\n",
      "Epoch: 0056 loss_train: 0.5414 acc_train: 0.7429 loss_val: 1.5844 acc_val: 0.6500 time: 0.6365s\n",
      "Epoch: 0057 loss_train: 0.4671 acc_train: 0.8143 loss_val: 1.4653 acc_val: 0.6400 time: 0.6048s\n",
      "Epoch: 0058 loss_train: 0.5305 acc_train: 0.7571 loss_val: 1.4255 acc_val: 0.6667 time: 0.6793s\n",
      "Epoch: 0059 loss_train: 0.4769 acc_train: 0.7857 loss_val: 1.4749 acc_val: 0.6633 time: 0.6511s\n",
      "Epoch: 0060 loss_train: 0.3785 acc_train: 0.8357 loss_val: 1.6309 acc_val: 0.6400 time: 0.7187s\n",
      "Epoch: 0061 loss_train: 0.3763 acc_train: 0.8286 loss_val: 1.7788 acc_val: 0.6567 time: 0.7708s\n",
      "Epoch: 0062 loss_train: 0.3652 acc_train: 0.8429 loss_val: 1.6891 acc_val: 0.6500 time: 0.8955s\n",
      "Epoch: 0063 loss_train: 0.3510 acc_train: 0.8286 loss_val: 1.6376 acc_val: 0.6967 time: 0.7085s\n",
      "Epoch: 0064 loss_train: 0.2976 acc_train: 0.8714 loss_val: 1.6997 acc_val: 0.6967 time: 0.7941s\n",
      "Epoch: 0065 loss_train: 0.3893 acc_train: 0.8500 loss_val: 1.9364 acc_val: 0.6467 time: 0.8002s\n",
      "Epoch: 0066 loss_train: 0.3038 acc_train: 0.8857 loss_val: 2.3311 acc_val: 0.6000 time: 0.7068s\n",
      "Epoch: 0067 loss_train: 0.2901 acc_train: 0.8714 loss_val: 2.0015 acc_val: 0.6867 time: 0.6782s\n",
      "Epoch: 0068 loss_train: 0.2018 acc_train: 0.9357 loss_val: 1.8569 acc_val: 0.7133 time: 0.7266s\n",
      "Epoch: 0069 loss_train: 0.4463 acc_train: 0.8500 loss_val: 1.8154 acc_val: 0.6867 time: 0.6905s\n",
      "Epoch: 0070 loss_train: 0.4670 acc_train: 0.8214 loss_val: 1.9345 acc_val: 0.6633 time: 0.6912s\n",
      "Epoch: 0071 loss_train: 0.4142 acc_train: 0.8500 loss_val: 2.4159 acc_val: 0.6733 time: 0.6678s\n",
      "Epoch: 0072 loss_train: 0.4365 acc_train: 0.8786 loss_val: 2.0640 acc_val: 0.7133 time: 0.6802s\n",
      "Epoch: 0073 loss_train: 0.3098 acc_train: 0.8786 loss_val: 1.4129 acc_val: 0.7500 time: 0.6667s\n",
      "Epoch: 0074 loss_train: 0.1620 acc_train: 0.9286 loss_val: 1.3942 acc_val: 0.7400 time: 0.7051s\n",
      "Epoch: 0075 loss_train: 0.2469 acc_train: 0.9000 loss_val: 1.3633 acc_val: 0.7333 time: 0.6789s\n",
      "Epoch: 0076 loss_train: 0.2890 acc_train: 0.8786 loss_val: 1.2719 acc_val: 0.7433 time: 0.6750s\n",
      "Epoch: 0077 loss_train: 0.2367 acc_train: 0.8929 loss_val: 1.2913 acc_val: 0.7300 time: 0.6892s\n",
      "Epoch: 0078 loss_train: 0.1792 acc_train: 0.9214 loss_val: 1.4261 acc_val: 0.7133 time: 0.6684s\n",
      "Epoch: 0079 loss_train: 0.2025 acc_train: 0.9286 loss_val: 1.3350 acc_val: 0.7300 time: 0.6591s\n",
      "Epoch: 0080 loss_train: 0.1479 acc_train: 0.9571 loss_val: 1.2626 acc_val: 0.7500 time: 0.6682s\n",
      "Epoch: 0081 loss_train: 0.1330 acc_train: 0.9643 loss_val: 1.2495 acc_val: 0.7433 time: 0.6707s\n",
      "Epoch: 0082 loss_train: 0.1272 acc_train: 0.9429 loss_val: 1.2697 acc_val: 0.7400 time: 0.6742s\n",
      "Epoch: 0083 loss_train: 0.1471 acc_train: 0.9500 loss_val: 1.3475 acc_val: 0.7400 time: 0.6609s\n",
      "Epoch: 0084 loss_train: 0.1548 acc_train: 0.9214 loss_val: 1.4001 acc_val: 0.7467 time: 0.6605s\n",
      "Epoch: 0085 loss_train: 0.1572 acc_train: 0.9214 loss_val: 1.5365 acc_val: 0.7433 time: 0.6775s\n",
      "Epoch: 0086 loss_train: 0.1082 acc_train: 0.9429 loss_val: 1.5817 acc_val: 0.7400 time: 0.6633s\n",
      "Epoch: 0087 loss_train: 0.1087 acc_train: 0.9500 loss_val: 1.4876 acc_val: 0.7600 time: 0.6642s\n",
      "Epoch: 0088 loss_train: 0.0979 acc_train: 0.9500 loss_val: 1.4256 acc_val: 0.7567 time: 0.6581s\n",
      "Epoch: 0089 loss_train: 0.1040 acc_train: 0.9429 loss_val: 1.4540 acc_val: 0.7533 time: 0.6854s\n",
      "Epoch: 0090 loss_train: 0.0879 acc_train: 0.9500 loss_val: 1.5429 acc_val: 0.7400 time: 0.6562s\n",
      "Epoch: 0091 loss_train: 0.0661 acc_train: 0.9929 loss_val: 1.6554 acc_val: 0.7467 time: 0.6841s\n",
      "Epoch: 0092 loss_train: 0.0747 acc_train: 0.9786 loss_val: 1.6701 acc_val: 0.7467 time: 0.6634s\n",
      "Epoch: 0093 loss_train: 0.0753 acc_train: 0.9714 loss_val: 1.6603 acc_val: 0.7367 time: 0.6589s\n",
      "Epoch: 0094 loss_train: 0.0659 acc_train: 0.9714 loss_val: 1.6519 acc_val: 0.7367 time: 0.6757s\n",
      "Epoch: 0095 loss_train: 0.0762 acc_train: 0.9857 loss_val: 1.5630 acc_val: 0.7500 time: 0.8678s\n",
      "Epoch: 0096 loss_train: 0.0641 acc_train: 0.9643 loss_val: 1.5894 acc_val: 0.7533 time: 0.7017s\n",
      "Epoch: 0097 loss_train: 0.0553 acc_train: 0.9857 loss_val: 1.6482 acc_val: 0.7767 time: 0.7301s\n",
      "Epoch: 0098 loss_train: 0.0724 acc_train: 0.9643 loss_val: 1.6340 acc_val: 0.7600 time: 0.8088s\n",
      "Epoch: 0099 loss_train: 0.0403 acc_train: 0.9857 loss_val: 1.6991 acc_val: 0.7467 time: 0.6685s\n",
      "Epoch: 0100 loss_train: 0.0641 acc_train: 0.9714 loss_val: 1.7567 acc_val: 0.7467 time: 0.6983s\n",
      "Epoch: 0101 loss_train: 0.0606 acc_train: 0.9857 loss_val: 1.6495 acc_val: 0.7767 time: 0.7419s\n",
      "Epoch: 0102 loss_train: 0.0512 acc_train: 0.9857 loss_val: 1.6846 acc_val: 0.7833 time: 0.8054s\n",
      "Epoch: 0103 loss_train: 0.0760 acc_train: 0.9571 loss_val: 1.5547 acc_val: 0.7700 time: 0.9154s\n",
      "Epoch: 0104 loss_train: 0.0337 acc_train: 0.9929 loss_val: 1.6681 acc_val: 0.7367 time: 0.7889s\n",
      "Epoch: 0105 loss_train: 0.0645 acc_train: 0.9786 loss_val: 1.6934 acc_val: 0.7500 time: 0.8455s\n",
      "Epoch: 0106 loss_train: 0.0433 acc_train: 1.0000 loss_val: 1.7013 acc_val: 0.7467 time: 0.7677s\n",
      "Epoch: 0107 loss_train: 0.0432 acc_train: 0.9929 loss_val: 1.6459 acc_val: 0.7533 time: 0.7633s\n",
      "Epoch: 0108 loss_train: 0.0468 acc_train: 0.9929 loss_val: 1.5827 acc_val: 0.7400 time: 0.6836s\n",
      "Epoch: 0109 loss_train: 0.0390 acc_train: 0.9929 loss_val: 1.5809 acc_val: 0.7400 time: 0.6257s\n",
      "Epoch: 0110 loss_train: 0.0357 acc_train: 0.9857 loss_val: 1.6230 acc_val: 0.7400 time: 0.6676s\n",
      "Epoch: 0111 loss_train: 0.0416 acc_train: 0.9929 loss_val: 1.8064 acc_val: 0.7400 time: 0.9968s\n",
      "Epoch: 0112 loss_train: 0.0726 acc_train: 0.9786 loss_val: 1.6505 acc_val: 0.7433 time: 0.7377s\n",
      "Epoch: 0113 loss_train: 0.0280 acc_train: 1.0000 loss_val: 1.5673 acc_val: 0.7400 time: 0.6640s\n",
      "Epoch: 0114 loss_train: 0.0431 acc_train: 0.9857 loss_val: 1.5144 acc_val: 0.7433 time: 0.6630s\n",
      "Epoch: 0115 loss_train: 0.0674 acc_train: 0.9786 loss_val: 1.5733 acc_val: 0.7567 time: 0.6483s\n",
      "Epoch: 0116 loss_train: 0.0365 acc_train: 0.9929 loss_val: 1.6073 acc_val: 0.7667 time: 0.6513s\n",
      "Epoch: 0117 loss_train: 0.0795 acc_train: 0.9643 loss_val: 1.5687 acc_val: 0.7567 time: 0.6619s\n",
      "Epoch: 0118 loss_train: 0.0568 acc_train: 0.9786 loss_val: 1.5740 acc_val: 0.7333 time: 0.6501s\n",
      "Epoch: 0119 loss_train: 0.0338 acc_train: 0.9857 loss_val: 1.5260 acc_val: 0.7367 time: 0.6430s\n",
      "Epoch: 0120 loss_train: 0.0487 acc_train: 0.9857 loss_val: 1.5750 acc_val: 0.7500 time: 0.6424s\n",
      "Epoch: 0121 loss_train: 0.0416 acc_train: 0.9929 loss_val: 1.7644 acc_val: 0.7433 time: 0.6458s\n",
      "Epoch: 0122 loss_train: 0.1049 acc_train: 0.9714 loss_val: 1.5066 acc_val: 0.7500 time: 0.6415s\n",
      "Epoch: 0123 loss_train: 0.0324 acc_train: 0.9929 loss_val: 1.4654 acc_val: 0.7433 time: 0.6436s\n",
      "Epoch: 0124 loss_train: 0.0814 acc_train: 0.9643 loss_val: 1.4447 acc_val: 0.7600 time: 0.6360s\n",
      "Epoch: 0125 loss_train: 0.0327 acc_train: 0.9929 loss_val: 1.5514 acc_val: 0.7600 time: 0.6525s\n",
      "Epoch: 0126 loss_train: 0.0338 acc_train: 0.9929 loss_val: 1.5641 acc_val: 0.7567 time: 0.6522s\n",
      "Epoch: 0127 loss_train: 0.0357 acc_train: 0.9929 loss_val: 1.5316 acc_val: 0.7467 time: 0.7766s\n",
      "Epoch: 0128 loss_train: 0.0215 acc_train: 1.0000 loss_val: 1.5008 acc_val: 0.7533 time: 0.8611s\n",
      "Epoch: 0129 loss_train: 0.0232 acc_train: 1.0000 loss_val: 1.5142 acc_val: 0.7567 time: 0.7193s\n",
      "Epoch: 0130 loss_train: 0.0224 acc_train: 1.0000 loss_val: 1.5323 acc_val: 0.7467 time: 0.8779s\n",
      "Epoch: 0131 loss_train: 0.0255 acc_train: 0.9929 loss_val: 1.5688 acc_val: 0.7533 time: 0.7362s\n",
      "Epoch: 0132 loss_train: 0.0187 acc_train: 1.0000 loss_val: 1.6476 acc_val: 0.7533 time: 0.6667s\n",
      "Epoch: 0133 loss_train: 0.0328 acc_train: 0.9929 loss_val: 1.6915 acc_val: 0.7467 time: 0.6742s\n",
      "Epoch: 0134 loss_train: 0.0560 acc_train: 0.9857 loss_val: 1.5860 acc_val: 0.7600 time: 0.8213s\n",
      "Epoch: 0135 loss_train: 0.0239 acc_train: 0.9929 loss_val: 1.5018 acc_val: 0.7633 time: 0.7489s\n",
      "Epoch: 0136 loss_train: 0.0173 acc_train: 1.0000 loss_val: 1.5273 acc_val: 0.7633 time: 0.6792s\n",
      "Epoch: 0137 loss_train: 0.0372 acc_train: 0.9929 loss_val: 1.4856 acc_val: 0.7600 time: 0.6528s\n",
      "Epoch: 0138 loss_train: 0.0354 acc_train: 0.9857 loss_val: 1.6432 acc_val: 0.7067 time: 0.6829s\n",
      "Epoch: 0139 loss_train: 0.1162 acc_train: 0.9571 loss_val: 1.4549 acc_val: 0.7667 time: 0.7037s\n",
      "Epoch: 0140 loss_train: 0.0293 acc_train: 0.9929 loss_val: 1.5682 acc_val: 0.7667 time: 0.9798s\n",
      "Epoch: 0141 loss_train: 0.1298 acc_train: 0.9500 loss_val: 1.7299 acc_val: 0.7033 time: 1.0367s\n",
      "Epoch: 0142 loss_train: 0.0533 acc_train: 0.9929 loss_val: 2.1069 acc_val: 0.6467 time: 0.8489s\n",
      "Epoch: 0143 loss_train: 0.3013 acc_train: 0.8857 loss_val: 1.5975 acc_val: 0.7467 time: 0.8979s\n",
      "Epoch: 0144 loss_train: 0.1122 acc_train: 0.9571 loss_val: 2.2627 acc_val: 0.6767 time: 0.7729s\n",
      "Epoch: 0145 loss_train: 0.6555 acc_train: 0.7571 loss_val: 1.6359 acc_val: 0.6867 time: 0.7464s\n",
      "Epoch: 0146 loss_train: 0.1995 acc_train: 0.9214 loss_val: 3.2998 acc_val: 0.3467 time: 0.7178s\n",
      "Epoch: 0147 loss_train: 1.6129 acc_train: 0.4643 loss_val: 1.2030 acc_val: 0.7300 time: 0.9785s\n",
      "Epoch: 0148 loss_train: 0.0858 acc_train: 0.9929 loss_val: 1.2105 acc_val: 0.7733 time: 1.0845s\n",
      "Epoch: 0149 loss_train: 0.2497 acc_train: 0.8929 loss_val: 1.7043 acc_val: 0.6767 time: 0.7486s\n",
      "Epoch: 0150 loss_train: 0.6428 acc_train: 0.7714 loss_val: 1.2986 acc_val: 0.7467 time: 0.7587s\n",
      "Epoch: 0151 loss_train: 0.4421 acc_train: 0.8429 loss_val: 0.9570 acc_val: 0.7667 time: 0.8351s\n",
      "Epoch: 0152 loss_train: 0.2473 acc_train: 0.9429 loss_val: 0.9588 acc_val: 0.7367 time: 0.7525s\n",
      "Epoch: 0153 loss_train: 0.2642 acc_train: 0.9214 loss_val: 0.9176 acc_val: 0.7267 time: 0.7451s\n",
      "Epoch: 0154 loss_train: 0.2067 acc_train: 0.9500 loss_val: 0.8842 acc_val: 0.7533 time: 0.7980s\n",
      "Epoch: 0155 loss_train: 0.1607 acc_train: 0.9786 loss_val: 0.9351 acc_val: 0.7767 time: 0.6953s\n",
      "Epoch: 0156 loss_train: 0.1640 acc_train: 0.9500 loss_val: 0.9394 acc_val: 0.7933 time: 0.8594s\n",
      "Epoch: 0157 loss_train: 0.1310 acc_train: 0.9429 loss_val: 0.9018 acc_val: 0.8067 time: 0.7296s\n",
      "Epoch: 0158 loss_train: 0.1182 acc_train: 0.9643 loss_val: 0.9351 acc_val: 0.8033 time: 0.7463s\n",
      "Epoch: 0159 loss_train: 0.0936 acc_train: 0.9786 loss_val: 1.0611 acc_val: 0.8000 time: 1.0889s\n",
      "Epoch: 0160 loss_train: 0.0902 acc_train: 0.9643 loss_val: 1.2099 acc_val: 0.8033 time: 0.8060s\n",
      "Epoch: 0161 loss_train: 0.0823 acc_train: 0.9786 loss_val: 1.1569 acc_val: 0.8033 time: 0.9213s\n",
      "Epoch: 0162 loss_train: 0.0880 acc_train: 0.9714 loss_val: 1.1193 acc_val: 0.7900 time: 0.9129s\n",
      "Epoch: 0163 loss_train: 0.0383 acc_train: 0.9714 loss_val: 1.1178 acc_val: 0.7933 time: 0.7707s\n",
      "Epoch: 0164 loss_train: 0.0484 acc_train: 0.9786 loss_val: 1.2060 acc_val: 0.7967 time: 0.6913s\n",
      "Epoch: 0165 loss_train: 0.0778 acc_train: 0.9643 loss_val: 1.2180 acc_val: 0.7900 time: 0.6953s\n",
      "Epoch: 0166 loss_train: 0.0593 acc_train: 0.9857 loss_val: 1.1455 acc_val: 0.7967 time: 0.6494s\n",
      "Epoch: 0167 loss_train: 0.0512 acc_train: 0.9786 loss_val: 1.1575 acc_val: 0.7967 time: 0.7574s\n",
      "Epoch: 0168 loss_train: 0.0328 acc_train: 0.9857 loss_val: 1.1969 acc_val: 0.8000 time: 0.6900s\n",
      "Epoch: 0169 loss_train: 0.0180 acc_train: 0.9929 loss_val: 1.2565 acc_val: 0.8000 time: 0.7047s\n",
      "Epoch: 0170 loss_train: 0.0232 acc_train: 0.9929 loss_val: 1.3511 acc_val: 0.7967 time: 0.7892s\n",
      "Epoch: 0171 loss_train: 0.0709 acc_train: 0.9786 loss_val: 1.2035 acc_val: 0.8067 time: 0.7814s\n",
      "Epoch: 0172 loss_train: 0.0284 acc_train: 0.9857 loss_val: 1.1281 acc_val: 0.8033 time: 0.5823s\n",
      "Epoch: 0173 loss_train: 0.0244 acc_train: 1.0000 loss_val: 1.1345 acc_val: 0.8100 time: 0.6414s\n",
      "Epoch: 0174 loss_train: 0.0234 acc_train: 0.9929 loss_val: 1.1910 acc_val: 0.8100 time: 0.5765s\n",
      "Epoch: 0175 loss_train: 0.0256 acc_train: 1.0000 loss_val: 1.3509 acc_val: 0.8067 time: 0.6086s\n",
      "Epoch: 0176 loss_train: 0.0214 acc_train: 1.0000 loss_val: 1.3563 acc_val: 0.8000 time: 0.5644s\n",
      "Epoch: 0177 loss_train: 0.0121 acc_train: 1.0000 loss_val: 1.3300 acc_val: 0.8033 time: 0.6641s\n",
      "Epoch: 0178 loss_train: 0.0277 acc_train: 0.9857 loss_val: 1.2934 acc_val: 0.8000 time: 0.6729s\n",
      "Epoch: 0179 loss_train: 0.0162 acc_train: 1.0000 loss_val: 1.2916 acc_val: 0.7933 time: 0.6253s\n",
      "Epoch: 0180 loss_train: 0.0140 acc_train: 1.0000 loss_val: 1.3092 acc_val: 0.7933 time: 0.5948s\n",
      "Epoch: 0181 loss_train: 0.0557 acc_train: 0.9786 loss_val: 1.4075 acc_val: 0.7933 time: 0.5967s\n",
      "Epoch: 0182 loss_train: 0.0081 acc_train: 1.0000 loss_val: 1.6015 acc_val: 0.7700 time: 0.5602s\n",
      "Epoch: 0183 loss_train: 0.0187 acc_train: 0.9929 loss_val: 1.6975 acc_val: 0.7567 time: 0.6080s\n",
      "Epoch: 0184 loss_train: 0.0656 acc_train: 0.9714 loss_val: 1.2919 acc_val: 0.7967 time: 0.6119s\n",
      "Epoch: 0185 loss_train: 0.0143 acc_train: 1.0000 loss_val: 1.1861 acc_val: 0.7900 time: 0.6164s\n",
      "Epoch: 0186 loss_train: 0.0310 acc_train: 0.9857 loss_val: 1.2170 acc_val: 0.7867 time: 0.6205s\n",
      "Epoch: 0187 loss_train: 0.0853 acc_train: 0.9714 loss_val: 1.1902 acc_val: 0.7900 time: 0.5961s\n",
      "Epoch: 0188 loss_train: 0.0155 acc_train: 1.0000 loss_val: 1.3077 acc_val: 0.7867 time: 0.6926s\n",
      "Epoch: 0189 loss_train: 0.0074 acc_train: 1.0000 loss_val: 1.4767 acc_val: 0.7633 time: 0.5624s\n",
      "Epoch: 0190 loss_train: 0.0231 acc_train: 0.9857 loss_val: 1.5386 acc_val: 0.7567 time: 0.6328s\n",
      "Epoch: 0191 loss_train: 0.0504 acc_train: 0.9857 loss_val: 1.4357 acc_val: 0.7667 time: 0.5706s\n",
      "Epoch: 0192 loss_train: 0.0277 acc_train: 0.9857 loss_val: 1.3198 acc_val: 0.7833 time: 0.6218s\n",
      "Epoch: 0193 loss_train: 0.0152 acc_train: 1.0000 loss_val: 1.2799 acc_val: 0.7700 time: 0.5871s\n",
      "Epoch: 0194 loss_train: 0.0221 acc_train: 0.9929 loss_val: 1.2850 acc_val: 0.7700 time: 0.6412s\n",
      "Epoch: 0195 loss_train: 0.0247 acc_train: 1.0000 loss_val: 1.2453 acc_val: 0.7667 time: 0.5965s\n",
      "Epoch: 0196 loss_train: 0.0294 acc_train: 0.9929 loss_val: 1.2627 acc_val: 0.7833 time: 0.6232s\n",
      "Epoch: 0197 loss_train: 0.0109 acc_train: 1.0000 loss_val: 1.3422 acc_val: 0.7767 time: 0.5942s\n",
      "Epoch: 0198 loss_train: 0.0127 acc_train: 1.0000 loss_val: 1.4406 acc_val: 0.7667 time: 0.5811s\n",
      "Epoch: 0199 loss_train: 0.0163 acc_train: 1.0000 loss_val: 1.4673 acc_val: 0.7667 time: 0.5805s\n",
      "Epoch: 0200 loss_train: 0.0273 acc_train: 0.9929 loss_val: 1.4060 acc_val: 0.7767 time: 0.6283s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 140.7201s\n",
      "Test set results: loss= 1.2620 accuracy= 0.7510\n",
      "inference time:  0.18983888626098633\n"
     ]
    }
   ],
   "source": [
    "run_experiment(num_epochs=num_epochs, model=model2, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  False\n"
     ]
    }
   ],
   "source": [
    "model3 = ite_GCN(nfeat=features.shape[1],\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout,\n",
    "            nite = 3,\n",
    "            allow_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runrunrun!\n",
      "Epoch: 0001 loss_train: 1.9459 acc_train: 0.1786 loss_val: 2.2295 acc_val: 0.3500 time: 0.7255s\n",
      "Epoch: 0002 loss_train: 2.2785 acc_train: 0.2929 loss_val: 1.9810 acc_val: 0.1267 time: 0.6458s\n",
      "Epoch: 0003 loss_train: 1.9734 acc_train: 0.1214 loss_val: 1.9417 acc_val: 0.1567 time: 0.6761s\n",
      "Epoch: 0004 loss_train: 1.9394 acc_train: 0.2000 loss_val: 1.9222 acc_val: 0.1567 time: 0.6179s\n",
      "Epoch: 0005 loss_train: 1.9208 acc_train: 0.2000 loss_val: 1.8523 acc_val: 0.3733 time: 0.6073s\n",
      "Epoch: 0006 loss_train: 1.8518 acc_train: 0.2071 loss_val: 1.8677 acc_val: 0.3500 time: 0.6052s\n",
      "Epoch: 0007 loss_train: 1.8882 acc_train: 0.2929 loss_val: 1.7835 acc_val: 0.3500 time: 0.6147s\n",
      "Epoch: 0008 loss_train: 1.7799 acc_train: 0.2929 loss_val: 1.7863 acc_val: 0.3500 time: 0.6192s\n",
      "Epoch: 0009 loss_train: 1.7912 acc_train: 0.2929 loss_val: 1.7620 acc_val: 0.3500 time: 0.6445s\n",
      "Epoch: 0010 loss_train: 1.7663 acc_train: 0.2929 loss_val: 1.7009 acc_val: 0.3500 time: 0.6714s\n",
      "Epoch: 0011 loss_train: 1.7175 acc_train: 0.2929 loss_val: 1.6467 acc_val: 0.3500 time: 0.6266s\n",
      "Epoch: 0012 loss_train: 1.6612 acc_train: 0.2929 loss_val: 1.5822 acc_val: 0.3500 time: 0.6062s\n",
      "Epoch: 0013 loss_train: 1.5820 acc_train: 0.2929 loss_val: 1.5361 acc_val: 0.3500 time: 0.6109s\n",
      "Epoch: 0014 loss_train: 1.5109 acc_train: 0.2929 loss_val: 1.4751 acc_val: 0.3433 time: 0.6815s\n",
      "Epoch: 0015 loss_train: 1.4365 acc_train: 0.3000 loss_val: 1.4265 acc_val: 0.3433 time: 0.6380s\n",
      "Epoch: 0016 loss_train: 1.3948 acc_train: 0.3000 loss_val: 1.4197 acc_val: 0.4000 time: 0.6505s\n",
      "Epoch: 0017 loss_train: 1.3752 acc_train: 0.3429 loss_val: 1.4454 acc_val: 0.4533 time: 0.6100s\n",
      "Epoch: 0018 loss_train: 1.3603 acc_train: 0.4357 loss_val: 1.4376 acc_val: 0.4467 time: 0.6211s\n",
      "Epoch: 0019 loss_train: 1.3405 acc_train: 0.4143 loss_val: 1.4339 acc_val: 0.4367 time: 0.7091s\n",
      "Epoch: 0020 loss_train: 1.3156 acc_train: 0.4286 loss_val: 1.4481 acc_val: 0.4433 time: 0.6722s\n",
      "Epoch: 0021 loss_train: 1.3086 acc_train: 0.4500 loss_val: 1.5197 acc_val: 0.4000 time: 0.6100s\n",
      "Epoch: 0022 loss_train: 1.3366 acc_train: 0.3571 loss_val: 1.5145 acc_val: 0.3867 time: 0.6141s\n",
      "Epoch: 0023 loss_train: 1.3266 acc_train: 0.5000 loss_val: 1.4722 acc_val: 0.4733 time: 0.6655s\n",
      "Epoch: 0024 loss_train: 1.2680 acc_train: 0.5143 loss_val: 1.4905 acc_val: 0.4333 time: 0.7936s\n",
      "Epoch: 0025 loss_train: 1.2617 acc_train: 0.4286 loss_val: 1.4748 acc_val: 0.4100 time: 0.8009s\n",
      "Epoch: 0026 loss_train: 1.2261 acc_train: 0.4571 loss_val: 1.5003 acc_val: 0.4933 time: 0.6613s\n",
      "Epoch: 0027 loss_train: 1.2325 acc_train: 0.5857 loss_val: 1.4641 acc_val: 0.4567 time: 0.6213s\n",
      "Epoch: 0028 loss_train: 1.1470 acc_train: 0.5571 loss_val: 1.5658 acc_val: 0.4367 time: 1.0242s\n",
      "Epoch: 0029 loss_train: 1.1629 acc_train: 0.4643 loss_val: 1.4979 acc_val: 0.4433 time: 0.7255s\n",
      "Epoch: 0030 loss_train: 1.0847 acc_train: 0.6000 loss_val: 1.5054 acc_val: 0.4433 time: 0.6088s\n",
      "Epoch: 0031 loss_train: 1.0570 acc_train: 0.5786 loss_val: 1.5716 acc_val: 0.5133 time: 0.6878s\n",
      "Epoch: 0032 loss_train: 1.0183 acc_train: 0.5357 loss_val: 1.4709 acc_val: 0.5233 time: 0.6073s\n",
      "Epoch: 0033 loss_train: 0.9671 acc_train: 0.5857 loss_val: 1.4740 acc_val: 0.4967 time: 0.6398s\n",
      "Epoch: 0034 loss_train: 0.9190 acc_train: 0.6143 loss_val: 1.4850 acc_val: 0.5033 time: 0.6129s\n",
      "Epoch: 0035 loss_train: 0.8781 acc_train: 0.6143 loss_val: 1.5722 acc_val: 0.5033 time: 0.6157s\n",
      "Epoch: 0036 loss_train: 0.8359 acc_train: 0.6000 loss_val: 1.5178 acc_val: 0.5067 time: 0.6036s\n",
      "Epoch: 0037 loss_train: 0.7711 acc_train: 0.6357 loss_val: 1.4994 acc_val: 0.6133 time: 0.5983s\n",
      "Epoch: 0038 loss_train: 0.7686 acc_train: 0.7214 loss_val: 1.5393 acc_val: 0.5700 time: 0.6104s\n",
      "Epoch: 0039 loss_train: 0.7501 acc_train: 0.6571 loss_val: 1.4582 acc_val: 0.6167 time: 0.6409s\n",
      "Epoch: 0040 loss_train: 0.6877 acc_train: 0.7071 loss_val: 1.4427 acc_val: 0.6100 time: 0.6491s\n",
      "Epoch: 0041 loss_train: 0.6336 acc_train: 0.7500 loss_val: 1.4942 acc_val: 0.6267 time: 0.6845s\n",
      "Epoch: 0042 loss_train: 0.6093 acc_train: 0.7429 loss_val: 1.4547 acc_val: 0.6467 time: 0.7839s\n",
      "Epoch: 0043 loss_train: 0.5310 acc_train: 0.8143 loss_val: 1.5510 acc_val: 0.6433 time: 0.5804s\n",
      "Epoch: 0044 loss_train: 0.5120 acc_train: 0.8071 loss_val: 1.7840 acc_val: 0.6533 time: 0.6033s\n",
      "Epoch: 0045 loss_train: 0.5383 acc_train: 0.7500 loss_val: 2.2557 acc_val: 0.4667 time: 0.5580s\n",
      "Epoch: 0046 loss_train: 1.1170 acc_train: 0.5714 loss_val: 1.8694 acc_val: 0.6200 time: 0.6272s\n",
      "Epoch: 0047 loss_train: 0.7824 acc_train: 0.7571 loss_val: 1.7180 acc_val: 0.6700 time: 0.6032s\n",
      "Epoch: 0048 loss_train: 0.5167 acc_train: 0.7929 loss_val: 1.6020 acc_val: 0.6800 time: 0.5451s\n",
      "Epoch: 0049 loss_train: 0.3999 acc_train: 0.8429 loss_val: 1.8481 acc_val: 0.6067 time: 0.6174s\n",
      "Epoch: 0050 loss_train: 0.5213 acc_train: 0.7929 loss_val: 1.6946 acc_val: 0.6533 time: 0.6154s\n",
      "Epoch: 0051 loss_train: 0.3964 acc_train: 0.8643 loss_val: 1.6581 acc_val: 0.6933 time: 0.5988s\n",
      "Epoch: 0052 loss_train: 0.3436 acc_train: 0.8929 loss_val: 1.6882 acc_val: 0.7033 time: 0.5268s\n",
      "Epoch: 0053 loss_train: 0.3084 acc_train: 0.8857 loss_val: 1.6049 acc_val: 0.7200 time: 0.5347s\n",
      "Epoch: 0054 loss_train: 0.2836 acc_train: 0.9143 loss_val: 1.5440 acc_val: 0.7433 time: 0.5865s\n",
      "Epoch: 0055 loss_train: 0.2469 acc_train: 0.9286 loss_val: 1.5630 acc_val: 0.7367 time: 0.5301s\n",
      "Epoch: 0056 loss_train: 0.2682 acc_train: 0.9000 loss_val: 1.5937 acc_val: 0.7333 time: 0.5600s\n",
      "Epoch: 0057 loss_train: 0.1930 acc_train: 0.9286 loss_val: 1.7125 acc_val: 0.7233 time: 0.5711s\n",
      "Epoch: 0058 loss_train: 0.1757 acc_train: 0.9357 loss_val: 1.8205 acc_val: 0.7067 time: 0.5776s\n",
      "Epoch: 0059 loss_train: 0.1935 acc_train: 0.9143 loss_val: 1.8087 acc_val: 0.7233 time: 0.5626s\n",
      "Epoch: 0060 loss_train: 0.1381 acc_train: 0.9429 loss_val: 1.7992 acc_val: 0.7333 time: 0.5520s\n",
      "Epoch: 0061 loss_train: 0.0964 acc_train: 0.9786 loss_val: 1.8774 acc_val: 0.7300 time: 0.5563s\n",
      "Epoch: 0062 loss_train: 0.1169 acc_train: 0.9571 loss_val: 1.8374 acc_val: 0.7400 time: 0.5477s\n",
      "Epoch: 0063 loss_train: 0.0896 acc_train: 0.9643 loss_val: 1.7846 acc_val: 0.7300 time: 0.5345s\n",
      "Epoch: 0064 loss_train: 0.1481 acc_train: 0.9500 loss_val: 2.1199 acc_val: 0.7400 time: 0.5279s\n",
      "Epoch: 0065 loss_train: 0.1412 acc_train: 0.9429 loss_val: 2.0499 acc_val: 0.7400 time: 0.5438s\n",
      "Epoch: 0066 loss_train: 0.0779 acc_train: 0.9714 loss_val: 1.8644 acc_val: 0.7167 time: 0.5545s\n",
      "Epoch: 0067 loss_train: 0.1511 acc_train: 0.9429 loss_val: 1.9193 acc_val: 0.7367 time: 0.5933s\n",
      "Epoch: 0068 loss_train: 0.0662 acc_train: 0.9643 loss_val: 2.2392 acc_val: 0.7300 time: 0.6276s\n",
      "Epoch: 0069 loss_train: 0.1394 acc_train: 0.9500 loss_val: 1.8610 acc_val: 0.7267 time: 0.5406s\n",
      "Epoch: 0070 loss_train: 0.0933 acc_train: 0.9714 loss_val: 1.9037 acc_val: 0.7233 time: 0.5301s\n",
      "Epoch: 0071 loss_train: 0.1122 acc_train: 0.9500 loss_val: 2.1066 acc_val: 0.7367 time: 0.5322s\n",
      "Epoch: 0072 loss_train: 0.0661 acc_train: 0.9786 loss_val: 2.2359 acc_val: 0.7267 time: 0.5832s\n",
      "Epoch: 0073 loss_train: 0.1340 acc_train: 0.9357 loss_val: 1.8118 acc_val: 0.7267 time: 0.6251s\n",
      "Epoch: 0074 loss_train: 0.0921 acc_train: 0.9643 loss_val: 1.8112 acc_val: 0.7333 time: 0.5937s\n",
      "Epoch: 0075 loss_train: 0.1043 acc_train: 0.9500 loss_val: 1.8920 acc_val: 0.7400 time: 0.5737s\n",
      "Epoch: 0076 loss_train: 0.0642 acc_train: 0.9786 loss_val: 2.0717 acc_val: 0.7267 time: 0.5378s\n",
      "Epoch: 0077 loss_train: 0.0709 acc_train: 0.9786 loss_val: 2.1179 acc_val: 0.7333 time: 0.5790s\n",
      "Epoch: 0078 loss_train: 0.0668 acc_train: 0.9857 loss_val: 1.8961 acc_val: 0.7333 time: 0.5310s\n",
      "Epoch: 0079 loss_train: 0.0445 acc_train: 0.9857 loss_val: 1.7219 acc_val: 0.7433 time: 0.5300s\n",
      "Epoch: 0080 loss_train: 0.0559 acc_train: 0.9929 loss_val: 1.6592 acc_val: 0.7467 time: 0.5412s\n",
      "Epoch: 0081 loss_train: 0.0450 acc_train: 0.9929 loss_val: 1.7455 acc_val: 0.7633 time: 0.5676s\n",
      "Epoch: 0082 loss_train: 0.0584 acc_train: 0.9786 loss_val: 1.8883 acc_val: 0.7500 time: 0.5446s\n",
      "Epoch: 0083 loss_train: 0.0544 acc_train: 0.9857 loss_val: 1.9016 acc_val: 0.7467 time: 0.5330s\n",
      "Epoch: 0084 loss_train: 0.0418 acc_train: 0.9857 loss_val: 1.9365 acc_val: 0.7400 time: 0.5405s\n",
      "Epoch: 0085 loss_train: 0.0621 acc_train: 0.9786 loss_val: 1.9387 acc_val: 0.7200 time: 0.6063s\n",
      "Epoch: 0086 loss_train: 0.0401 acc_train: 0.9929 loss_val: 1.9985 acc_val: 0.7200 time: 0.6071s\n",
      "Epoch: 0087 loss_train: 0.0578 acc_train: 0.9714 loss_val: 1.7660 acc_val: 0.7433 time: 0.5859s\n",
      "Epoch: 0088 loss_train: 0.0330 acc_train: 0.9857 loss_val: 1.6524 acc_val: 0.7633 time: 0.6066s\n",
      "Epoch: 0089 loss_train: 0.0477 acc_train: 0.9857 loss_val: 1.7162 acc_val: 0.7667 time: 0.5352s\n",
      "Epoch: 0090 loss_train: 0.0714 acc_train: 0.9857 loss_val: 1.6677 acc_val: 0.7733 time: 0.5692s\n",
      "Epoch: 0091 loss_train: 0.0344 acc_train: 1.0000 loss_val: 1.7276 acc_val: 0.7467 time: 0.6476s\n",
      "Epoch: 0092 loss_train: 0.0209 acc_train: 1.0000 loss_val: 1.8693 acc_val: 0.7400 time: 0.5954s\n",
      "Epoch: 0093 loss_train: 0.0262 acc_train: 1.0000 loss_val: 1.9278 acc_val: 0.7333 time: 0.7603s\n",
      "Epoch: 0094 loss_train: 0.0315 acc_train: 0.9929 loss_val: 1.8531 acc_val: 0.7433 time: 0.8009s\n",
      "Epoch: 0095 loss_train: 0.0302 acc_train: 0.9929 loss_val: 1.7813 acc_val: 0.7433 time: 0.6688s\n",
      "Epoch: 0096 loss_train: 0.0435 acc_train: 0.9857 loss_val: 1.6634 acc_val: 0.7533 time: 0.6686s\n",
      "Epoch: 0097 loss_train: 0.0184 acc_train: 1.0000 loss_val: 1.6255 acc_val: 0.7400 time: 0.6918s\n",
      "Epoch: 0098 loss_train: 0.0226 acc_train: 1.0000 loss_val: 1.7021 acc_val: 0.7367 time: 0.6479s\n",
      "Epoch: 0099 loss_train: 0.0256 acc_train: 1.0000 loss_val: 1.8297 acc_val: 0.7267 time: 0.6521s\n",
      "Epoch: 0100 loss_train: 0.0335 acc_train: 0.9929 loss_val: 1.7475 acc_val: 0.7467 time: 0.6984s\n",
      "Epoch: 0101 loss_train: 0.0180 acc_train: 0.9929 loss_val: 1.8463 acc_val: 0.7500 time: 0.6837s\n",
      "Epoch: 0102 loss_train: 0.0355 acc_train: 0.9857 loss_val: 1.9902 acc_val: 0.7267 time: 0.6701s\n",
      "Epoch: 0103 loss_train: 0.0220 acc_train: 1.0000 loss_val: 2.0247 acc_val: 0.7233 time: 0.7520s\n",
      "Epoch: 0104 loss_train: 0.0618 acc_train: 0.9714 loss_val: 1.7218 acc_val: 0.7300 time: 0.5969s\n",
      "Epoch: 0105 loss_train: 0.0442 acc_train: 0.9857 loss_val: 1.5629 acc_val: 0.7467 time: 0.6755s\n",
      "Epoch: 0106 loss_train: 0.0526 acc_train: 0.9786 loss_val: 1.7372 acc_val: 0.7700 time: 0.7293s\n",
      "Epoch: 0107 loss_train: 0.0203 acc_train: 1.0000 loss_val: 1.8688 acc_val: 0.7700 time: 0.7837s\n",
      "Epoch: 0108 loss_train: 0.0380 acc_train: 0.9929 loss_val: 1.7553 acc_val: 0.7500 time: 0.8915s\n",
      "Epoch: 0109 loss_train: 0.0327 acc_train: 0.9929 loss_val: 1.7837 acc_val: 0.7267 time: 0.6947s\n",
      "Epoch: 0110 loss_train: 0.0781 acc_train: 0.9714 loss_val: 1.8602 acc_val: 0.7500 time: 0.8170s\n",
      "Epoch: 0111 loss_train: 0.0334 acc_train: 0.9929 loss_val: 2.0338 acc_val: 0.7567 time: 1.0295s\n",
      "Epoch: 0112 loss_train: 0.0534 acc_train: 0.9786 loss_val: 1.6519 acc_val: 0.7567 time: 0.8082s\n",
      "Epoch: 0113 loss_train: 0.0500 acc_train: 0.9929 loss_val: 1.5444 acc_val: 0.7500 time: 0.6942s\n",
      "Epoch: 0114 loss_train: 0.0347 acc_train: 0.9857 loss_val: 1.7601 acc_val: 0.6967 time: 0.7675s\n",
      "Epoch: 0115 loss_train: 0.0849 acc_train: 0.9714 loss_val: 1.9647 acc_val: 0.7433 time: 0.6632s\n",
      "Epoch: 0116 loss_train: 0.0555 acc_train: 0.9786 loss_val: 1.9746 acc_val: 0.7633 time: 0.7896s\n",
      "Epoch: 0117 loss_train: 0.1004 acc_train: 0.9571 loss_val: 1.4335 acc_val: 0.7567 time: 0.7508s\n",
      "Epoch: 0118 loss_train: 0.0454 acc_train: 0.9786 loss_val: 1.4058 acc_val: 0.7733 time: 0.6647s\n",
      "Epoch: 0119 loss_train: 0.0751 acc_train: 0.9714 loss_val: 1.6144 acc_val: 0.7400 time: 0.6378s\n",
      "Epoch: 0120 loss_train: 0.0389 acc_train: 0.9929 loss_val: 1.9374 acc_val: 0.7367 time: 0.6503s\n",
      "Epoch: 0121 loss_train: 0.0816 acc_train: 0.9714 loss_val: 1.8122 acc_val: 0.7467 time: 0.6402s\n",
      "Epoch: 0122 loss_train: 0.0623 acc_train: 0.9857 loss_val: 1.4720 acc_val: 0.7667 time: 0.6579s\n",
      "Epoch: 0123 loss_train: 0.0316 acc_train: 0.9929 loss_val: 1.3053 acc_val: 0.7633 time: 0.6441s\n",
      "Epoch: 0124 loss_train: 0.0618 acc_train: 0.9857 loss_val: 1.3464 acc_val: 0.7667 time: 0.6450s\n",
      "Epoch: 0125 loss_train: 0.0583 acc_train: 0.9857 loss_val: 1.5751 acc_val: 0.7400 time: 0.6542s\n",
      "Epoch: 0126 loss_train: 0.0343 acc_train: 0.9929 loss_val: 1.8612 acc_val: 0.7200 time: 0.6380s\n",
      "Epoch: 0127 loss_train: 0.0516 acc_train: 0.9857 loss_val: 1.8586 acc_val: 0.7233 time: 0.6944s\n",
      "Epoch: 0128 loss_train: 0.0363 acc_train: 0.9929 loss_val: 1.7164 acc_val: 0.7233 time: 0.6560s\n",
      "Epoch: 0129 loss_train: 0.0407 acc_train: 0.9857 loss_val: 1.4180 acc_val: 0.7533 time: 0.6889s\n",
      "Epoch: 0130 loss_train: 0.0362 acc_train: 0.9929 loss_val: 1.3533 acc_val: 0.7700 time: 0.6872s\n",
      "Epoch: 0131 loss_train: 0.0418 acc_train: 0.9929 loss_val: 1.3418 acc_val: 0.7600 time: 0.6423s\n",
      "Epoch: 0132 loss_train: 0.0325 acc_train: 0.9929 loss_val: 1.5561 acc_val: 0.7333 time: 0.6447s\n",
      "Epoch: 0133 loss_train: 0.0181 acc_train: 1.0000 loss_val: 1.7930 acc_val: 0.7267 time: 0.6621s\n",
      "Epoch: 0134 loss_train: 0.0261 acc_train: 1.0000 loss_val: 1.7761 acc_val: 0.7333 time: 0.6475s\n",
      "Epoch: 0135 loss_train: 0.0229 acc_train: 1.0000 loss_val: 1.6023 acc_val: 0.7433 time: 0.7335s\n",
      "Epoch: 0136 loss_train: 0.0138 acc_train: 1.0000 loss_val: 1.5002 acc_val: 0.7567 time: 0.6478s\n",
      "Epoch: 0137 loss_train: 0.0226 acc_train: 0.9929 loss_val: 1.4925 acc_val: 0.7500 time: 0.6459s\n",
      "Epoch: 0138 loss_train: 0.0231 acc_train: 0.9929 loss_val: 1.5785 acc_val: 0.7400 time: 0.6337s\n",
      "Epoch: 0139 loss_train: 0.0179 acc_train: 0.9929 loss_val: 1.7516 acc_val: 0.7433 time: 0.6440s\n",
      "Epoch: 0140 loss_train: 0.0197 acc_train: 0.9929 loss_val: 1.8191 acc_val: 0.7400 time: 0.6587s\n",
      "Epoch: 0141 loss_train: 0.0116 acc_train: 1.0000 loss_val: 1.8026 acc_val: 0.7367 time: 0.6695s\n",
      "Epoch: 0142 loss_train: 0.0165 acc_train: 1.0000 loss_val: 1.6882 acc_val: 0.7367 time: 0.7247s\n",
      "Epoch: 0143 loss_train: 0.0215 acc_train: 0.9929 loss_val: 1.5907 acc_val: 0.7367 time: 0.9091s\n",
      "Epoch: 0144 loss_train: 0.0318 acc_train: 1.0000 loss_val: 1.5424 acc_val: 0.7467 time: 0.8255s\n",
      "Epoch: 0145 loss_train: 0.0156 acc_train: 1.0000 loss_val: 1.6725 acc_val: 0.7533 time: 0.7115s\n",
      "Epoch: 0146 loss_train: 0.0231 acc_train: 0.9929 loss_val: 1.7985 acc_val: 0.7433 time: 0.6792s\n",
      "Epoch: 0147 loss_train: 0.0127 acc_train: 1.0000 loss_val: 1.8597 acc_val: 0.7433 time: 0.5420s\n",
      "Epoch: 0148 loss_train: 0.0115 acc_train: 1.0000 loss_val: 1.8351 acc_val: 0.7400 time: 0.5454s\n",
      "Epoch: 0149 loss_train: 0.0370 acc_train: 0.9929 loss_val: 1.5940 acc_val: 0.7333 time: 0.7490s\n",
      "Epoch: 0150 loss_train: 0.0187 acc_train: 1.0000 loss_val: 1.4768 acc_val: 0.7533 time: 0.6759s\n",
      "Epoch: 0151 loss_train: 0.0379 acc_train: 0.9929 loss_val: 1.5718 acc_val: 0.7433 time: 0.6200s\n",
      "Epoch: 0152 loss_train: 0.0278 acc_train: 0.9929 loss_val: 1.8535 acc_val: 0.7300 time: 0.6766s\n",
      "Epoch: 0153 loss_train: 0.0350 acc_train: 0.9857 loss_val: 1.8253 acc_val: 0.7500 time: 0.7810s\n",
      "Epoch: 0154 loss_train: 0.0172 acc_train: 1.0000 loss_val: 1.7691 acc_val: 0.7333 time: 0.6608s\n",
      "Epoch: 0155 loss_train: 0.0708 acc_train: 0.9786 loss_val: 1.6175 acc_val: 0.7467 time: 0.9221s\n",
      "Epoch: 0156 loss_train: 0.0250 acc_train: 1.0000 loss_val: 1.3896 acc_val: 0.7700 time: 0.6630s\n",
      "Epoch: 0157 loss_train: 0.0314 acc_train: 0.9857 loss_val: 1.3026 acc_val: 0.7500 time: 0.6974s\n",
      "Epoch: 0158 loss_train: 0.0587 acc_train: 0.9857 loss_val: 1.3977 acc_val: 0.7533 time: 0.6972s\n",
      "Epoch: 0159 loss_train: 0.0377 acc_train: 0.9929 loss_val: 1.6291 acc_val: 0.7467 time: 0.6449s\n",
      "Epoch: 0160 loss_train: 0.0203 acc_train: 1.0000 loss_val: 1.8525 acc_val: 0.7233 time: 0.6119s\n",
      "Epoch: 0161 loss_train: 0.0377 acc_train: 0.9857 loss_val: 1.7267 acc_val: 0.7333 time: 0.6154s\n",
      "Epoch: 0162 loss_train: 0.0297 acc_train: 1.0000 loss_val: 1.4518 acc_val: 0.7567 time: 0.6302s\n",
      "Epoch: 0163 loss_train: 0.0286 acc_train: 0.9929 loss_val: 1.3351 acc_val: 0.7633 time: 0.6493s\n",
      "Epoch: 0164 loss_train: 0.0220 acc_train: 1.0000 loss_val: 1.3642 acc_val: 0.7533 time: 0.6460s\n",
      "Epoch: 0165 loss_train: 0.0267 acc_train: 1.0000 loss_val: 1.5254 acc_val: 0.7467 time: 0.7607s\n",
      "Epoch: 0166 loss_train: 0.0249 acc_train: 0.9929 loss_val: 1.6918 acc_val: 0.7533 time: 0.6502s\n",
      "Epoch: 0167 loss_train: 0.0271 acc_train: 1.0000 loss_val: 1.6918 acc_val: 0.7467 time: 0.7484s\n",
      "Epoch: 0168 loss_train: 0.0292 acc_train: 0.9929 loss_val: 1.6329 acc_val: 0.7367 time: 0.7490s\n",
      "Epoch: 0169 loss_train: 0.0446 acc_train: 0.9786 loss_val: 1.5280 acc_val: 0.7600 time: 1.0138s\n",
      "Epoch: 0170 loss_train: 0.0243 acc_train: 0.9929 loss_val: 1.5440 acc_val: 0.7567 time: 1.1330s\n",
      "Epoch: 0171 loss_train: 0.0495 acc_train: 0.9857 loss_val: 1.5631 acc_val: 0.7567 time: 1.1759s\n",
      "Epoch: 0172 loss_train: 0.0119 acc_train: 1.0000 loss_val: 1.8018 acc_val: 0.7133 time: 1.2325s\n",
      "Epoch: 0173 loss_train: 0.0423 acc_train: 0.9857 loss_val: 1.8968 acc_val: 0.7333 time: 0.8345s\n",
      "Epoch: 0174 loss_train: 0.0431 acc_train: 0.9929 loss_val: 1.6728 acc_val: 0.7633 time: 0.8661s\n",
      "Epoch: 0175 loss_train: 0.0223 acc_train: 0.9929 loss_val: 1.5868 acc_val: 0.7667 time: 0.7582s\n",
      "Epoch: 0176 loss_train: 0.0428 acc_train: 0.9857 loss_val: 1.4831 acc_val: 0.7667 time: 0.6867s\n",
      "Epoch: 0177 loss_train: 0.0302 acc_train: 0.9929 loss_val: 1.5867 acc_val: 0.7300 time: 0.7960s\n",
      "Epoch: 0178 loss_train: 0.0282 acc_train: 0.9929 loss_val: 1.7422 acc_val: 0.7067 time: 0.7857s\n",
      "Epoch: 0179 loss_train: 0.0668 acc_train: 0.9857 loss_val: 1.5828 acc_val: 0.7400 time: 0.7298s\n",
      "Epoch: 0180 loss_train: 0.0370 acc_train: 0.9929 loss_val: 1.4082 acc_val: 0.7667 time: 0.7105s\n",
      "Epoch: 0181 loss_train: 0.0376 acc_train: 0.9929 loss_val: 1.3625 acc_val: 0.7667 time: 0.6884s\n",
      "Epoch: 0182 loss_train: 0.0338 acc_train: 0.9929 loss_val: 1.4689 acc_val: 0.7533 time: 0.6316s\n",
      "Epoch: 0183 loss_train: 0.0224 acc_train: 0.9929 loss_val: 1.6417 acc_val: 0.7300 time: 0.6329s\n",
      "Epoch: 0184 loss_train: 0.0172 acc_train: 1.0000 loss_val: 1.7758 acc_val: 0.7367 time: 0.6375s\n",
      "Epoch: 0185 loss_train: 0.0343 acc_train: 0.9929 loss_val: 1.6085 acc_val: 0.7333 time: 0.6809s\n",
      "Epoch: 0186 loss_train: 0.0248 acc_train: 1.0000 loss_val: 1.4355 acc_val: 0.7467 time: 0.6366s\n",
      "Epoch: 0187 loss_train: 0.0368 acc_train: 0.9929 loss_val: 1.2418 acc_val: 0.7733 time: 0.6504s\n",
      "Epoch: 0188 loss_train: 0.0281 acc_train: 0.9857 loss_val: 1.3752 acc_val: 0.7800 time: 0.6270s\n",
      "Epoch: 0189 loss_train: 0.0540 acc_train: 0.9786 loss_val: 1.4672 acc_val: 0.7533 time: 0.6601s\n",
      "Epoch: 0190 loss_train: 0.0254 acc_train: 0.9929 loss_val: 1.6189 acc_val: 0.7300 time: 0.6345s\n",
      "Epoch: 0191 loss_train: 0.0395 acc_train: 0.9857 loss_val: 1.6513 acc_val: 0.7500 time: 0.6523s\n",
      "Epoch: 0192 loss_train: 0.0276 acc_train: 1.0000 loss_val: 1.5547 acc_val: 0.7633 time: 0.6281s\n",
      "Epoch: 0193 loss_train: 0.0114 acc_train: 1.0000 loss_val: 1.5132 acc_val: 0.7600 time: 0.6206s\n",
      "Epoch: 0194 loss_train: 0.0208 acc_train: 0.9929 loss_val: 1.5159 acc_val: 0.7633 time: 0.6493s\n",
      "Epoch: 0195 loss_train: 0.0158 acc_train: 1.0000 loss_val: 1.4934 acc_val: 0.7633 time: 0.6449s\n",
      "Epoch: 0196 loss_train: 0.0103 acc_train: 1.0000 loss_val: 1.5240 acc_val: 0.7633 time: 0.6370s\n",
      "Epoch: 0197 loss_train: 0.0335 acc_train: 0.9929 loss_val: 1.6719 acc_val: 0.7300 time: 0.6353s\n",
      "Epoch: 0198 loss_train: 0.0302 acc_train: 0.9929 loss_val: 1.5855 acc_val: 0.7533 time: 0.6309s\n",
      "Epoch: 0199 loss_train: 0.0184 acc_train: 1.0000 loss_val: 1.6328 acc_val: 0.7667 time: 0.6327s\n",
      "Epoch: 0200 loss_train: 0.0134 acc_train: 1.0000 loss_val: 1.5922 acc_val: 0.7667 time: 0.6271s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 132.7379s\n",
      "Test set results: loss= 1.4263 accuracy= 0.7260\n",
      "inference time:  0.16505002975463867\n"
     ]
    }
   ],
   "source": [
    "run_experiment(num_epochs=num_epochs, model=model3, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iterativeENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
