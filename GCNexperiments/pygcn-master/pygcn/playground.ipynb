{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import load_data, test, train, accuracy\n",
    "from models import GCN_2, GCN_3\n",
    "from layers import GraphConvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(path=\"../data/cora/\", dataset=\"cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 16\n",
    "dropout = 0.5\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "num_epochs = 200\n",
    "smooth_fac = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ite_GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nclass, dropout, train_nite, eval_nite=0, allow_grad=True, smooth_fac=0):\n",
    "        '''     \n",
    "        - This model is a 1-layer GCN with nite iterations, followed by a linear layer and a log_softmax\n",
    "            - GC layer:     nfeat to nfeat\n",
    "            - linear layer: nfeat to nclass, (to cast hidden representations of nodes to a dimension of nclass)\n",
    "        - Activation: ReLu\n",
    "        - Input:\n",
    "            - nfeat:        the number of features of each node\n",
    "            - nclass:       the number of target classes (we are doing a node classification task here)\n",
    "            - dropout:      dropout rate\n",
    "            - train_nite:   the number of iterations during training\n",
    "            - eval_nite:    the number of iterations during evaluation, \n",
    "                            if not specified (or invalid), intialize to the same as train_nite\n",
    "            - allow_grad:   (bool) defaulted to True. \n",
    "                            whether or nor allow gradients to flow through all GC iterations, \n",
    "                            if False, gradients will only flow to the last iteration\n",
    "            - smooth_fac:   a number in [0,1], smoothing factor, controls how much of the OLD iteration result is\n",
    "                            counted in the skip connection in each iteration\n",
    "                            for example, smooth_fac = x means y_{i+1} = x * y_i + (1-x) * y_{i+1}\n",
    "                            Invalid inputs will be treated as 0.\n",
    "        - Output:\n",
    "            - A probability vector of length nclass, by log_softmax\n",
    "        '''\n",
    "        super(ite_GCN, self).__init__()\n",
    "\n",
    "        self.gc = GraphConvolution(nfeat, nfeat)\n",
    "        self.linear_no_bias = nn.Linear(nfeat, nclass, bias=False)\n",
    "        self.dropout = dropout\n",
    "        self.train_nite = train_nite\n",
    "        self.allow_grad = allow_grad\n",
    "        self.smooth_fac = smooth_fac\n",
    "        self.eval_nite = eval_nite\n",
    "        \n",
    "        if (smooth_fac > 1) or (smooth_fac < 0):\n",
    "            print(\"Invalid smoothing factor. Treat as 0.\")\n",
    "            self.smooth_fac = 0\n",
    "        if (eval_nite <= 0):\n",
    "            print(\"Unspecified or invalid number of iterations for inference. Treat as the same as training iterations.\")\n",
    "            self.eval_nite = self.train_nite\n",
    "        \n",
    "        print(\"Initialize a 1-layer GCN with \", self.train_nite, \"iterations\")\n",
    "        print(\"Gradient flows to all iterations: \", allow_grad)\n",
    "\n",
    "    def run_one_layer(self, x, adj):\n",
    "        x_old = x\n",
    "        x_new = self.gc(x, adj)\n",
    "        x = F.relu(self.smooth_fac * x_old + (1 - self.smooth_fac) * x_new)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        if self.training:\n",
    "            for i in range(self.train_nite):\n",
    "                if not self.allow_grad:\n",
    "                    # print(\"no no no! new new\")\n",
    "                    x = x.detach()\n",
    "                    x = self.run_one_layer(x, adj)\n",
    "                    # x.requires_grad_()\n",
    "                    # self.gc.weight.requires_grad_()\n",
    "                    # self.gc.weight.retain_grad()\n",
    "                    # print(self.gc.weight.requires_grad)\n",
    "                    # for name, param in self.named_parameters():\n",
    "                    #         print(name, param.grad)\n",
    "                else:\n",
    "                    # print(\"yea yea yea\")\n",
    "                    x = self.run_one_layer(x, adj)\n",
    "                    # for name, param in self.named_parameters():\n",
    "                    #         print(name, param.grad)\n",
    "        else:\n",
    "            for i in range(self.eval_nite):\n",
    "                x = self.run_one_layer(x, adj)\n",
    "\n",
    "        x = self.linear_no_bias(x)\n",
    "        # self.gc.weight.requires_grad_()\n",
    "        # print(\"???\")\n",
    "        # for name, param in self.named_parameters():\n",
    "        #     if param.grad is not None:\n",
    "        #         print(name, param.grad.abs().sum())\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(num_epochs, model, lr, weight_decay, features, adj, idx_train, idx_val, idx_test, labels):\n",
    "    print(\"runrunrun!\")\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "    t_total = time.time()\n",
    "    loss_TRAIN = []\n",
    "    acc_TRAIN = []\n",
    "    loss_VAL = []\n",
    "    acc_VAL = []\n",
    "    for epoch in range(num_epochs):\n",
    "        t = time.time()\n",
    "    \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(features, adj)\n",
    "        \n",
    "        loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "        loss_TRAIN.append(loss_train)\n",
    "        acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "        acc_TRAIN.append(acc_train)\n",
    "\n",
    "        # t3 = time.time()\n",
    "        loss_train.backward()\n",
    "        # t4 = time.time()\n",
    "        # print(\"backward: \", t4-t3)\n",
    "        # print(\"before step: \", model.gc.weight)\n",
    "        optimizer.step()\n",
    "        # print(\"after step: \", model.gc.weight)\n",
    "\n",
    "        \n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        # t1 = time.time()\n",
    "        output = model(features, adj)\n",
    "        # print(\"eval output: \", output)\n",
    "        # t2 = time.time()\n",
    "        # print(\"forward time: \", t2-t1)\n",
    "\n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "        loss_VAL.append(loss_val)\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "        acc_VAL.append(acc_val)\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "            'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "            'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "            'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "            'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "            'time: {:.4f}s'.format(time.time() - t))\n",
    "        \n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "    # Testing\n",
    "    test(model, features, adj, idx_test, labels)\n",
    "    return loss_TRAIN, acc_TRAIN, loss_VAL, acc_VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = ite_GCN(nfeat=features.shape[1],\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout,\n",
    "            train_nite = 3,\n",
    "            allow_grad=True,\n",
    "            smooth_fac=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# totally messed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_TRAIN, acc_TRAIN, loss_VAL, acc_VAL = run_experiment(num_epochs=200, model=model3, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_t = []\n",
    "for ten in loss_TRAIN:\n",
    "    l_t.append(ten.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(l_t, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_t = []\n",
    "for ten in acc_TRAIN:\n",
    "    a_t.append(ten.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a_t, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_v = []\n",
    "for ten in loss_VAL:\n",
    "    l_v.append(ten.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(l_v, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_v = []\n",
    "for ten in acc_VAL:\n",
    "    a_v.append(ten.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a_v, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = ite_GCN(nfeat=features.shape[1],\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout,\n",
    "            train_nite = 3,\n",
    "            allow_grad=False,\n",
    "            smooth_fac=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(num_epochs=400, model=model4, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model3.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(name, param.grad.abs().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = GCN_3(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unspecified or invalid number of iterations for inference. Treat as the same as training iterations.\n",
      "Initialize a 1-layer GCN with  2 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "runrunrun!\n",
      "Epoch: 0001 loss_train: 1.9463 acc_train: 0.2000 loss_val: 1.8221 acc_val: 0.3500 time: 0.4371s\n",
      "Epoch: 0002 loss_train: 1.8292 acc_train: 0.2929 loss_val: 2.0807 acc_val: 0.3500 time: 0.3771s\n",
      "Epoch: 0003 loss_train: 2.1051 acc_train: 0.2929 loss_val: 1.7790 acc_val: 0.3500 time: 0.3797s\n",
      "Epoch: 0004 loss_train: 1.7740 acc_train: 0.2929 loss_val: 1.8437 acc_val: 0.3500 time: 0.3782s\n",
      "Epoch: 0005 loss_train: 1.8328 acc_train: 0.2929 loss_val: 1.8414 acc_val: 0.3500 time: 0.3758s\n",
      "Epoch: 0006 loss_train: 1.8273 acc_train: 0.2929 loss_val: 1.7934 acc_val: 0.3500 time: 0.3801s\n",
      "Epoch: 0007 loss_train: 1.7722 acc_train: 0.2929 loss_val: 1.7071 acc_val: 0.3500 time: 0.3750s\n",
      "Epoch: 0008 loss_train: 1.6709 acc_train: 0.2929 loss_val: 1.6272 acc_val: 0.3500 time: 0.3811s\n",
      "Epoch: 0009 loss_train: 1.5764 acc_train: 0.2929 loss_val: 1.4873 acc_val: 0.3533 time: 0.4247s\n",
      "Epoch: 0010 loss_train: 1.4373 acc_train: 0.3000 loss_val: 1.4148 acc_val: 0.4933 time: 0.3833s\n",
      "Epoch: 0011 loss_train: 1.3436 acc_train: 0.4500 loss_val: 1.3150 acc_val: 0.5467 time: 0.4583s\n",
      "Epoch: 0012 loss_train: 1.2215 acc_train: 0.5143 loss_val: 1.2526 acc_val: 0.5000 time: 0.4477s\n",
      "Epoch: 0013 loss_train: 1.1095 acc_train: 0.5286 loss_val: 1.2066 acc_val: 0.4933 time: 0.3699s\n",
      "Epoch: 0014 loss_train: 0.9965 acc_train: 0.5571 loss_val: 1.1245 acc_val: 0.5533 time: 0.3718s\n",
      "Epoch: 0015 loss_train: 0.8795 acc_train: 0.6500 loss_val: 1.0451 acc_val: 0.6933 time: 0.3703s\n",
      "Epoch: 0016 loss_train: 0.7624 acc_train: 0.7786 loss_val: 1.0192 acc_val: 0.6233 time: 0.3791s\n",
      "Epoch: 0017 loss_train: 0.6655 acc_train: 0.7500 loss_val: 0.9745 acc_val: 0.6900 time: 0.3706s\n",
      "Epoch: 0018 loss_train: 0.5517 acc_train: 0.8571 loss_val: 0.9455 acc_val: 0.6567 time: 0.3964s\n",
      "Epoch: 0019 loss_train: 0.4700 acc_train: 0.8357 loss_val: 0.9399 acc_val: 0.7167 time: 0.3745s\n",
      "Epoch: 0020 loss_train: 0.4049 acc_train: 0.8357 loss_val: 0.8533 acc_val: 0.7300 time: 0.3742s\n",
      "Epoch: 0021 loss_train: 0.3456 acc_train: 0.8857 loss_val: 0.8290 acc_val: 0.7433 time: 0.3732s\n",
      "Epoch: 0022 loss_train: 0.3004 acc_train: 0.9071 loss_val: 0.9119 acc_val: 0.7567 time: 0.4374s\n",
      "Epoch: 0023 loss_train: 0.2577 acc_train: 0.9429 loss_val: 0.8450 acc_val: 0.7900 time: 0.4775s\n",
      "Epoch: 0024 loss_train: 0.2099 acc_train: 0.9857 loss_val: 0.8731 acc_val: 0.7700 time: 0.3810s\n",
      "Epoch: 0025 loss_train: 0.1664 acc_train: 0.9929 loss_val: 0.8606 acc_val: 0.7700 time: 0.3721s\n",
      "Epoch: 0026 loss_train: 0.1231 acc_train: 1.0000 loss_val: 0.8042 acc_val: 0.8000 time: 0.3792s\n",
      "Epoch: 0027 loss_train: 0.0953 acc_train: 1.0000 loss_val: 0.8830 acc_val: 0.7833 time: 0.3762s\n",
      "Epoch: 0028 loss_train: 0.0791 acc_train: 1.0000 loss_val: 0.8111 acc_val: 0.7967 time: 0.3745s\n",
      "Epoch: 0029 loss_train: 0.0708 acc_train: 0.9929 loss_val: 0.9120 acc_val: 0.7733 time: 0.3868s\n",
      "Epoch: 0030 loss_train: 0.0609 acc_train: 0.9929 loss_val: 0.8228 acc_val: 0.8067 time: 0.3757s\n",
      "Epoch: 0031 loss_train: 0.0509 acc_train: 1.0000 loss_val: 0.8206 acc_val: 0.8033 time: 0.3749s\n",
      "Epoch: 0032 loss_train: 0.0482 acc_train: 1.0000 loss_val: 0.9086 acc_val: 0.7767 time: 0.3801s\n",
      "Epoch: 0033 loss_train: 0.0529 acc_train: 1.0000 loss_val: 0.8481 acc_val: 0.8033 time: 0.3819s\n",
      "Epoch: 0034 loss_train: 0.0559 acc_train: 1.0000 loss_val: 0.8189 acc_val: 0.7867 time: 0.3790s\n",
      "Epoch: 0035 loss_train: 0.0504 acc_train: 1.0000 loss_val: 0.7780 acc_val: 0.8167 time: 0.3790s\n",
      "Epoch: 0036 loss_train: 0.0484 acc_train: 1.0000 loss_val: 0.7921 acc_val: 0.8033 time: 0.3840s\n",
      "Epoch: 0037 loss_train: 0.0540 acc_train: 1.0000 loss_val: 0.8113 acc_val: 0.7867 time: 0.3798s\n",
      "Epoch: 0038 loss_train: 0.0595 acc_train: 1.0000 loss_val: 0.7813 acc_val: 0.8000 time: 0.3845s\n",
      "Epoch: 0039 loss_train: 0.0707 acc_train: 1.0000 loss_val: 0.9445 acc_val: 0.7633 time: 0.3827s\n",
      "Epoch: 0040 loss_train: 0.1066 acc_train: 0.9929 loss_val: 1.0739 acc_val: 0.6900 time: 0.3837s\n",
      "Epoch: 0041 loss_train: 0.2391 acc_train: 0.9143 loss_val: 1.3450 acc_val: 0.6267 time: 0.3936s\n",
      "Epoch: 0042 loss_train: 0.3590 acc_train: 0.8714 loss_val: 0.7191 acc_val: 0.8100 time: 0.3965s\n",
      "Epoch: 0043 loss_train: 0.0707 acc_train: 0.9929 loss_val: 1.1978 acc_val: 0.6800 time: 0.4372s\n",
      "Epoch: 0044 loss_train: 0.2625 acc_train: 0.9000 loss_val: 0.7521 acc_val: 0.8233 time: 0.4366s\n",
      "Epoch: 0045 loss_train: 0.0654 acc_train: 0.9929 loss_val: 0.8792 acc_val: 0.7700 time: 0.3918s\n",
      "Epoch: 0046 loss_train: 0.0658 acc_train: 0.9929 loss_val: 1.1393 acc_val: 0.6933 time: 0.3885s\n",
      "Epoch: 0047 loss_train: 0.1472 acc_train: 0.9571 loss_val: 0.9518 acc_val: 0.7500 time: 0.3935s\n",
      "Epoch: 0048 loss_train: 0.0752 acc_train: 1.0000 loss_val: 0.7375 acc_val: 0.8067 time: 0.3997s\n",
      "Epoch: 0049 loss_train: 0.0391 acc_train: 1.0000 loss_val: 0.6916 acc_val: 0.8300 time: 0.3953s\n",
      "Epoch: 0050 loss_train: 0.0410 acc_train: 1.0000 loss_val: 0.7984 acc_val: 0.8167 time: 0.3902s\n",
      "Epoch: 0051 loss_train: 0.0573 acc_train: 1.0000 loss_val: 0.8582 acc_val: 0.7900 time: 0.3844s\n",
      "Epoch: 0052 loss_train: 0.0649 acc_train: 0.9929 loss_val: 0.7490 acc_val: 0.8200 time: 0.3966s\n",
      "Epoch: 0053 loss_train: 0.0401 acc_train: 1.0000 loss_val: 0.7060 acc_val: 0.8067 time: 0.3897s\n",
      "Epoch: 0054 loss_train: 0.0288 acc_train: 1.0000 loss_val: 0.7480 acc_val: 0.8033 time: 0.3958s\n",
      "Epoch: 0055 loss_train: 0.0251 acc_train: 1.0000 loss_val: 0.8346 acc_val: 0.7800 time: 0.3893s\n",
      "Epoch: 0056 loss_train: 0.0280 acc_train: 1.0000 loss_val: 0.9100 acc_val: 0.7600 time: 0.3955s\n",
      "Epoch: 0057 loss_train: 0.0351 acc_train: 1.0000 loss_val: 0.9113 acc_val: 0.7567 time: 0.3886s\n",
      "Epoch: 0058 loss_train: 0.0340 acc_train: 1.0000 loss_val: 0.8604 acc_val: 0.7733 time: 0.3963s\n",
      "Epoch: 0059 loss_train: 0.0303 acc_train: 1.0000 loss_val: 0.7981 acc_val: 0.7800 time: 0.3939s\n",
      "Epoch: 0060 loss_train: 0.0268 acc_train: 1.0000 loss_val: 0.7562 acc_val: 0.8033 time: 0.3944s\n",
      "Epoch: 0061 loss_train: 0.0247 acc_train: 1.0000 loss_val: 0.7558 acc_val: 0.7967 time: 0.3897s\n",
      "Epoch: 0062 loss_train: 0.0279 acc_train: 1.0000 loss_val: 0.7520 acc_val: 0.8000 time: 0.3886s\n",
      "Epoch: 0063 loss_train: 0.0318 acc_train: 1.0000 loss_val: 0.7215 acc_val: 0.8067 time: 0.3933s\n",
      "Epoch: 0064 loss_train: 0.0329 acc_train: 1.0000 loss_val: 0.7084 acc_val: 0.7900 time: 0.3904s\n",
      "Epoch: 0065 loss_train: 0.0339 acc_train: 1.0000 loss_val: 0.7263 acc_val: 0.8067 time: 0.3900s\n",
      "Epoch: 0066 loss_train: 0.0334 acc_train: 1.0000 loss_val: 0.7651 acc_val: 0.7867 time: 0.3959s\n",
      "Epoch: 0067 loss_train: 0.0377 acc_train: 1.0000 loss_val: 0.7546 acc_val: 0.7867 time: 0.3980s\n",
      "Epoch: 0068 loss_train: 0.0396 acc_train: 1.0000 loss_val: 0.7186 acc_val: 0.7833 time: 0.4047s\n",
      "Epoch: 0069 loss_train: 0.0404 acc_train: 1.0000 loss_val: 0.6962 acc_val: 0.7933 time: 0.3990s\n",
      "Epoch: 0070 loss_train: 0.0409 acc_train: 1.0000 loss_val: 0.6976 acc_val: 0.8033 time: 0.4297s\n",
      "Epoch: 0071 loss_train: 0.0430 acc_train: 1.0000 loss_val: 0.6897 acc_val: 0.8133 time: 0.4012s\n",
      "Epoch: 0072 loss_train: 0.0436 acc_train: 1.0000 loss_val: 0.6745 acc_val: 0.8033 time: 0.3984s\n",
      "Epoch: 0073 loss_train: 0.0432 acc_train: 1.0000 loss_val: 0.6821 acc_val: 0.8033 time: 0.3961s\n",
      "Epoch: 0074 loss_train: 0.0434 acc_train: 1.0000 loss_val: 0.6992 acc_val: 0.8000 time: 0.3974s\n",
      "Epoch: 0075 loss_train: 0.0434 acc_train: 1.0000 loss_val: 0.6934 acc_val: 0.8000 time: 0.3979s\n",
      "Epoch: 0076 loss_train: 0.0422 acc_train: 1.0000 loss_val: 0.6788 acc_val: 0.8000 time: 0.3986s\n",
      "Epoch: 0077 loss_train: 0.0418 acc_train: 1.0000 loss_val: 0.6813 acc_val: 0.7967 time: 0.4186s\n",
      "Epoch: 0078 loss_train: 0.0406 acc_train: 1.0000 loss_val: 0.6930 acc_val: 0.8000 time: 0.4093s\n",
      "Epoch: 0079 loss_train: 0.0395 acc_train: 1.0000 loss_val: 0.6885 acc_val: 0.8033 time: 0.4298s\n",
      "Epoch: 0080 loss_train: 0.0382 acc_train: 1.0000 loss_val: 0.6824 acc_val: 0.8000 time: 0.3973s\n",
      "Epoch: 0081 loss_train: 0.0374 acc_train: 1.0000 loss_val: 0.6860 acc_val: 0.8033 time: 0.4037s\n",
      "Epoch: 0082 loss_train: 0.0361 acc_train: 1.0000 loss_val: 0.6902 acc_val: 0.8067 time: 0.4018s\n",
      "Epoch: 0083 loss_train: 0.0356 acc_train: 1.0000 loss_val: 0.6827 acc_val: 0.8033 time: 0.3948s\n",
      "Epoch: 0084 loss_train: 0.0346 acc_train: 1.0000 loss_val: 0.6840 acc_val: 0.8067 time: 0.3925s\n",
      "Epoch: 0085 loss_train: 0.0341 acc_train: 1.0000 loss_val: 0.6946 acc_val: 0.8033 time: 0.4196s\n",
      "Epoch: 0086 loss_train: 0.0337 acc_train: 1.0000 loss_val: 0.6921 acc_val: 0.8033 time: 0.3977s\n",
      "Epoch: 0087 loss_train: 0.0333 acc_train: 1.0000 loss_val: 0.6822 acc_val: 0.8033 time: 0.4025s\n",
      "Epoch: 0088 loss_train: 0.0330 acc_train: 1.0000 loss_val: 0.6844 acc_val: 0.8067 time: 0.6530s\n",
      "Epoch: 0089 loss_train: 0.0327 acc_train: 1.0000 loss_val: 0.6907 acc_val: 0.8067 time: 0.5216s\n",
      "Epoch: 0090 loss_train: 0.0326 acc_train: 1.0000 loss_val: 0.6858 acc_val: 0.7967 time: 0.4494s\n",
      "Epoch: 0091 loss_train: 0.0324 acc_train: 1.0000 loss_val: 0.6847 acc_val: 0.7967 time: 0.4501s\n",
      "Epoch: 0092 loss_train: 0.0323 acc_train: 1.0000 loss_val: 0.6910 acc_val: 0.8067 time: 0.4189s\n",
      "Epoch: 0093 loss_train: 0.0323 acc_train: 1.0000 loss_val: 0.6868 acc_val: 0.8000 time: 0.4147s\n",
      "Epoch: 0094 loss_train: 0.0322 acc_train: 1.0000 loss_val: 0.6877 acc_val: 0.8067 time: 0.4394s\n",
      "Epoch: 0095 loss_train: 0.0320 acc_train: 1.0000 loss_val: 0.6929 acc_val: 0.8067 time: 0.4349s\n",
      "Epoch: 0096 loss_train: 0.0319 acc_train: 1.0000 loss_val: 0.6836 acc_val: 0.8067 time: 0.4909s\n",
      "Epoch: 0097 loss_train: 0.0317 acc_train: 1.0000 loss_val: 0.6929 acc_val: 0.8033 time: 0.4757s\n",
      "Epoch: 0098 loss_train: 0.0314 acc_train: 1.0000 loss_val: 0.6904 acc_val: 0.8100 time: 0.4176s\n",
      "Epoch: 0099 loss_train: 0.0311 acc_train: 1.0000 loss_val: 0.6905 acc_val: 0.8033 time: 0.3913s\n",
      "Epoch: 0100 loss_train: 0.0309 acc_train: 1.0000 loss_val: 0.6943 acc_val: 0.8033 time: 0.3986s\n",
      "Epoch: 0101 loss_train: 0.0307 acc_train: 1.0000 loss_val: 0.6933 acc_val: 0.8033 time: 0.4185s\n",
      "Epoch: 0102 loss_train: 0.0312 acc_train: 1.0000 loss_val: 0.7171 acc_val: 0.7933 time: 0.4085s\n",
      "Epoch: 0103 loss_train: 0.0349 acc_train: 1.0000 loss_val: 0.7366 acc_val: 0.7867 time: 0.4099s\n",
      "Epoch: 0104 loss_train: 0.0470 acc_train: 1.0000 loss_val: 0.8377 acc_val: 0.7667 time: 0.3997s\n",
      "Epoch: 0105 loss_train: 0.0796 acc_train: 0.9929 loss_val: 1.1245 acc_val: 0.6833 time: 0.3991s\n",
      "Epoch: 0106 loss_train: 0.2061 acc_train: 0.9571 loss_val: 1.9971 acc_val: 0.5700 time: 0.4011s\n",
      "Epoch: 0107 loss_train: 1.0698 acc_train: 0.6357 loss_val: 1.6434 acc_val: 0.5533 time: 0.4268s\n",
      "Epoch: 0108 loss_train: 0.4714 acc_train: 0.8357 loss_val: 1.5030 acc_val: 0.6133 time: 0.4071s\n",
      "Epoch: 0109 loss_train: 0.3189 acc_train: 0.9214 loss_val: 0.6858 acc_val: 0.8167 time: 0.4021s\n",
      "Epoch: 0110 loss_train: 0.0463 acc_train: 1.0000 loss_val: 0.8553 acc_val: 0.7833 time: 0.4054s\n",
      "Epoch: 0111 loss_train: 0.1557 acc_train: 0.9643 loss_val: 1.0044 acc_val: 0.7567 time: 0.4002s\n",
      "Epoch: 0112 loss_train: 0.2379 acc_train: 0.9429 loss_val: 0.9305 acc_val: 0.7600 time: 0.4037s\n",
      "Epoch: 0113 loss_train: 0.1861 acc_train: 0.9571 loss_val: 0.7704 acc_val: 0.7900 time: 0.4012s\n",
      "Epoch: 0114 loss_train: 0.1070 acc_train: 0.9857 loss_val: 0.6689 acc_val: 0.7967 time: 0.3968s\n",
      "Epoch: 0115 loss_train: 0.0613 acc_train: 0.9929 loss_val: 0.6822 acc_val: 0.8133 time: 0.4032s\n",
      "Epoch: 0116 loss_train: 0.0598 acc_train: 0.9857 loss_val: 0.7447 acc_val: 0.8033 time: 0.4027s\n",
      "Epoch: 0117 loss_train: 0.0668 acc_train: 0.9786 loss_val: 0.7760 acc_val: 0.7933 time: 0.4008s\n",
      "Epoch: 0118 loss_train: 0.0604 acc_train: 0.9857 loss_val: 0.7617 acc_val: 0.7833 time: 0.4299s\n",
      "Epoch: 0119 loss_train: 0.0399 acc_train: 0.9929 loss_val: 0.7683 acc_val: 0.7900 time: 0.3955s\n",
      "Epoch: 0120 loss_train: 0.0278 acc_train: 1.0000 loss_val: 0.7984 acc_val: 0.7767 time: 0.3942s\n",
      "Epoch: 0121 loss_train: 0.0253 acc_train: 1.0000 loss_val: 0.8122 acc_val: 0.7800 time: 0.3932s\n",
      "Epoch: 0122 loss_train: 0.0239 acc_train: 1.0000 loss_val: 0.8022 acc_val: 0.7900 time: 0.3975s\n",
      "Epoch: 0123 loss_train: 0.0212 acc_train: 1.0000 loss_val: 0.7853 acc_val: 0.8067 time: 0.3964s\n",
      "Epoch: 0124 loss_train: 0.0188 acc_train: 1.0000 loss_val: 0.7705 acc_val: 0.8200 time: 0.4200s\n",
      "Epoch: 0125 loss_train: 0.0171 acc_train: 1.0000 loss_val: 0.7585 acc_val: 0.8267 time: 0.4549s\n",
      "Epoch: 0126 loss_train: 0.0160 acc_train: 1.0000 loss_val: 0.7479 acc_val: 0.8233 time: 0.3979s\n",
      "Epoch: 0127 loss_train: 0.0149 acc_train: 1.0000 loss_val: 0.7410 acc_val: 0.8267 time: 0.3991s\n",
      "Epoch: 0128 loss_train: 0.0136 acc_train: 1.0000 loss_val: 0.7461 acc_val: 0.8167 time: 0.4013s\n",
      "Epoch: 0129 loss_train: 0.0128 acc_train: 1.0000 loss_val: 0.7648 acc_val: 0.8033 time: 0.4049s\n",
      "Epoch: 0130 loss_train: 0.0135 acc_train: 1.0000 loss_val: 0.7813 acc_val: 0.8067 time: 0.4950s\n",
      "Epoch: 0131 loss_train: 0.0147 acc_train: 1.0000 loss_val: 0.7772 acc_val: 0.7967 time: 0.3925s\n",
      "Epoch: 0132 loss_train: 0.0150 acc_train: 1.0000 loss_val: 0.7632 acc_val: 0.8033 time: 0.3999s\n",
      "Epoch: 0133 loss_train: 0.0153 acc_train: 1.0000 loss_val: 0.7540 acc_val: 0.8200 time: 0.3932s\n",
      "Epoch: 0134 loss_train: 0.0167 acc_train: 1.0000 loss_val: 0.7540 acc_val: 0.8200 time: 0.3916s\n",
      "Epoch: 0135 loss_train: 0.0182 acc_train: 1.0000 loss_val: 0.7583 acc_val: 0.8067 time: 0.4008s\n",
      "Epoch: 0136 loss_train: 0.0196 acc_train: 1.0000 loss_val: 0.7520 acc_val: 0.8067 time: 0.4055s\n",
      "Epoch: 0137 loss_train: 0.0209 acc_train: 1.0000 loss_val: 0.7385 acc_val: 0.8100 time: 0.3923s\n",
      "Epoch: 0138 loss_train: 0.0220 acc_train: 1.0000 loss_val: 0.7345 acc_val: 0.8000 time: 0.3958s\n",
      "Epoch: 0139 loss_train: 0.0240 acc_train: 1.0000 loss_val: 0.7250 acc_val: 0.8000 time: 0.4060s\n",
      "Epoch: 0140 loss_train: 0.0254 acc_train: 1.0000 loss_val: 0.7203 acc_val: 0.8000 time: 0.4055s\n",
      "Epoch: 0141 loss_train: 0.0269 acc_train: 1.0000 loss_val: 0.7226 acc_val: 0.8033 time: 0.3947s\n",
      "Epoch: 0142 loss_train: 0.0289 acc_train: 1.0000 loss_val: 0.7105 acc_val: 0.8000 time: 0.3931s\n",
      "Epoch: 0143 loss_train: 0.0306 acc_train: 1.0000 loss_val: 0.7306 acc_val: 0.7933 time: 0.4222s\n",
      "Epoch: 0144 loss_train: 0.0327 acc_train: 1.0000 loss_val: 0.7018 acc_val: 0.8167 time: 0.4043s\n",
      "Epoch: 0145 loss_train: 0.0355 acc_train: 1.0000 loss_val: 0.7283 acc_val: 0.7933 time: 0.3996s\n",
      "Epoch: 0146 loss_train: 0.0360 acc_train: 1.0000 loss_val: 0.6924 acc_val: 0.8067 time: 0.3943s\n",
      "Epoch: 0147 loss_train: 0.0350 acc_train: 1.0000 loss_val: 0.6873 acc_val: 0.8167 time: 0.4016s\n",
      "Epoch: 0148 loss_train: 0.0357 acc_train: 1.0000 loss_val: 0.7142 acc_val: 0.7967 time: 0.3883s\n",
      "Epoch: 0149 loss_train: 0.0365 acc_train: 1.0000 loss_val: 0.6901 acc_val: 0.8033 time: 0.3929s\n",
      "Epoch: 0150 loss_train: 0.0351 acc_train: 1.0000 loss_val: 0.6825 acc_val: 0.8167 time: 0.3976s\n",
      "Epoch: 0151 loss_train: 0.0352 acc_train: 1.0000 loss_val: 0.7058 acc_val: 0.8033 time: 0.4184s\n",
      "Epoch: 0152 loss_train: 0.0349 acc_train: 1.0000 loss_val: 0.6971 acc_val: 0.8033 time: 0.4137s\n",
      "Epoch: 0153 loss_train: 0.0334 acc_train: 1.0000 loss_val: 0.6871 acc_val: 0.8100 time: 0.3925s\n",
      "Epoch: 0154 loss_train: 0.0338 acc_train: 1.0000 loss_val: 0.6994 acc_val: 0.8000 time: 0.3959s\n",
      "Epoch: 0155 loss_train: 0.0321 acc_train: 1.0000 loss_val: 0.7038 acc_val: 0.8100 time: 0.4003s\n",
      "Epoch: 0156 loss_train: 0.0320 acc_train: 1.0000 loss_val: 0.6876 acc_val: 0.8133 time: 0.3999s\n",
      "Epoch: 0157 loss_train: 0.0310 acc_train: 1.0000 loss_val: 0.6964 acc_val: 0.8100 time: 0.4031s\n",
      "Epoch: 0158 loss_train: 0.0304 acc_train: 1.0000 loss_val: 0.7080 acc_val: 0.8033 time: 0.3905s\n",
      "Epoch: 0159 loss_train: 0.0298 acc_train: 1.0000 loss_val: 0.6985 acc_val: 0.8167 time: 0.4010s\n",
      "Epoch: 0160 loss_train: 0.0292 acc_train: 1.0000 loss_val: 0.6970 acc_val: 0.8100 time: 0.3970s\n",
      "Epoch: 0161 loss_train: 0.0287 acc_train: 1.0000 loss_val: 0.7051 acc_val: 0.8000 time: 0.3953s\n",
      "Epoch: 0162 loss_train: 0.0282 acc_train: 1.0000 loss_val: 0.7050 acc_val: 0.8033 time: 0.3898s\n",
      "Epoch: 0163 loss_train: 0.0280 acc_train: 1.0000 loss_val: 0.7040 acc_val: 0.8067 time: 0.3903s\n",
      "Epoch: 0164 loss_train: 0.0275 acc_train: 1.0000 loss_val: 0.7031 acc_val: 0.8067 time: 0.3942s\n",
      "Epoch: 0165 loss_train: 0.0272 acc_train: 1.0000 loss_val: 0.7033 acc_val: 0.8067 time: 0.4018s\n",
      "Epoch: 0166 loss_train: 0.0271 acc_train: 1.0000 loss_val: 0.7084 acc_val: 0.8067 time: 0.4088s\n",
      "Epoch: 0167 loss_train: 0.0268 acc_train: 1.0000 loss_val: 0.7048 acc_val: 0.8067 time: 0.5382s\n",
      "Epoch: 0168 loss_train: 0.0266 acc_train: 1.0000 loss_val: 0.7046 acc_val: 0.8067 time: 0.7085s\n",
      "Epoch: 0169 loss_train: 0.0266 acc_train: 1.0000 loss_val: 0.7079 acc_val: 0.8033 time: 0.7264s\n",
      "Epoch: 0170 loss_train: 0.0264 acc_train: 1.0000 loss_val: 0.7040 acc_val: 0.8067 time: 0.3968s\n",
      "Epoch: 0171 loss_train: 0.0264 acc_train: 1.0000 loss_val: 0.7117 acc_val: 0.8033 time: 0.4003s\n",
      "Epoch: 0172 loss_train: 0.0264 acc_train: 1.0000 loss_val: 0.7030 acc_val: 0.8133 time: 0.3951s\n",
      "Epoch: 0173 loss_train: 0.0263 acc_train: 1.0000 loss_val: 0.7093 acc_val: 0.8033 time: 0.3984s\n",
      "Epoch: 0174 loss_train: 0.0263 acc_train: 1.0000 loss_val: 0.7084 acc_val: 0.8067 time: 0.4054s\n",
      "Epoch: 0175 loss_train: 0.0263 acc_train: 1.0000 loss_val: 0.7069 acc_val: 0.8067 time: 0.3959s\n",
      "Epoch: 0176 loss_train: 0.0263 acc_train: 1.0000 loss_val: 0.7069 acc_val: 0.8067 time: 0.4020s\n",
      "Epoch: 0177 loss_train: 0.0262 acc_train: 1.0000 loss_val: 0.7134 acc_val: 0.8033 time: 0.4044s\n",
      "Epoch: 0178 loss_train: 0.0263 acc_train: 1.0000 loss_val: 0.7007 acc_val: 0.8200 time: 0.4240s\n",
      "Epoch: 0179 loss_train: 0.0265 acc_train: 1.0000 loss_val: 0.7382 acc_val: 0.7933 time: 0.3926s\n",
      "Epoch: 0180 loss_train: 0.0280 acc_train: 1.0000 loss_val: 0.7030 acc_val: 0.8267 time: 0.3938s\n",
      "Epoch: 0181 loss_train: 0.0328 acc_train: 1.0000 loss_val: 0.8236 acc_val: 0.7833 time: 0.4012s\n",
      "Epoch: 0182 loss_train: 0.0406 acc_train: 1.0000 loss_val: 0.7158 acc_val: 0.8000 time: 0.4057s\n",
      "Epoch: 0183 loss_train: 0.0411 acc_train: 1.0000 loss_val: 0.7416 acc_val: 0.7933 time: 0.4125s\n",
      "Epoch: 0184 loss_train: 0.0278 acc_train: 1.0000 loss_val: 0.7903 acc_val: 0.7767 time: 0.4027s\n",
      "Epoch: 0185 loss_train: 0.0312 acc_train: 1.0000 loss_val: 0.7071 acc_val: 0.8100 time: 0.3966s\n",
      "Epoch: 0186 loss_train: 0.0277 acc_train: 1.0000 loss_val: 0.7061 acc_val: 0.8233 time: 0.4803s\n",
      "Epoch: 0187 loss_train: 0.0270 acc_train: 1.0000 loss_val: 0.7543 acc_val: 0.7867 time: 0.3922s\n",
      "Epoch: 0188 loss_train: 0.0246 acc_train: 1.0000 loss_val: 0.7810 acc_val: 0.7833 time: 0.3977s\n",
      "Epoch: 0189 loss_train: 0.0260 acc_train: 1.0000 loss_val: 0.7198 acc_val: 0.8100 time: 0.4377s\n",
      "Epoch: 0190 loss_train: 0.0229 acc_train: 1.0000 loss_val: 0.7177 acc_val: 0.8233 time: 0.4245s\n",
      "Epoch: 0191 loss_train: 0.0260 acc_train: 1.0000 loss_val: 0.7361 acc_val: 0.8000 time: 0.4145s\n",
      "Epoch: 0192 loss_train: 0.0221 acc_train: 1.0000 loss_val: 0.7778 acc_val: 0.7833 time: 0.4387s\n",
      "Epoch: 0193 loss_train: 0.0258 acc_train: 1.0000 loss_val: 0.7259 acc_val: 0.8033 time: 0.3849s\n",
      "Epoch: 0194 loss_train: 0.0224 acc_train: 1.0000 loss_val: 0.7112 acc_val: 0.8200 time: 0.3918s\n",
      "Epoch: 0195 loss_train: 0.0253 acc_train: 1.0000 loss_val: 0.7212 acc_val: 0.8067 time: 0.4059s\n",
      "Epoch: 0196 loss_train: 0.0233 acc_train: 1.0000 loss_val: 0.7536 acc_val: 0.7833 time: 0.3859s\n",
      "Epoch: 0197 loss_train: 0.0250 acc_train: 1.0000 loss_val: 0.7291 acc_val: 0.8033 time: 0.3826s\n",
      "Epoch: 0198 loss_train: 0.0244 acc_train: 1.0000 loss_val: 0.7042 acc_val: 0.8133 time: 0.3929s\n",
      "Epoch: 0199 loss_train: 0.0251 acc_train: 1.0000 loss_val: 0.7151 acc_val: 0.8100 time: 0.3920s\n",
      "Epoch: 0200 loss_train: 0.0252 acc_train: 1.0000 loss_val: 0.7368 acc_val: 0.7967 time: 0.3982s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 81.4714s\n",
      "Test set results: loss= 0.7120 accuracy= 0.7780\n",
      "inference time:  0.11055469512939453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor(1.9463, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.8292, grad_fn=<NllLossBackward0>),\n",
       "  tensor(2.1051, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.7740, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.8328, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.8273, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.7722, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.6709, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.5764, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.4373, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.3436, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.2215, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.1095, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.9965, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8795, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7624, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6655, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.5517, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.4700, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.4049, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.3456, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.3004, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.2577, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.2099, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.1664, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.1231, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0953, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0791, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0708, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0609, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0509, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0482, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0529, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0559, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0504, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0484, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0540, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0595, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0707, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.1066, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.2391, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.3590, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0707, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.2625, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0654, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0658, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.1472, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0752, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0391, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0410, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0573, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0649, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0401, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0288, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0251, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0280, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0351, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0340, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0303, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0268, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0247, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0279, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0318, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0329, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0339, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0334, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0377, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0396, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0404, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0409, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0430, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0436, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0432, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0434, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0434, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0422, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0418, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0406, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0395, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0382, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0374, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0361, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0356, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0346, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0341, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0337, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0333, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0330, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0327, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0326, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0324, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0323, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0323, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0322, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0320, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0319, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0317, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0314, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0311, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0309, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0307, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0312, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0349, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0470, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0796, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.2061, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.0698, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.4714, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.3189, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0463, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.1557, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.2379, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.1861, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.1070, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0613, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0598, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0668, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0604, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0399, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0278, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0253, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0239, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0212, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0188, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0171, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0160, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0149, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0136, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0128, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0135, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0147, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0150, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0153, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0167, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0182, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0196, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0209, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0220, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0240, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0254, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0269, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0289, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0306, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0327, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0355, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0360, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0350, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0357, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0365, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0351, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0352, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0349, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0334, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0338, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0321, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0320, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0310, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0304, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0298, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0292, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0287, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0282, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0280, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0275, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0272, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0271, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0268, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0266, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0266, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0264, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0264, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0264, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0263, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0263, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0263, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0263, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0262, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0263, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0265, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0280, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0328, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0406, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0411, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0278, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0312, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0277, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0270, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0246, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0260, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0229, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0260, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0221, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0258, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0224, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0253, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0233, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0250, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0244, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0251, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.0252, grad_fn=<NllLossBackward0>)],\n",
       " [tensor(0.2000, dtype=torch.float64),\n",
       "  tensor(0.2929, dtype=torch.float64),\n",
       "  tensor(0.2929, dtype=torch.float64),\n",
       "  tensor(0.2929, dtype=torch.float64),\n",
       "  tensor(0.2929, dtype=torch.float64),\n",
       "  tensor(0.2929, dtype=torch.float64),\n",
       "  tensor(0.2929, dtype=torch.float64),\n",
       "  tensor(0.2929, dtype=torch.float64),\n",
       "  tensor(0.2929, dtype=torch.float64),\n",
       "  tensor(0.3000, dtype=torch.float64),\n",
       "  tensor(0.4500, dtype=torch.float64),\n",
       "  tensor(0.5143, dtype=torch.float64),\n",
       "  tensor(0.5286, dtype=torch.float64),\n",
       "  tensor(0.5571, dtype=torch.float64),\n",
       "  tensor(0.6500, dtype=torch.float64),\n",
       "  tensor(0.7786, dtype=torch.float64),\n",
       "  tensor(0.7500, dtype=torch.float64),\n",
       "  tensor(0.8571, dtype=torch.float64),\n",
       "  tensor(0.8357, dtype=torch.float64),\n",
       "  tensor(0.8357, dtype=torch.float64),\n",
       "  tensor(0.8857, dtype=torch.float64),\n",
       "  tensor(0.9071, dtype=torch.float64),\n",
       "  tensor(0.9429, dtype=torch.float64),\n",
       "  tensor(0.9857, dtype=torch.float64),\n",
       "  tensor(0.9929, dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(0.9929, dtype=torch.float64),\n",
       "  tensor(0.9929, dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(0.9929, dtype=torch.float64),\n",
       "  tensor(0.9143, dtype=torch.float64),\n",
       "  tensor(0.8714, dtype=torch.float64),\n",
       "  tensor(0.9929, dtype=torch.float64),\n",
       "  tensor(0.9000, dtype=torch.float64),\n",
       "  tensor(0.9929, dtype=torch.float64),\n",
       "  tensor(0.9929, dtype=torch.float64),\n",
       "  tensor(0.9571, dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(0.9929, dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(0.9929, dtype=torch.float64),\n",
       "  tensor(0.9571, dtype=torch.float64),\n",
       "  tensor(0.6357, dtype=torch.float64),\n",
       "  tensor(0.8357, dtype=torch.float64),\n",
       "  tensor(0.9214, dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(0.9643, dtype=torch.float64),\n",
       "  tensor(0.9429, dtype=torch.float64),\n",
       "  tensor(0.9571, dtype=torch.float64),\n",
       "  tensor(0.9857, dtype=torch.float64),\n",
       "  tensor(0.9929, dtype=torch.float64),\n",
       "  tensor(0.9857, dtype=torch.float64),\n",
       "  tensor(0.9786, dtype=torch.float64),\n",
       "  tensor(0.9857, dtype=torch.float64),\n",
       "  tensor(0.9929, dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64),\n",
       "  tensor(1., dtype=torch.float64)],\n",
       " [tensor(1.8221, grad_fn=<NllLossBackward0>),\n",
       "  tensor(2.0807, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.7790, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.8437, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.8414, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.7934, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.7071, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.6272, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.4873, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.4148, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.3150, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.2526, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.2066, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.1245, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.0451, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.0192, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.9745, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.9455, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.9399, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8533, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8290, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.9119, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8450, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8731, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8606, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8042, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8830, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8111, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.9120, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8228, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8206, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.9086, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8481, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8189, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7780, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7921, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8113, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7813, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.9445, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.0739, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.3450, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7191, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.1978, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7521, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8792, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.1393, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.9518, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7375, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6916, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7984, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8582, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7490, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7060, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7480, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8346, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.9100, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.9113, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8604, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7981, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7562, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7558, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7520, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7215, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7084, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7263, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7651, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7546, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7186, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6962, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6976, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6897, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6745, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6821, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6992, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6934, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6788, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6813, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6930, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6885, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6824, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6860, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6902, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6827, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6840, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6946, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6921, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6822, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6844, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6907, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6858, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6847, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6910, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6868, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6877, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6929, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6836, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6929, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6904, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6905, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6943, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6933, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7171, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7366, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8377, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.1245, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.9971, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.6434, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.5030, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6858, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8553, grad_fn=<NllLossBackward0>),\n",
       "  tensor(1.0044, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.9305, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7704, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6689, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6822, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7447, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7760, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7617, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7683, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7984, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8122, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8022, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7853, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7705, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7585, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7479, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7410, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7461, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7648, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7813, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7772, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7632, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7540, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7540, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7583, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7520, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7385, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7345, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7250, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7203, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7226, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7105, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7306, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7018, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7283, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6924, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6873, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7142, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6901, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6825, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7058, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6971, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6871, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6994, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7038, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6876, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6964, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7080, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6985, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6970, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7051, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7050, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7040, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7031, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7033, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7084, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7048, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7046, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7079, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7040, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7117, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7030, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7093, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7084, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7069, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7069, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7134, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7007, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7382, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7030, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.8236, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7158, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7416, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7903, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7071, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7061, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7543, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7810, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7198, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7177, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7361, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7778, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7259, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7112, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7212, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7536, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7291, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7042, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7151, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.7368, grad_fn=<NllLossBackward0>)],\n",
       " [tensor(0.3500, dtype=torch.float64),\n",
       "  tensor(0.3500, dtype=torch.float64),\n",
       "  tensor(0.3500, dtype=torch.float64),\n",
       "  tensor(0.3500, dtype=torch.float64),\n",
       "  tensor(0.3500, dtype=torch.float64),\n",
       "  tensor(0.3500, dtype=torch.float64),\n",
       "  tensor(0.3500, dtype=torch.float64),\n",
       "  tensor(0.3500, dtype=torch.float64),\n",
       "  tensor(0.3533, dtype=torch.float64),\n",
       "  tensor(0.4933, dtype=torch.float64),\n",
       "  tensor(0.5467, dtype=torch.float64),\n",
       "  tensor(0.5000, dtype=torch.float64),\n",
       "  tensor(0.4933, dtype=torch.float64),\n",
       "  tensor(0.5533, dtype=torch.float64),\n",
       "  tensor(0.6933, dtype=torch.float64),\n",
       "  tensor(0.6233, dtype=torch.float64),\n",
       "  tensor(0.6900, dtype=torch.float64),\n",
       "  tensor(0.6567, dtype=torch.float64),\n",
       "  tensor(0.7167, dtype=torch.float64),\n",
       "  tensor(0.7300, dtype=torch.float64),\n",
       "  tensor(0.7433, dtype=torch.float64),\n",
       "  tensor(0.7567, dtype=torch.float64),\n",
       "  tensor(0.7900, dtype=torch.float64),\n",
       "  tensor(0.7700, dtype=torch.float64),\n",
       "  tensor(0.7700, dtype=torch.float64),\n",
       "  tensor(0.8000, dtype=torch.float64),\n",
       "  tensor(0.7833, dtype=torch.float64),\n",
       "  tensor(0.7967, dtype=torch.float64),\n",
       "  tensor(0.7733, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.7767, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.7867, dtype=torch.float64),\n",
       "  tensor(0.8167, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.7867, dtype=torch.float64),\n",
       "  tensor(0.8000, dtype=torch.float64),\n",
       "  tensor(0.7633, dtype=torch.float64),\n",
       "  tensor(0.6900, dtype=torch.float64),\n",
       "  tensor(0.6267, dtype=torch.float64),\n",
       "  tensor(0.8100, dtype=torch.float64),\n",
       "  tensor(0.6800, dtype=torch.float64),\n",
       "  tensor(0.8233, dtype=torch.float64),\n",
       "  tensor(0.7700, dtype=torch.float64),\n",
       "  tensor(0.6933, dtype=torch.float64),\n",
       "  tensor(0.7500, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8300, dtype=torch.float64),\n",
       "  tensor(0.8167, dtype=torch.float64),\n",
       "  tensor(0.7900, dtype=torch.float64),\n",
       "  tensor(0.8200, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.7800, dtype=torch.float64),\n",
       "  tensor(0.7600, dtype=torch.float64),\n",
       "  tensor(0.7567, dtype=torch.float64),\n",
       "  tensor(0.7733, dtype=torch.float64),\n",
       "  tensor(0.7800, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.7967, dtype=torch.float64),\n",
       "  tensor(0.8000, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.7900, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.7867, dtype=torch.float64),\n",
       "  tensor(0.7867, dtype=torch.float64),\n",
       "  tensor(0.7833, dtype=torch.float64),\n",
       "  tensor(0.7933, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8133, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8000, dtype=torch.float64),\n",
       "  tensor(0.8000, dtype=torch.float64),\n",
       "  tensor(0.8000, dtype=torch.float64),\n",
       "  tensor(0.7967, dtype=torch.float64),\n",
       "  tensor(0.8000, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8000, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.7967, dtype=torch.float64),\n",
       "  tensor(0.7967, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8000, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8100, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.7933, dtype=torch.float64),\n",
       "  tensor(0.7867, dtype=torch.float64),\n",
       "  tensor(0.7667, dtype=torch.float64),\n",
       "  tensor(0.6833, dtype=torch.float64),\n",
       "  tensor(0.5700, dtype=torch.float64),\n",
       "  tensor(0.5533, dtype=torch.float64),\n",
       "  tensor(0.6133, dtype=torch.float64),\n",
       "  tensor(0.8167, dtype=torch.float64),\n",
       "  tensor(0.7833, dtype=torch.float64),\n",
       "  tensor(0.7567, dtype=torch.float64),\n",
       "  tensor(0.7600, dtype=torch.float64),\n",
       "  tensor(0.7900, dtype=torch.float64),\n",
       "  tensor(0.7967, dtype=torch.float64),\n",
       "  tensor(0.8133, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.7933, dtype=torch.float64),\n",
       "  tensor(0.7833, dtype=torch.float64),\n",
       "  tensor(0.7900, dtype=torch.float64),\n",
       "  tensor(0.7767, dtype=torch.float64),\n",
       "  tensor(0.7800, dtype=torch.float64),\n",
       "  tensor(0.7900, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8200, dtype=torch.float64),\n",
       "  tensor(0.8267, dtype=torch.float64),\n",
       "  tensor(0.8233, dtype=torch.float64),\n",
       "  tensor(0.8267, dtype=torch.float64),\n",
       "  tensor(0.8167, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.7967, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8200, dtype=torch.float64),\n",
       "  tensor(0.8200, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8100, dtype=torch.float64),\n",
       "  tensor(0.8000, dtype=torch.float64),\n",
       "  tensor(0.8000, dtype=torch.float64),\n",
       "  tensor(0.8000, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8000, dtype=torch.float64),\n",
       "  tensor(0.7933, dtype=torch.float64),\n",
       "  tensor(0.8167, dtype=torch.float64),\n",
       "  tensor(0.7933, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8167, dtype=torch.float64),\n",
       "  tensor(0.7967, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8167, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8100, dtype=torch.float64),\n",
       "  tensor(0.8000, dtype=torch.float64),\n",
       "  tensor(0.8100, dtype=torch.float64),\n",
       "  tensor(0.8133, dtype=torch.float64),\n",
       "  tensor(0.8100, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8167, dtype=torch.float64),\n",
       "  tensor(0.8100, dtype=torch.float64),\n",
       "  tensor(0.8000, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8133, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8200, dtype=torch.float64),\n",
       "  tensor(0.7933, dtype=torch.float64),\n",
       "  tensor(0.8267, dtype=torch.float64),\n",
       "  tensor(0.7833, dtype=torch.float64),\n",
       "  tensor(0.8000, dtype=torch.float64),\n",
       "  tensor(0.7933, dtype=torch.float64),\n",
       "  tensor(0.7767, dtype=torch.float64),\n",
       "  tensor(0.8100, dtype=torch.float64),\n",
       "  tensor(0.8233, dtype=torch.float64),\n",
       "  tensor(0.7867, dtype=torch.float64),\n",
       "  tensor(0.7833, dtype=torch.float64),\n",
       "  tensor(0.8100, dtype=torch.float64),\n",
       "  tensor(0.8233, dtype=torch.float64),\n",
       "  tensor(0.8000, dtype=torch.float64),\n",
       "  tensor(0.7833, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8200, dtype=torch.float64),\n",
       "  tensor(0.8067, dtype=torch.float64),\n",
       "  tensor(0.7833, dtype=torch.float64),\n",
       "  tensor(0.8033, dtype=torch.float64),\n",
       "  tensor(0.8133, dtype=torch.float64),\n",
       "  tensor(0.8100, dtype=torch.float64),\n",
       "  tensor(0.7967, dtype=torch.float64)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = ite_GCN(nfeat=features.shape[1],\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0,\n",
    "            train_nite= 2,\n",
    "            eval_nite= 0,\n",
    "            allow_grad=True,\n",
    "            smooth_fac=smooth_fac)\n",
    "run_experiment(num_epochs, model2, lr, weight_decay, features, adj, idx_train, idx_val, idx_test, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 1.7943 accuracy= 0.1620\n",
      "inference time:  0.1115729808807373\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 0.7120 accuracy= 0.7780\n",
      "inference time:  0.1319718360900879\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 8.1743 accuracy= 0.3420\n",
      "inference time:  0.1900501251220703\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 27.2977 accuracy= 0.3540\n",
      "inference time:  0.2713141441345215\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 204.8581 accuracy= 0.2110\n",
      "inference time:  0.4239051342010498\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 722.1061 accuracy= 0.2390\n",
      "inference time:  0.39115381240844727\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 3141.8953 accuracy= 0.1750\n",
      "inference time:  0.45572519302368164\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 11322.1367 accuracy= 0.2060\n",
      "inference time:  0.5789189338684082\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 44796.0039 accuracy= 0.1680\n",
      "inference time:  0.5935399532318115\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 171245.7031 accuracy= 0.2810\n",
      "inference time:  0.6135060787200928\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 704483.2500 accuracy= 0.2860\n",
      "inference time:  0.6539671421051025\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 2887993.7500 accuracy= 0.2930\n",
      "inference time:  0.850844144821167\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 12310760.0000 accuracy= 0.2870\n",
      "inference time:  0.8096110820770264\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 52552016.0000 accuracy= 0.2930\n",
      "inference time:  1.0008769035339355\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 226961440.0000 accuracy= 0.2930\n",
      "inference time:  0.9477770328521729\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 981502208.0000 accuracy= 0.3010\n",
      "inference time:  1.1328151226043701\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 4266723584.0000 accuracy= 0.2940\n",
      "inference time:  1.065114974975586\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 18556422144.0000 accuracy= 0.3010\n",
      "inference time:  1.2627463340759277\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 80865419264.0000 accuracy= 0.3000\n",
      "inference time:  1.4481170177459717\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 352760954880.0000 accuracy= 0.2990\n",
      "inference time:  1.384653091430664\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 1540183949312.0000 accuracy= 0.2910\n",
      "inference time:  1.3647632598876953\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 6730026057728.0000 accuracy= 0.2750\n",
      "inference time:  1.443354845046997\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 29446335627264.0000 accuracy= 0.2680\n",
      "inference time:  1.5772221088409424\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 128836729569280.0000 accuracy= 0.2590\n",
      "inference time:  1.6410658359527588\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 563716135321600.0000 accuracy= 0.2510\n",
      "inference time:  1.8095498085021973\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 2467533068173312.0000 accuracy= 0.2440\n",
      "inference time:  1.760725975036621\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 10807344602546176.0000 accuracy= 0.2280\n",
      "inference time:  2.0200438499450684\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 47326884584751104.0000 accuracy= 0.2110\n",
      "inference time:  1.8033149242401123\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n",
      "Test set results: loss= 207333670699139072.0000 accuracy= 0.1960\n",
      "inference time:  1.9140820503234863\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for i in range(1, 30):\n",
    "    model = ite_GCN(nfeat=features.shape[1],\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0,\n",
    "            train_nite= 3,\n",
    "            eval_nite= i,\n",
    "            allow_grad=True,\n",
    "            smooth_fac=smooth_fac)\n",
    "    model.load_state_dict(model2.state_dict().copy())\n",
    "    loss_test, acc_test = test(model, features, adj, idx_test, labels)\n",
    "    test_losses.append(loss_test.item())\n",
    "    test_accuracies.append(acc_test.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.7942862510681152,\n",
       " 0.711990475654602,\n",
       " 8.17428970336914,\n",
       " 27.297683715820312,\n",
       " 204.85812377929688,\n",
       " 722.1061401367188,\n",
       " 3141.895263671875,\n",
       " 11322.13671875,\n",
       " 44796.00390625,\n",
       " 171245.703125,\n",
       " 704483.25,\n",
       " 2887993.75,\n",
       " 12310760.0,\n",
       " 52552016.0,\n",
       " 226961440.0,\n",
       " 981502208.0,\n",
       " 4266723584.0,\n",
       " 18556422144.0,\n",
       " 80865419264.0,\n",
       " 352760954880.0,\n",
       " 1540183949312.0,\n",
       " 6730026057728.0,\n",
       " 29446335627264.0,\n",
       " 128836729569280.0,\n",
       " 563716135321600.0,\n",
       " 2467533068173312.0,\n",
       " 1.0807344602546176e+16,\n",
       " 4.73268845847511e+16,\n",
       " 2.0733367069913907e+17]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.162,\n",
       " 0.778,\n",
       " 0.342,\n",
       " 0.354,\n",
       " 0.211,\n",
       " 0.239,\n",
       " 0.175,\n",
       " 0.206,\n",
       " 0.168,\n",
       " 0.281,\n",
       " 0.286,\n",
       " 0.293,\n",
       " 0.287,\n",
       " 0.293,\n",
       " 0.293,\n",
       " 0.301,\n",
       " 0.294,\n",
       " 0.301,\n",
       " 0.3,\n",
       " 0.299,\n",
       " 0.291,\n",
       " 0.275,\n",
       " 0.268,\n",
       " 0.259,\n",
       " 0.251,\n",
       " 0.244,\n",
       " 0.228,\n",
       " 0.211,\n",
       " 0.196]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1380afe20>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(test_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x138128820>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iterENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
