{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import load_data, accuracy, run_experiment\n",
    "from models import GCN_2, GCN_3, ite_GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(path=\"../data/cora/\", dataset=\"cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 16\n",
    "dropout = 0.5\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "num_epochs = 200\n",
    "smooth_fac = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intialize a 2-layer GCN\n"
     ]
    }
   ],
   "source": [
    "model0 = GCN_2(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runrunrun!\n",
      "Epoch: 0001 loss_train: 1.8955 acc_train: 0.2214 loss_val: 1.8864 acc_val: 0.1567 time: 0.0324s\n",
      "Epoch: 0002 loss_train: 1.8776 acc_train: 0.2143 loss_val: 1.8775 acc_val: 0.1567 time: 0.0109s\n",
      "Epoch: 0003 loss_train: 1.8804 acc_train: 0.2143 loss_val: 1.8686 acc_val: 0.1567 time: 0.0080s\n",
      "Epoch: 0004 loss_train: 1.8698 acc_train: 0.2214 loss_val: 1.8598 acc_val: 0.1633 time: 0.0100s\n",
      "Epoch: 0005 loss_train: 1.8515 acc_train: 0.3000 loss_val: 1.8505 acc_val: 0.1767 time: 0.0088s\n",
      "Epoch: 0006 loss_train: 1.8407 acc_train: 0.3000 loss_val: 1.8405 acc_val: 0.3400 time: 0.0098s\n",
      "Epoch: 0007 loss_train: 1.8345 acc_train: 0.2714 loss_val: 1.8303 acc_val: 0.4367 time: 0.0090s\n",
      "Epoch: 0008 loss_train: 1.8297 acc_train: 0.2857 loss_val: 1.8202 acc_val: 0.3700 time: 0.0089s\n",
      "Epoch: 0009 loss_train: 1.8080 acc_train: 0.3214 loss_val: 1.8101 acc_val: 0.3633 time: 0.0099s\n",
      "Epoch: 0010 loss_train: 1.8066 acc_train: 0.3857 loss_val: 1.8002 acc_val: 0.3500 time: 0.0084s\n",
      "Epoch: 0011 loss_train: 1.7757 acc_train: 0.3429 loss_val: 1.7904 acc_val: 0.3500 time: 0.0093s\n",
      "Epoch: 0012 loss_train: 1.7828 acc_train: 0.2857 loss_val: 1.7808 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0013 loss_train: 1.7782 acc_train: 0.3071 loss_val: 1.7719 acc_val: 0.3500 time: 0.0102s\n",
      "Epoch: 0014 loss_train: 1.7788 acc_train: 0.3000 loss_val: 1.7635 acc_val: 0.3500 time: 0.0091s\n",
      "Epoch: 0015 loss_train: 1.7631 acc_train: 0.2929 loss_val: 1.7559 acc_val: 0.3500 time: 0.0084s\n",
      "Epoch: 0016 loss_train: 1.7458 acc_train: 0.2929 loss_val: 1.7487 acc_val: 0.3500 time: 0.0087s\n",
      "Epoch: 0017 loss_train: 1.7421 acc_train: 0.3000 loss_val: 1.7420 acc_val: 0.3500 time: 0.0093s\n",
      "Epoch: 0018 loss_train: 1.7311 acc_train: 0.2929 loss_val: 1.7356 acc_val: 0.3500 time: 0.0105s\n",
      "Epoch: 0019 loss_train: 1.7407 acc_train: 0.3000 loss_val: 1.7293 acc_val: 0.3500 time: 0.0085s\n",
      "Epoch: 0020 loss_train: 1.7137 acc_train: 0.3071 loss_val: 1.7230 acc_val: 0.3500 time: 0.0097s\n",
      "Epoch: 0021 loss_train: 1.6860 acc_train: 0.3000 loss_val: 1.7165 acc_val: 0.3500 time: 0.0097s\n",
      "Epoch: 0022 loss_train: 1.6981 acc_train: 0.2929 loss_val: 1.7098 acc_val: 0.3500 time: 0.0093s\n",
      "Epoch: 0023 loss_train: 1.6759 acc_train: 0.3071 loss_val: 1.7029 acc_val: 0.3500 time: 0.0105s\n",
      "Epoch: 0024 loss_train: 1.6878 acc_train: 0.2929 loss_val: 1.6959 acc_val: 0.3500 time: 0.0098s\n",
      "Epoch: 0025 loss_train: 1.6600 acc_train: 0.3000 loss_val: 1.6886 acc_val: 0.3500 time: 0.0090s\n",
      "Epoch: 0026 loss_train: 1.6603 acc_train: 0.3214 loss_val: 1.6811 acc_val: 0.3500 time: 0.0090s\n",
      "Epoch: 0027 loss_train: 1.6678 acc_train: 0.2929 loss_val: 1.6737 acc_val: 0.3500 time: 0.0079s\n",
      "Epoch: 0028 loss_train: 1.6297 acc_train: 0.3143 loss_val: 1.6661 acc_val: 0.3533 time: 0.0121s\n",
      "Epoch: 0029 loss_train: 1.6184 acc_train: 0.3357 loss_val: 1.6582 acc_val: 0.3633 time: 0.0103s\n",
      "Epoch: 0030 loss_train: 1.5980 acc_train: 0.3500 loss_val: 1.6500 acc_val: 0.3633 time: 0.0086s\n",
      "Epoch: 0031 loss_train: 1.6061 acc_train: 0.3571 loss_val: 1.6416 acc_val: 0.3633 time: 0.0074s\n",
      "Epoch: 0032 loss_train: 1.5865 acc_train: 0.3786 loss_val: 1.6328 acc_val: 0.3667 time: 0.0093s\n",
      "Epoch: 0033 loss_train: 1.5680 acc_train: 0.3857 loss_val: 1.6238 acc_val: 0.3733 time: 0.0088s\n",
      "Epoch: 0034 loss_train: 1.5455 acc_train: 0.4000 loss_val: 1.6146 acc_val: 0.3733 time: 0.0082s\n",
      "Epoch: 0035 loss_train: 1.5510 acc_train: 0.4143 loss_val: 1.6052 acc_val: 0.3833 time: 0.0089s\n",
      "Epoch: 0036 loss_train: 1.5216 acc_train: 0.4429 loss_val: 1.5957 acc_val: 0.3867 time: 0.0083s\n",
      "Epoch: 0037 loss_train: 1.5210 acc_train: 0.4357 loss_val: 1.5859 acc_val: 0.4067 time: 0.0079s\n",
      "Epoch: 0038 loss_train: 1.5007 acc_train: 0.4643 loss_val: 1.5760 acc_val: 0.4067 time: 0.0093s\n",
      "Epoch: 0039 loss_train: 1.4812 acc_train: 0.4786 loss_val: 1.5657 acc_val: 0.4133 time: 0.0093s\n",
      "Epoch: 0040 loss_train: 1.4816 acc_train: 0.4571 loss_val: 1.5547 acc_val: 0.4267 time: 0.0086s\n",
      "Epoch: 0041 loss_train: 1.4768 acc_train: 0.4714 loss_val: 1.5435 acc_val: 0.4267 time: 0.0535s\n",
      "Epoch: 0042 loss_train: 1.4449 acc_train: 0.4857 loss_val: 1.5322 acc_val: 0.4367 time: 0.0080s\n",
      "Epoch: 0043 loss_train: 1.4219 acc_train: 0.5071 loss_val: 1.5206 acc_val: 0.4400 time: 0.0080s\n",
      "Epoch: 0044 loss_train: 1.4039 acc_train: 0.5071 loss_val: 1.5087 acc_val: 0.4400 time: 0.0094s\n",
      "Epoch: 0045 loss_train: 1.3750 acc_train: 0.4857 loss_val: 1.4969 acc_val: 0.4400 time: 0.0087s\n",
      "Epoch: 0046 loss_train: 1.3762 acc_train: 0.4857 loss_val: 1.4852 acc_val: 0.4400 time: 0.0094s\n",
      "Epoch: 0047 loss_train: 1.3624 acc_train: 0.4857 loss_val: 1.4739 acc_val: 0.4467 time: 0.0087s\n",
      "Epoch: 0048 loss_train: 1.3567 acc_train: 0.5000 loss_val: 1.4628 acc_val: 0.4767 time: 0.0087s\n",
      "Epoch: 0049 loss_train: 1.3467 acc_train: 0.5286 loss_val: 1.4517 acc_val: 0.4767 time: 0.0128s\n",
      "Epoch: 0050 loss_train: 1.3033 acc_train: 0.5714 loss_val: 1.4405 acc_val: 0.4900 time: 0.0101s\n",
      "Epoch: 0051 loss_train: 1.3271 acc_train: 0.5143 loss_val: 1.4295 acc_val: 0.5200 time: 0.0259s\n",
      "Epoch: 0052 loss_train: 1.3100 acc_train: 0.5643 loss_val: 1.4185 acc_val: 0.5333 time: 0.0089s\n",
      "Epoch: 0053 loss_train: 1.3035 acc_train: 0.5643 loss_val: 1.4076 acc_val: 0.5500 time: 0.0129s\n",
      "Epoch: 0054 loss_train: 1.2626 acc_train: 0.6000 loss_val: 1.3968 acc_val: 0.5633 time: 0.0130s\n",
      "Epoch: 0055 loss_train: 1.2508 acc_train: 0.6071 loss_val: 1.3855 acc_val: 0.5733 time: 0.0117s\n",
      "Epoch: 0056 loss_train: 1.2278 acc_train: 0.6071 loss_val: 1.3738 acc_val: 0.5700 time: 0.0146s\n",
      "Epoch: 0057 loss_train: 1.2291 acc_train: 0.6143 loss_val: 1.3617 acc_val: 0.5700 time: 0.0106s\n",
      "Epoch: 0058 loss_train: 1.2070 acc_train: 0.6286 loss_val: 1.3498 acc_val: 0.5800 time: 0.0102s\n",
      "Epoch: 0059 loss_train: 1.1909 acc_train: 0.6357 loss_val: 1.3383 acc_val: 0.5800 time: 0.0097s\n",
      "Epoch: 0060 loss_train: 1.1410 acc_train: 0.6643 loss_val: 1.3269 acc_val: 0.5867 time: 0.0088s\n",
      "Epoch: 0061 loss_train: 1.1530 acc_train: 0.6500 loss_val: 1.3157 acc_val: 0.5867 time: 0.0092s\n",
      "Epoch: 0062 loss_train: 1.1198 acc_train: 0.6929 loss_val: 1.3045 acc_val: 0.5933 time: 0.0094s\n",
      "Epoch: 0063 loss_train: 1.1241 acc_train: 0.6571 loss_val: 1.2936 acc_val: 0.6033 time: 0.0092s\n",
      "Epoch: 0064 loss_train: 1.1263 acc_train: 0.6429 loss_val: 1.2829 acc_val: 0.6133 time: 0.0086s\n",
      "Epoch: 0065 loss_train: 1.0881 acc_train: 0.6643 loss_val: 1.2724 acc_val: 0.6267 time: 0.0079s\n",
      "Epoch: 0066 loss_train: 1.0948 acc_train: 0.6929 loss_val: 1.2620 acc_val: 0.6367 time: 0.0093s\n",
      "Epoch: 0067 loss_train: 1.0759 acc_train: 0.7000 loss_val: 1.2513 acc_val: 0.6467 time: 0.0088s\n",
      "Epoch: 0068 loss_train: 1.0874 acc_train: 0.7000 loss_val: 1.2401 acc_val: 0.6500 time: 0.0082s\n",
      "Epoch: 0069 loss_train: 1.0740 acc_train: 0.6714 loss_val: 1.2293 acc_val: 0.6567 time: 0.0083s\n",
      "Epoch: 0070 loss_train: 1.0228 acc_train: 0.7429 loss_val: 1.2194 acc_val: 0.6700 time: 0.0085s\n",
      "Epoch: 0071 loss_train: 1.0501 acc_train: 0.7214 loss_val: 1.2099 acc_val: 0.6733 time: 0.0101s\n",
      "Epoch: 0072 loss_train: 1.0069 acc_train: 0.7571 loss_val: 1.2002 acc_val: 0.6800 time: 0.0115s\n",
      "Epoch: 0073 loss_train: 1.0317 acc_train: 0.7500 loss_val: 1.1908 acc_val: 0.6867 time: 0.0108s\n",
      "Epoch: 0074 loss_train: 1.0164 acc_train: 0.7214 loss_val: 1.1814 acc_val: 0.6933 time: 0.0144s\n",
      "Epoch: 0075 loss_train: 0.9783 acc_train: 0.7643 loss_val: 1.1725 acc_val: 0.7033 time: 0.0114s\n",
      "Epoch: 0076 loss_train: 0.9599 acc_train: 0.7643 loss_val: 1.1639 acc_val: 0.7067 time: 0.0126s\n",
      "Epoch: 0077 loss_train: 0.9265 acc_train: 0.7786 loss_val: 1.1548 acc_val: 0.7133 time: 0.0167s\n",
      "Epoch: 0078 loss_train: 0.9523 acc_train: 0.7714 loss_val: 1.1451 acc_val: 0.7167 time: 0.0118s\n",
      "Epoch: 0079 loss_train: 0.9455 acc_train: 0.8000 loss_val: 1.1346 acc_val: 0.7133 time: 0.0133s\n",
      "Epoch: 0080 loss_train: 0.9285 acc_train: 0.7714 loss_val: 1.1241 acc_val: 0.7133 time: 0.0135s\n",
      "Epoch: 0081 loss_train: 0.8907 acc_train: 0.8214 loss_val: 1.1141 acc_val: 0.7200 time: 0.0133s\n",
      "Epoch: 0082 loss_train: 0.9267 acc_train: 0.7786 loss_val: 1.1046 acc_val: 0.7267 time: 0.0112s\n",
      "Epoch: 0083 loss_train: 0.8833 acc_train: 0.8286 loss_val: 1.0955 acc_val: 0.7267 time: 0.0106s\n",
      "Epoch: 0084 loss_train: 0.9291 acc_train: 0.7857 loss_val: 1.0866 acc_val: 0.7433 time: 0.0104s\n",
      "Epoch: 0085 loss_train: 0.8361 acc_train: 0.8214 loss_val: 1.0784 acc_val: 0.7467 time: 0.0097s\n",
      "Epoch: 0086 loss_train: 0.8496 acc_train: 0.8500 loss_val: 1.0704 acc_val: 0.7467 time: 0.0090s\n",
      "Epoch: 0087 loss_train: 0.8475 acc_train: 0.8429 loss_val: 1.0624 acc_val: 0.7533 time: 0.0101s\n",
      "Epoch: 0088 loss_train: 0.8356 acc_train: 0.8357 loss_val: 1.0529 acc_val: 0.7633 time: 0.0096s\n",
      "Epoch: 0089 loss_train: 0.8222 acc_train: 0.8357 loss_val: 1.0423 acc_val: 0.7667 time: 0.0099s\n",
      "Epoch: 0090 loss_train: 0.7880 acc_train: 0.8357 loss_val: 1.0321 acc_val: 0.7667 time: 0.0087s\n",
      "Epoch: 0091 loss_train: 0.7999 acc_train: 0.8571 loss_val: 1.0227 acc_val: 0.7700 time: 0.0182s\n",
      "Epoch: 0092 loss_train: 0.7787 acc_train: 0.8286 loss_val: 1.0138 acc_val: 0.7733 time: 0.0260s\n",
      "Epoch: 0093 loss_train: 0.8368 acc_train: 0.8571 loss_val: 1.0054 acc_val: 0.7867 time: 0.0133s\n",
      "Epoch: 0094 loss_train: 0.7785 acc_train: 0.8571 loss_val: 0.9980 acc_val: 0.7833 time: 0.0117s\n",
      "Epoch: 0095 loss_train: 0.7976 acc_train: 0.8286 loss_val: 0.9912 acc_val: 0.7933 time: 0.0243s\n",
      "Epoch: 0096 loss_train: 0.7616 acc_train: 0.8643 loss_val: 0.9846 acc_val: 0.7967 time: 0.0218s\n",
      "Epoch: 0097 loss_train: 0.7225 acc_train: 0.8357 loss_val: 0.9783 acc_val: 0.7967 time: 0.0098s\n",
      "Epoch: 0098 loss_train: 0.7555 acc_train: 0.8714 loss_val: 0.9712 acc_val: 0.8033 time: 0.0104s\n",
      "Epoch: 0099 loss_train: 0.7242 acc_train: 0.8643 loss_val: 0.9632 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0100 loss_train: 0.7969 acc_train: 0.8357 loss_val: 0.9540 acc_val: 0.8067 time: 0.0101s\n",
      "Epoch: 0101 loss_train: 0.7307 acc_train: 0.8929 loss_val: 0.9443 acc_val: 0.8100 time: 0.0085s\n",
      "Epoch: 0102 loss_train: 0.7035 acc_train: 0.8571 loss_val: 0.9354 acc_val: 0.8167 time: 0.0096s\n",
      "Epoch: 0103 loss_train: 0.7125 acc_train: 0.8714 loss_val: 0.9277 acc_val: 0.8100 time: 0.0089s\n",
      "Epoch: 0104 loss_train: 0.6951 acc_train: 0.8714 loss_val: 0.9208 acc_val: 0.8067 time: 0.0090s\n",
      "Epoch: 0105 loss_train: 0.6703 acc_train: 0.9071 loss_val: 0.9140 acc_val: 0.8100 time: 0.0082s\n",
      "Epoch: 0106 loss_train: 0.6775 acc_train: 0.8714 loss_val: 0.9078 acc_val: 0.8100 time: 0.0081s\n",
      "Epoch: 0107 loss_train: 0.6319 acc_train: 0.8714 loss_val: 0.9020 acc_val: 0.8133 time: 0.0088s\n",
      "Epoch: 0108 loss_train: 0.6509 acc_train: 0.9071 loss_val: 0.8967 acc_val: 0.8133 time: 0.0083s\n",
      "Epoch: 0109 loss_train: 0.6550 acc_train: 0.8571 loss_val: 0.8913 acc_val: 0.8067 time: 0.0088s\n",
      "Epoch: 0110 loss_train: 0.6505 acc_train: 0.8786 loss_val: 0.8872 acc_val: 0.8033 time: 0.0089s\n",
      "Epoch: 0111 loss_train: 0.6263 acc_train: 0.8786 loss_val: 0.8833 acc_val: 0.8100 time: 0.0102s\n",
      "Epoch: 0112 loss_train: 0.6422 acc_train: 0.8571 loss_val: 0.8777 acc_val: 0.8133 time: 0.0080s\n",
      "Epoch: 0113 loss_train: 0.6313 acc_train: 0.8857 loss_val: 0.8718 acc_val: 0.8100 time: 0.0092s\n",
      "Epoch: 0114 loss_train: 0.6351 acc_train: 0.9000 loss_val: 0.8661 acc_val: 0.8067 time: 0.0079s\n",
      "Epoch: 0115 loss_train: 0.6731 acc_train: 0.8643 loss_val: 0.8594 acc_val: 0.8067 time: 0.0093s\n",
      "Epoch: 0116 loss_train: 0.6165 acc_train: 0.8643 loss_val: 0.8533 acc_val: 0.8100 time: 0.0096s\n",
      "Epoch: 0117 loss_train: 0.6223 acc_train: 0.8714 loss_val: 0.8476 acc_val: 0.8100 time: 0.0092s\n",
      "Epoch: 0118 loss_train: 0.6144 acc_train: 0.8786 loss_val: 0.8426 acc_val: 0.8133 time: 0.0084s\n",
      "Epoch: 0119 loss_train: 0.5847 acc_train: 0.9214 loss_val: 0.8384 acc_val: 0.8067 time: 0.0095s\n",
      "Epoch: 0120 loss_train: 0.5918 acc_train: 0.9000 loss_val: 0.8348 acc_val: 0.8100 time: 0.0083s\n",
      "Epoch: 0121 loss_train: 0.5953 acc_train: 0.8929 loss_val: 0.8317 acc_val: 0.8067 time: 0.0090s\n",
      "Epoch: 0122 loss_train: 0.5783 acc_train: 0.8929 loss_val: 0.8294 acc_val: 0.8033 time: 0.0092s\n",
      "Epoch: 0123 loss_train: 0.6098 acc_train: 0.9071 loss_val: 0.8280 acc_val: 0.8033 time: 0.0090s\n",
      "Epoch: 0124 loss_train: 0.5843 acc_train: 0.8786 loss_val: 0.8275 acc_val: 0.8100 time: 0.0091s\n",
      "Epoch: 0125 loss_train: 0.5564 acc_train: 0.9000 loss_val: 0.8275 acc_val: 0.8100 time: 0.0078s\n",
      "Epoch: 0126 loss_train: 0.5404 acc_train: 0.8929 loss_val: 0.8260 acc_val: 0.8100 time: 0.0089s\n",
      "Epoch: 0127 loss_train: 0.5266 acc_train: 0.9214 loss_val: 0.8217 acc_val: 0.8133 time: 0.0086s\n",
      "Epoch: 0128 loss_train: 0.5353 acc_train: 0.9214 loss_val: 0.8162 acc_val: 0.8100 time: 0.0101s\n",
      "Epoch: 0129 loss_train: 0.5576 acc_train: 0.8786 loss_val: 0.8104 acc_val: 0.8100 time: 0.0088s\n",
      "Epoch: 0130 loss_train: 0.5643 acc_train: 0.9143 loss_val: 0.8045 acc_val: 0.8100 time: 0.0124s\n",
      "Epoch: 0131 loss_train: 0.5612 acc_train: 0.8929 loss_val: 0.7990 acc_val: 0.8167 time: 0.0105s\n",
      "Epoch: 0132 loss_train: 0.5382 acc_train: 0.9143 loss_val: 0.7943 acc_val: 0.8167 time: 0.0095s\n",
      "Epoch: 0133 loss_train: 0.5819 acc_train: 0.9071 loss_val: 0.7907 acc_val: 0.8200 time: 0.0099s\n",
      "Epoch: 0134 loss_train: 0.5122 acc_train: 0.9286 loss_val: 0.7879 acc_val: 0.8200 time: 0.0097s\n",
      "Epoch: 0135 loss_train: 0.5707 acc_train: 0.9000 loss_val: 0.7858 acc_val: 0.8167 time: 0.0101s\n",
      "Epoch: 0136 loss_train: 0.4983 acc_train: 0.9286 loss_val: 0.7846 acc_val: 0.8167 time: 0.0101s\n",
      "Epoch: 0137 loss_train: 0.5058 acc_train: 0.9143 loss_val: 0.7838 acc_val: 0.8200 time: 0.0095s\n",
      "Epoch: 0138 loss_train: 0.5296 acc_train: 0.9357 loss_val: 0.7828 acc_val: 0.8233 time: 0.0110s\n",
      "Epoch: 0139 loss_train: 0.5116 acc_train: 0.9286 loss_val: 0.7807 acc_val: 0.8233 time: 0.0120s\n",
      "Epoch: 0140 loss_train: 0.5021 acc_train: 0.9571 loss_val: 0.7780 acc_val: 0.8233 time: 0.0097s\n",
      "Epoch: 0141 loss_train: 0.5118 acc_train: 0.9143 loss_val: 0.7752 acc_val: 0.8267 time: 0.0091s\n",
      "Epoch: 0142 loss_train: 0.4906 acc_train: 0.9357 loss_val: 0.7713 acc_val: 0.8267 time: 0.0079s\n",
      "Epoch: 0143 loss_train: 0.4991 acc_train: 0.9214 loss_val: 0.7671 acc_val: 0.8233 time: 0.0641s\n",
      "Epoch: 0144 loss_train: 0.4802 acc_train: 0.9357 loss_val: 0.7638 acc_val: 0.8233 time: 0.0087s\n",
      "Epoch: 0145 loss_train: 0.5549 acc_train: 0.8857 loss_val: 0.7613 acc_val: 0.8233 time: 0.0090s\n",
      "Epoch: 0146 loss_train: 0.4750 acc_train: 0.9500 loss_val: 0.7583 acc_val: 0.8233 time: 0.0081s\n",
      "Epoch: 0147 loss_train: 0.5150 acc_train: 0.8714 loss_val: 0.7559 acc_val: 0.8233 time: 0.0129s\n",
      "Epoch: 0148 loss_train: 0.4750 acc_train: 0.9429 loss_val: 0.7542 acc_val: 0.8233 time: 0.0132s\n",
      "Epoch: 0149 loss_train: 0.5032 acc_train: 0.9071 loss_val: 0.7530 acc_val: 0.8233 time: 0.0324s\n",
      "Epoch: 0150 loss_train: 0.4671 acc_train: 0.9571 loss_val: 0.7522 acc_val: 0.8233 time: 0.0428s\n",
      "Epoch: 0151 loss_train: 0.4530 acc_train: 0.9286 loss_val: 0.7514 acc_val: 0.8233 time: 0.0216s\n",
      "Epoch: 0152 loss_train: 0.4803 acc_train: 0.9000 loss_val: 0.7499 acc_val: 0.8200 time: 0.0152s\n",
      "Epoch: 0153 loss_train: 0.4805 acc_train: 0.9357 loss_val: 0.7490 acc_val: 0.8133 time: 0.0164s\n",
      "Epoch: 0154 loss_train: 0.4632 acc_train: 0.9714 loss_val: 0.7479 acc_val: 0.8167 time: 0.0124s\n",
      "Epoch: 0155 loss_train: 0.4291 acc_train: 0.9357 loss_val: 0.7452 acc_val: 0.8167 time: 0.0102s\n",
      "Epoch: 0156 loss_train: 0.4608 acc_train: 0.9214 loss_val: 0.7410 acc_val: 0.8200 time: 0.0112s\n",
      "Epoch: 0157 loss_train: 0.4516 acc_train: 0.9357 loss_val: 0.7377 acc_val: 0.8233 time: 0.0091s\n",
      "Epoch: 0158 loss_train: 0.4511 acc_train: 0.9357 loss_val: 0.7347 acc_val: 0.8233 time: 0.0099s\n",
      "Epoch: 0159 loss_train: 0.4750 acc_train: 0.9214 loss_val: 0.7326 acc_val: 0.8233 time: 0.0107s\n",
      "Epoch: 0160 loss_train: 0.4536 acc_train: 0.9357 loss_val: 0.7309 acc_val: 0.8233 time: 0.0095s\n",
      "Epoch: 0161 loss_train: 0.4602 acc_train: 0.9429 loss_val: 0.7299 acc_val: 0.8200 time: 0.0113s\n",
      "Epoch: 0162 loss_train: 0.4377 acc_train: 0.9357 loss_val: 0.7278 acc_val: 0.8200 time: 0.0196s\n",
      "Epoch: 0163 loss_train: 0.4525 acc_train: 0.9286 loss_val: 0.7255 acc_val: 0.8200 time: 0.0314s\n",
      "Epoch: 0164 loss_train: 0.4382 acc_train: 0.9214 loss_val: 0.7239 acc_val: 0.8233 time: 0.0236s\n",
      "Epoch: 0165 loss_train: 0.4018 acc_train: 0.9429 loss_val: 0.7223 acc_val: 0.8233 time: 0.0121s\n",
      "Epoch: 0166 loss_train: 0.4626 acc_train: 0.9214 loss_val: 0.7203 acc_val: 0.8233 time: 0.0098s\n",
      "Epoch: 0167 loss_train: 0.4522 acc_train: 0.8857 loss_val: 0.7178 acc_val: 0.8200 time: 0.0095s\n",
      "Epoch: 0168 loss_train: 0.4444 acc_train: 0.9429 loss_val: 0.7166 acc_val: 0.8200 time: 0.0092s\n",
      "Epoch: 0169 loss_train: 0.4224 acc_train: 0.9500 loss_val: 0.7150 acc_val: 0.8233 time: 0.0079s\n",
      "Epoch: 0170 loss_train: 0.4117 acc_train: 0.9286 loss_val: 0.7134 acc_val: 0.8233 time: 0.0093s\n",
      "Epoch: 0171 loss_train: 0.4301 acc_train: 0.9429 loss_val: 0.7123 acc_val: 0.8267 time: 0.0084s\n",
      "Epoch: 0172 loss_train: 0.3677 acc_train: 0.9429 loss_val: 0.7108 acc_val: 0.8267 time: 0.0096s\n",
      "Epoch: 0173 loss_train: 0.4202 acc_train: 0.9214 loss_val: 0.7101 acc_val: 0.8267 time: 0.0096s\n",
      "Epoch: 0174 loss_train: 0.4485 acc_train: 0.9286 loss_val: 0.7085 acc_val: 0.8233 time: 0.0079s\n",
      "Epoch: 0175 loss_train: 0.4278 acc_train: 0.9286 loss_val: 0.7075 acc_val: 0.8267 time: 0.0094s\n",
      "Epoch: 0176 loss_train: 0.4477 acc_train: 0.9500 loss_val: 0.7063 acc_val: 0.8267 time: 0.0091s\n",
      "Epoch: 0177 loss_train: 0.4391 acc_train: 0.9429 loss_val: 0.7063 acc_val: 0.8267 time: 0.0103s\n",
      "Epoch: 0178 loss_train: 0.4013 acc_train: 0.9357 loss_val: 0.7048 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0179 loss_train: 0.4087 acc_train: 0.9429 loss_val: 0.7039 acc_val: 0.8233 time: 0.0093s\n",
      "Epoch: 0180 loss_train: 0.4038 acc_train: 0.9500 loss_val: 0.7033 acc_val: 0.8233 time: 0.0081s\n",
      "Epoch: 0181 loss_train: 0.3744 acc_train: 0.9500 loss_val: 0.7031 acc_val: 0.8233 time: 0.0089s\n",
      "Epoch: 0182 loss_train: 0.4121 acc_train: 0.9429 loss_val: 0.7020 acc_val: 0.8267 time: 0.0076s\n",
      "Epoch: 0183 loss_train: 0.4214 acc_train: 0.9286 loss_val: 0.7022 acc_val: 0.8233 time: 0.0094s\n",
      "Epoch: 0184 loss_train: 0.3969 acc_train: 0.9429 loss_val: 0.7025 acc_val: 0.8233 time: 0.0083s\n",
      "Epoch: 0185 loss_train: 0.3862 acc_train: 0.9643 loss_val: 0.7041 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0186 loss_train: 0.4097 acc_train: 0.9357 loss_val: 0.7057 acc_val: 0.8200 time: 0.0075s\n",
      "Epoch: 0187 loss_train: 0.4463 acc_train: 0.9143 loss_val: 0.7058 acc_val: 0.8167 time: 0.0088s\n",
      "Epoch: 0188 loss_train: 0.3825 acc_train: 0.9357 loss_val: 0.7040 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0189 loss_train: 0.4245 acc_train: 0.9214 loss_val: 0.7018 acc_val: 0.8133 time: 0.0108s\n",
      "Epoch: 0190 loss_train: 0.4209 acc_train: 0.9429 loss_val: 0.6980 acc_val: 0.8167 time: 0.0142s\n",
      "Epoch: 0191 loss_train: 0.3887 acc_train: 0.9429 loss_val: 0.6956 acc_val: 0.8133 time: 0.0171s\n",
      "Epoch: 0192 loss_train: 0.4130 acc_train: 0.9214 loss_val: 0.6926 acc_val: 0.8200 time: 0.0119s\n",
      "Epoch: 0193 loss_train: 0.4062 acc_train: 0.9429 loss_val: 0.6907 acc_val: 0.8233 time: 0.0107s\n",
      "Epoch: 0194 loss_train: 0.4005 acc_train: 0.9571 loss_val: 0.6903 acc_val: 0.8200 time: 0.0177s\n",
      "Epoch: 0195 loss_train: 0.3742 acc_train: 0.9714 loss_val: 0.6904 acc_val: 0.8167 time: 0.0100s\n",
      "Epoch: 0196 loss_train: 0.3768 acc_train: 0.9500 loss_val: 0.6907 acc_val: 0.8100 time: 0.0142s\n",
      "Epoch: 0197 loss_train: 0.4086 acc_train: 0.9214 loss_val: 0.6918 acc_val: 0.8100 time: 0.0095s\n",
      "Epoch: 0198 loss_train: 0.4493 acc_train: 0.9429 loss_val: 0.6930 acc_val: 0.8100 time: 0.0102s\n",
      "Epoch: 0199 loss_train: 0.3879 acc_train: 0.9357 loss_val: 0.6942 acc_val: 0.8167 time: 0.0100s\n",
      "Epoch: 0200 loss_train: 0.3684 acc_train: 0.9571 loss_val: 0.6947 acc_val: 0.8133 time: 0.0104s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 2.2920s\n",
      "Test set results: loss= 0.7112 accuracy= 0.8410\n",
      "inference time:  0.0026559829711914062\n"
     ]
    }
   ],
   "source": [
    "run_experiment(num_epochs=num_epochs, model=model0, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intialize a 3-layer GCN\n"
     ]
    }
   ],
   "source": [
    "model1 = GCN_3(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runrunrun!\n",
      "Epoch: 0001 loss_train: 2.0460 acc_train: 0.1429 loss_val: 2.0207 acc_val: 0.1033 time: 0.0136s\n",
      "Epoch: 0002 loss_train: 2.0118 acc_train: 0.1071 loss_val: 1.9877 acc_val: 0.1033 time: 0.0121s\n",
      "Epoch: 0003 loss_train: 1.9815 acc_train: 0.1429 loss_val: 1.9580 acc_val: 0.1533 time: 0.0112s\n",
      "Epoch: 0004 loss_train: 1.9611 acc_train: 0.0857 loss_val: 1.9313 acc_val: 0.1567 time: 0.0119s\n",
      "Epoch: 0005 loss_train: 1.9206 acc_train: 0.1500 loss_val: 1.9067 acc_val: 0.1567 time: 0.0136s\n",
      "Epoch: 0006 loss_train: 1.8993 acc_train: 0.2500 loss_val: 1.8842 acc_val: 0.1567 time: 0.0114s\n",
      "Epoch: 0007 loss_train: 1.8800 acc_train: 0.2071 loss_val: 1.8642 acc_val: 0.1567 time: 0.0130s\n",
      "Epoch: 0008 loss_train: 1.8516 acc_train: 0.2429 loss_val: 1.8463 acc_val: 0.3533 time: 0.0130s\n",
      "Epoch: 0009 loss_train: 1.8668 acc_train: 0.2071 loss_val: 1.8312 acc_val: 0.3500 time: 0.0119s\n",
      "Epoch: 0010 loss_train: 1.8489 acc_train: 0.1857 loss_val: 1.8186 acc_val: 0.3500 time: 0.0115s\n",
      "Epoch: 0011 loss_train: 1.8359 acc_train: 0.2500 loss_val: 1.8089 acc_val: 0.3500 time: 0.0124s\n",
      "Epoch: 0012 loss_train: 1.8483 acc_train: 0.2357 loss_val: 1.8014 acc_val: 0.3500 time: 0.0124s\n",
      "Epoch: 0013 loss_train: 1.8459 acc_train: 0.2500 loss_val: 1.7952 acc_val: 0.3500 time: 0.0114s\n",
      "Epoch: 0014 loss_train: 1.8258 acc_train: 0.2714 loss_val: 1.7904 acc_val: 0.3500 time: 0.0135s\n",
      "Epoch: 0015 loss_train: 1.8263 acc_train: 0.2714 loss_val: 1.7863 acc_val: 0.3500 time: 0.0133s\n",
      "Epoch: 0016 loss_train: 1.8109 acc_train: 0.2857 loss_val: 1.7824 acc_val: 0.3500 time: 0.0128s\n",
      "Epoch: 0017 loss_train: 1.7975 acc_train: 0.2929 loss_val: 1.7786 acc_val: 0.3500 time: 0.0140s\n",
      "Epoch: 0018 loss_train: 1.7816 acc_train: 0.2929 loss_val: 1.7746 acc_val: 0.3500 time: 0.0119s\n",
      "Epoch: 0019 loss_train: 1.8085 acc_train: 0.2857 loss_val: 1.7708 acc_val: 0.3500 time: 0.0109s\n",
      "Epoch: 0020 loss_train: 1.8051 acc_train: 0.3071 loss_val: 1.7670 acc_val: 0.3500 time: 0.0132s\n",
      "Epoch: 0021 loss_train: 1.8052 acc_train: 0.2643 loss_val: 1.7634 acc_val: 0.3500 time: 0.0125s\n",
      "Epoch: 0022 loss_train: 1.7986 acc_train: 0.2929 loss_val: 1.7604 acc_val: 0.3500 time: 0.0114s\n",
      "Epoch: 0023 loss_train: 1.7696 acc_train: 0.2929 loss_val: 1.7574 acc_val: 0.3500 time: 0.0142s\n",
      "Epoch: 0024 loss_train: 1.7628 acc_train: 0.2857 loss_val: 1.7540 acc_val: 0.3500 time: 0.0131s\n",
      "Epoch: 0025 loss_train: 1.7571 acc_train: 0.3143 loss_val: 1.7495 acc_val: 0.3500 time: 0.0124s\n",
      "Epoch: 0026 loss_train: 1.7571 acc_train: 0.3071 loss_val: 1.7442 acc_val: 0.3500 time: 0.0109s\n",
      "Epoch: 0027 loss_train: 1.7421 acc_train: 0.2929 loss_val: 1.7378 acc_val: 0.3500 time: 0.0140s\n",
      "Epoch: 0028 loss_train: 1.7478 acc_train: 0.2786 loss_val: 1.7305 acc_val: 0.3500 time: 0.0125s\n",
      "Epoch: 0029 loss_train: 1.7343 acc_train: 0.3500 loss_val: 1.7224 acc_val: 0.3500 time: 0.0120s\n",
      "Epoch: 0030 loss_train: 1.7014 acc_train: 0.3643 loss_val: 1.7126 acc_val: 0.3633 time: 0.0121s\n",
      "Epoch: 0031 loss_train: 1.6846 acc_train: 0.3500 loss_val: 1.7018 acc_val: 0.3633 time: 0.0766s\n",
      "Epoch: 0032 loss_train: 1.6760 acc_train: 0.3786 loss_val: 1.6897 acc_val: 0.3767 time: 0.0134s\n",
      "Epoch: 0033 loss_train: 1.6761 acc_train: 0.3429 loss_val: 1.6761 acc_val: 0.3767 time: 0.0202s\n",
      "Epoch: 0034 loss_train: 1.6428 acc_train: 0.4000 loss_val: 1.6606 acc_val: 0.3933 time: 0.0486s\n",
      "Epoch: 0035 loss_train: 1.6399 acc_train: 0.3714 loss_val: 1.6439 acc_val: 0.4067 time: 0.0525s\n",
      "Epoch: 0036 loss_train: 1.6084 acc_train: 0.3714 loss_val: 1.6261 acc_val: 0.4167 time: 0.0159s\n",
      "Epoch: 0037 loss_train: 1.5731 acc_train: 0.4357 loss_val: 1.6063 acc_val: 0.4167 time: 0.0242s\n",
      "Epoch: 0038 loss_train: 1.5601 acc_train: 0.4571 loss_val: 1.5849 acc_val: 0.4233 time: 0.0208s\n",
      "Epoch: 0039 loss_train: 1.5260 acc_train: 0.4143 loss_val: 1.5623 acc_val: 0.4267 time: 0.0134s\n",
      "Epoch: 0040 loss_train: 1.5162 acc_train: 0.4357 loss_val: 1.5386 acc_val: 0.4300 time: 0.0135s\n",
      "Epoch: 0041 loss_train: 1.4755 acc_train: 0.4357 loss_val: 1.5143 acc_val: 0.4367 time: 0.0150s\n",
      "Epoch: 0042 loss_train: 1.4668 acc_train: 0.4429 loss_val: 1.4883 acc_val: 0.4367 time: 0.0131s\n",
      "Epoch: 0043 loss_train: 1.4289 acc_train: 0.4429 loss_val: 1.4616 acc_val: 0.4433 time: 0.0215s\n",
      "Epoch: 0044 loss_train: 1.3944 acc_train: 0.4357 loss_val: 1.4346 acc_val: 0.4567 time: 0.0125s\n",
      "Epoch: 0045 loss_train: 1.3592 acc_train: 0.4643 loss_val: 1.4065 acc_val: 0.4667 time: 0.0117s\n",
      "Epoch: 0046 loss_train: 1.3052 acc_train: 0.4786 loss_val: 1.3771 acc_val: 0.4767 time: 0.0103s\n",
      "Epoch: 0047 loss_train: 1.2655 acc_train: 0.5071 loss_val: 1.3462 acc_val: 0.5300 time: 0.0121s\n",
      "Epoch: 0048 loss_train: 1.2326 acc_train: 0.5286 loss_val: 1.3150 acc_val: 0.5633 time: 0.0105s\n",
      "Epoch: 0049 loss_train: 1.2097 acc_train: 0.5500 loss_val: 1.2836 acc_val: 0.5967 time: 0.0108s\n",
      "Epoch: 0050 loss_train: 1.1579 acc_train: 0.6000 loss_val: 1.2517 acc_val: 0.6267 time: 0.0137s\n",
      "Epoch: 0051 loss_train: 1.1187 acc_train: 0.6786 loss_val: 1.2193 acc_val: 0.6400 time: 0.0119s\n",
      "Epoch: 0052 loss_train: 1.0842 acc_train: 0.6929 loss_val: 1.1869 acc_val: 0.6700 time: 0.0129s\n",
      "Epoch: 0053 loss_train: 1.0512 acc_train: 0.7000 loss_val: 1.1550 acc_val: 0.6867 time: 0.0112s\n",
      "Epoch: 0054 loss_train: 1.0335 acc_train: 0.7714 loss_val: 1.1235 acc_val: 0.7033 time: 0.0122s\n",
      "Epoch: 0055 loss_train: 0.9907 acc_train: 0.7429 loss_val: 1.0923 acc_val: 0.7000 time: 0.0131s\n",
      "Epoch: 0056 loss_train: 0.9396 acc_train: 0.7429 loss_val: 1.0616 acc_val: 0.7033 time: 0.0128s\n",
      "Epoch: 0057 loss_train: 0.8756 acc_train: 0.7714 loss_val: 1.0339 acc_val: 0.7333 time: 0.0113s\n",
      "Epoch: 0058 loss_train: 0.8501 acc_train: 0.7714 loss_val: 1.0086 acc_val: 0.7333 time: 0.0165s\n",
      "Epoch: 0059 loss_train: 0.8411 acc_train: 0.7786 loss_val: 0.9790 acc_val: 0.7367 time: 0.0101s\n",
      "Epoch: 0060 loss_train: 0.8275 acc_train: 0.7571 loss_val: 0.9517 acc_val: 0.7267 time: 0.0134s\n",
      "Epoch: 0061 loss_train: 0.8005 acc_train: 0.7714 loss_val: 0.9243 acc_val: 0.7333 time: 0.0138s\n",
      "Epoch: 0062 loss_train: 0.7464 acc_train: 0.7929 loss_val: 0.8998 acc_val: 0.7400 time: 0.0125s\n",
      "Epoch: 0063 loss_train: 0.7266 acc_train: 0.8000 loss_val: 0.8758 acc_val: 0.7400 time: 0.0222s\n",
      "Epoch: 0064 loss_train: 0.6991 acc_train: 0.8071 loss_val: 0.8527 acc_val: 0.7367 time: 0.0340s\n",
      "Epoch: 0065 loss_train: 0.6752 acc_train: 0.8071 loss_val: 0.8321 acc_val: 0.7367 time: 0.0596s\n",
      "Epoch: 0066 loss_train: 0.6495 acc_train: 0.8071 loss_val: 0.8152 acc_val: 0.7367 time: 0.0245s\n",
      "Epoch: 0067 loss_train: 0.6143 acc_train: 0.8286 loss_val: 0.8027 acc_val: 0.7400 time: 0.0384s\n",
      "Epoch: 0068 loss_train: 0.6493 acc_train: 0.8214 loss_val: 0.7934 acc_val: 0.7567 time: 0.0334s\n",
      "Epoch: 0069 loss_train: 0.5740 acc_train: 0.8214 loss_val: 0.7847 acc_val: 0.7667 time: 0.0216s\n",
      "Epoch: 0070 loss_train: 0.5362 acc_train: 0.8357 loss_val: 0.7732 acc_val: 0.7633 time: 0.0233s\n",
      "Epoch: 0071 loss_train: 0.5670 acc_train: 0.8357 loss_val: 0.7588 acc_val: 0.7667 time: 0.0237s\n",
      "Epoch: 0072 loss_train: 0.5363 acc_train: 0.8286 loss_val: 0.7417 acc_val: 0.7767 time: 0.0140s\n",
      "Epoch: 0073 loss_train: 0.4792 acc_train: 0.8714 loss_val: 0.7265 acc_val: 0.7900 time: 0.0163s\n",
      "Epoch: 0074 loss_train: 0.4826 acc_train: 0.8643 loss_val: 0.7155 acc_val: 0.8033 time: 0.0165s\n",
      "Epoch: 0075 loss_train: 0.4747 acc_train: 0.8643 loss_val: 0.7037 acc_val: 0.8067 time: 0.0508s\n",
      "Epoch: 0076 loss_train: 0.5015 acc_train: 0.8714 loss_val: 0.6949 acc_val: 0.8067 time: 0.0471s\n",
      "Epoch: 0077 loss_train: 0.4968 acc_train: 0.8714 loss_val: 0.6919 acc_val: 0.8000 time: 0.0263s\n",
      "Epoch: 0078 loss_train: 0.4552 acc_train: 0.8643 loss_val: 0.6954 acc_val: 0.7967 time: 0.0564s\n",
      "Epoch: 0079 loss_train: 0.4544 acc_train: 0.8786 loss_val: 0.6987 acc_val: 0.7933 time: 0.0656s\n",
      "Epoch: 0080 loss_train: 0.4219 acc_train: 0.8929 loss_val: 0.6951 acc_val: 0.7933 time: 0.0441s\n",
      "Epoch: 0081 loss_train: 0.4405 acc_train: 0.8643 loss_val: 0.6813 acc_val: 0.7967 time: 0.0172s\n",
      "Epoch: 0082 loss_train: 0.4014 acc_train: 0.8857 loss_val: 0.6681 acc_val: 0.8033 time: 0.0722s\n",
      "Epoch: 0083 loss_train: 0.4170 acc_train: 0.9000 loss_val: 0.6608 acc_val: 0.8000 time: 0.0235s\n",
      "Epoch: 0084 loss_train: 0.4013 acc_train: 0.8643 loss_val: 0.6575 acc_val: 0.8000 time: 0.0156s\n",
      "Epoch: 0085 loss_train: 0.3973 acc_train: 0.8857 loss_val: 0.6553 acc_val: 0.8000 time: 0.0137s\n",
      "Epoch: 0086 loss_train: 0.3719 acc_train: 0.8786 loss_val: 0.6526 acc_val: 0.8000 time: 0.0716s\n",
      "Epoch: 0087 loss_train: 0.3847 acc_train: 0.9071 loss_val: 0.6519 acc_val: 0.8000 time: 0.0615s\n",
      "Epoch: 0088 loss_train: 0.3172 acc_train: 0.9000 loss_val: 0.6553 acc_val: 0.8033 time: 0.0194s\n",
      "Epoch: 0089 loss_train: 0.3735 acc_train: 0.8929 loss_val: 0.6585 acc_val: 0.8000 time: 0.0143s\n",
      "Epoch: 0090 loss_train: 0.3904 acc_train: 0.8786 loss_val: 0.6575 acc_val: 0.8000 time: 0.0152s\n",
      "Epoch: 0091 loss_train: 0.3277 acc_train: 0.8929 loss_val: 0.6535 acc_val: 0.8067 time: 0.0174s\n",
      "Epoch: 0092 loss_train: 0.4043 acc_train: 0.9000 loss_val: 0.6442 acc_val: 0.8100 time: 0.0299s\n",
      "Epoch: 0093 loss_train: 0.3265 acc_train: 0.9214 loss_val: 0.6407 acc_val: 0.8133 time: 0.0385s\n",
      "Epoch: 0094 loss_train: 0.3385 acc_train: 0.9000 loss_val: 0.6408 acc_val: 0.8133 time: 0.0761s\n",
      "Epoch: 0095 loss_train: 0.3225 acc_train: 0.9000 loss_val: 0.6441 acc_val: 0.8067 time: 0.0415s\n",
      "Epoch: 0096 loss_train: 0.2948 acc_train: 0.9214 loss_val: 0.6462 acc_val: 0.8067 time: 0.0464s\n",
      "Epoch: 0097 loss_train: 0.3125 acc_train: 0.9143 loss_val: 0.6476 acc_val: 0.8033 time: 0.0228s\n",
      "Epoch: 0098 loss_train: 0.3126 acc_train: 0.9143 loss_val: 0.6452 acc_val: 0.8033 time: 0.0176s\n",
      "Epoch: 0099 loss_train: 0.2998 acc_train: 0.9357 loss_val: 0.6461 acc_val: 0.8033 time: 0.0197s\n",
      "Epoch: 0100 loss_train: 0.2827 acc_train: 0.9214 loss_val: 0.6448 acc_val: 0.8167 time: 0.0244s\n",
      "Epoch: 0101 loss_train: 0.2660 acc_train: 0.9286 loss_val: 0.6437 acc_val: 0.8200 time: 0.0203s\n",
      "Epoch: 0102 loss_train: 0.2557 acc_train: 0.9429 loss_val: 0.6443 acc_val: 0.8167 time: 0.0279s\n",
      "Epoch: 0103 loss_train: 0.2822 acc_train: 0.9071 loss_val: 0.6496 acc_val: 0.8000 time: 0.0610s\n",
      "Epoch: 0104 loss_train: 0.2705 acc_train: 0.9500 loss_val: 0.6535 acc_val: 0.8000 time: 0.0212s\n",
      "Epoch: 0105 loss_train: 0.2866 acc_train: 0.9357 loss_val: 0.6540 acc_val: 0.7967 time: 0.0146s\n",
      "Epoch: 0106 loss_train: 0.2540 acc_train: 0.9286 loss_val: 0.6434 acc_val: 0.8067 time: 0.0142s\n",
      "Epoch: 0107 loss_train: 0.2754 acc_train: 0.9357 loss_val: 0.6331 acc_val: 0.8067 time: 0.0168s\n",
      "Epoch: 0108 loss_train: 0.2704 acc_train: 0.9286 loss_val: 0.6313 acc_val: 0.8133 time: 0.0140s\n",
      "Epoch: 0109 loss_train: 0.2196 acc_train: 0.9571 loss_val: 0.6338 acc_val: 0.8133 time: 0.0130s\n",
      "Epoch: 0110 loss_train: 0.2379 acc_train: 0.9500 loss_val: 0.6340 acc_val: 0.8067 time: 0.0123s\n",
      "Epoch: 0111 loss_train: 0.2578 acc_train: 0.9214 loss_val: 0.6395 acc_val: 0.8033 time: 0.0151s\n",
      "Epoch: 0112 loss_train: 0.2584 acc_train: 0.9429 loss_val: 0.6472 acc_val: 0.8067 time: 0.0166s\n",
      "Epoch: 0113 loss_train: 0.2420 acc_train: 0.9357 loss_val: 0.6485 acc_val: 0.8133 time: 0.0182s\n",
      "Epoch: 0114 loss_train: 0.2562 acc_train: 0.9429 loss_val: 0.6458 acc_val: 0.8067 time: 0.0158s\n",
      "Epoch: 0115 loss_train: 0.2363 acc_train: 0.9357 loss_val: 0.6458 acc_val: 0.8033 time: 0.0128s\n",
      "Epoch: 0116 loss_train: 0.2202 acc_train: 0.9357 loss_val: 0.6470 acc_val: 0.8033 time: 0.0131s\n",
      "Epoch: 0117 loss_train: 0.2201 acc_train: 0.9643 loss_val: 0.6532 acc_val: 0.7967 time: 0.0128s\n",
      "Epoch: 0118 loss_train: 0.2616 acc_train: 0.9286 loss_val: 0.6535 acc_val: 0.8000 time: 0.0146s\n",
      "Epoch: 0119 loss_train: 0.2288 acc_train: 0.9500 loss_val: 0.6486 acc_val: 0.8033 time: 0.0154s\n",
      "Epoch: 0120 loss_train: 0.2334 acc_train: 0.9429 loss_val: 0.6453 acc_val: 0.8133 time: 0.0175s\n",
      "Epoch: 0121 loss_train: 0.2311 acc_train: 0.9643 loss_val: 0.6466 acc_val: 0.8133 time: 0.0148s\n",
      "Epoch: 0122 loss_train: 0.2224 acc_train: 0.9571 loss_val: 0.6479 acc_val: 0.8100 time: 0.0151s\n",
      "Epoch: 0123 loss_train: 0.2271 acc_train: 0.9357 loss_val: 0.6430 acc_val: 0.8067 time: 0.0232s\n",
      "Epoch: 0124 loss_train: 0.2135 acc_train: 0.9214 loss_val: 0.6424 acc_val: 0.8067 time: 0.0155s\n",
      "Epoch: 0125 loss_train: 0.2578 acc_train: 0.9286 loss_val: 0.6450 acc_val: 0.8100 time: 0.0240s\n",
      "Epoch: 0126 loss_train: 0.2204 acc_train: 0.9500 loss_val: 0.6594 acc_val: 0.8100 time: 0.0132s\n",
      "Epoch: 0127 loss_train: 0.2447 acc_train: 0.9143 loss_val: 0.6770 acc_val: 0.7967 time: 0.0124s\n",
      "Epoch: 0128 loss_train: 0.2063 acc_train: 0.9643 loss_val: 0.7010 acc_val: 0.7867 time: 0.0127s\n",
      "Epoch: 0129 loss_train: 0.2373 acc_train: 0.9214 loss_val: 0.6986 acc_val: 0.7933 time: 0.0129s\n",
      "Epoch: 0130 loss_train: 0.2191 acc_train: 0.9429 loss_val: 0.6820 acc_val: 0.7867 time: 0.0147s\n",
      "Epoch: 0131 loss_train: 0.2009 acc_train: 0.9643 loss_val: 0.6594 acc_val: 0.8000 time: 0.0133s\n",
      "Epoch: 0132 loss_train: 0.2086 acc_train: 0.9500 loss_val: 0.6423 acc_val: 0.8100 time: 0.0105s\n",
      "Epoch: 0133 loss_train: 0.2152 acc_train: 0.9357 loss_val: 0.6384 acc_val: 0.8067 time: 0.0127s\n",
      "Epoch: 0134 loss_train: 0.2019 acc_train: 0.9643 loss_val: 0.6417 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0135 loss_train: 0.2054 acc_train: 0.9643 loss_val: 0.6502 acc_val: 0.7967 time: 0.0134s\n",
      "Epoch: 0136 loss_train: 0.1718 acc_train: 0.9786 loss_val: 0.6613 acc_val: 0.7900 time: 0.0116s\n",
      "Epoch: 0137 loss_train: 0.2279 acc_train: 0.9571 loss_val: 0.6843 acc_val: 0.7900 time: 0.0132s\n",
      "Epoch: 0138 loss_train: 0.1926 acc_train: 0.9643 loss_val: 0.7145 acc_val: 0.7800 time: 0.0138s\n",
      "Epoch: 0139 loss_train: 0.2326 acc_train: 0.9357 loss_val: 0.7130 acc_val: 0.7833 time: 0.0730s\n",
      "Epoch: 0140 loss_train: 0.1850 acc_train: 0.9714 loss_val: 0.7027 acc_val: 0.7867 time: 0.0147s\n",
      "Epoch: 0141 loss_train: 0.1947 acc_train: 0.9643 loss_val: 0.6849 acc_val: 0.7900 time: 0.0187s\n",
      "Epoch: 0142 loss_train: 0.2233 acc_train: 0.9357 loss_val: 0.6651 acc_val: 0.7933 time: 0.0346s\n",
      "Epoch: 0143 loss_train: 0.2072 acc_train: 0.9357 loss_val: 0.6435 acc_val: 0.7967 time: 0.0235s\n",
      "Epoch: 0144 loss_train: 0.1711 acc_train: 0.9786 loss_val: 0.6401 acc_val: 0.7967 time: 0.0584s\n",
      "Epoch: 0145 loss_train: 0.1872 acc_train: 0.9643 loss_val: 0.6525 acc_val: 0.8067 time: 0.0243s\n",
      "Epoch: 0146 loss_train: 0.2320 acc_train: 0.9500 loss_val: 0.6706 acc_val: 0.7933 time: 0.0178s\n",
      "Epoch: 0147 loss_train: 0.1773 acc_train: 0.9571 loss_val: 0.6889 acc_val: 0.7867 time: 0.0141s\n",
      "Epoch: 0148 loss_train: 0.1940 acc_train: 0.9429 loss_val: 0.6896 acc_val: 0.8000 time: 0.0301s\n",
      "Epoch: 0149 loss_train: 0.1791 acc_train: 0.9571 loss_val: 0.6852 acc_val: 0.7900 time: 0.0290s\n",
      "Epoch: 0150 loss_train: 0.2179 acc_train: 0.9714 loss_val: 0.6914 acc_val: 0.7900 time: 0.0253s\n",
      "Epoch: 0151 loss_train: 0.1860 acc_train: 0.9643 loss_val: 0.6921 acc_val: 0.8000 time: 0.0283s\n",
      "Epoch: 0152 loss_train: 0.1862 acc_train: 0.9643 loss_val: 0.6757 acc_val: 0.8000 time: 0.0196s\n",
      "Epoch: 0153 loss_train: 0.1886 acc_train: 0.9643 loss_val: 0.6581 acc_val: 0.8000 time: 0.0178s\n",
      "Epoch: 0154 loss_train: 0.1929 acc_train: 0.9643 loss_val: 0.6533 acc_val: 0.8000 time: 0.0134s\n",
      "Epoch: 0155 loss_train: 0.1655 acc_train: 0.9714 loss_val: 0.6526 acc_val: 0.8067 time: 0.0172s\n",
      "Epoch: 0156 loss_train: 0.1752 acc_train: 0.9571 loss_val: 0.6566 acc_val: 0.8100 time: 0.0176s\n",
      "Epoch: 0157 loss_train: 0.1653 acc_train: 0.9786 loss_val: 0.6607 acc_val: 0.8033 time: 0.0133s\n",
      "Epoch: 0158 loss_train: 0.1772 acc_train: 0.9643 loss_val: 0.6610 acc_val: 0.8133 time: 0.0124s\n",
      "Epoch: 0159 loss_train: 0.1840 acc_train: 0.9357 loss_val: 0.6717 acc_val: 0.8033 time: 0.0117s\n",
      "Epoch: 0160 loss_train: 0.1479 acc_train: 0.9857 loss_val: 0.6747 acc_val: 0.8000 time: 0.0094s\n",
      "Epoch: 0161 loss_train: 0.1934 acc_train: 0.9571 loss_val: 0.6760 acc_val: 0.7933 time: 0.0103s\n",
      "Epoch: 0162 loss_train: 0.1933 acc_train: 0.9429 loss_val: 0.6698 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0163 loss_train: 0.1745 acc_train: 0.9643 loss_val: 0.6643 acc_val: 0.8100 time: 0.0106s\n",
      "Epoch: 0164 loss_train: 0.1809 acc_train: 0.9571 loss_val: 0.6577 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0165 loss_train: 0.2120 acc_train: 0.9571 loss_val: 0.6647 acc_val: 0.8100 time: 0.0118s\n",
      "Epoch: 0166 loss_train: 0.1593 acc_train: 0.9643 loss_val: 0.6813 acc_val: 0.8033 time: 0.0104s\n",
      "Epoch: 0167 loss_train: 0.1642 acc_train: 0.9429 loss_val: 0.6952 acc_val: 0.7967 time: 0.0098s\n",
      "Epoch: 0168 loss_train: 0.1717 acc_train: 0.9714 loss_val: 0.7017 acc_val: 0.7900 time: 0.0103s\n",
      "Epoch: 0169 loss_train: 0.1441 acc_train: 0.9714 loss_val: 0.6991 acc_val: 0.7933 time: 0.0104s\n",
      "Epoch: 0170 loss_train: 0.1653 acc_train: 0.9714 loss_val: 0.7068 acc_val: 0.7833 time: 0.0105s\n",
      "Epoch: 0171 loss_train: 0.1451 acc_train: 0.9786 loss_val: 0.7202 acc_val: 0.7833 time: 0.0093s\n",
      "Epoch: 0172 loss_train: 0.1907 acc_train: 0.9500 loss_val: 0.7116 acc_val: 0.7833 time: 0.0154s\n",
      "Epoch: 0173 loss_train: 0.1774 acc_train: 0.9571 loss_val: 0.6934 acc_val: 0.7933 time: 0.0276s\n",
      "Epoch: 0174 loss_train: 0.1623 acc_train: 0.9571 loss_val: 0.6758 acc_val: 0.8067 time: 0.0334s\n",
      "Epoch: 0175 loss_train: 0.1833 acc_train: 0.9571 loss_val: 0.6669 acc_val: 0.7900 time: 0.0308s\n",
      "Epoch: 0176 loss_train: 0.1730 acc_train: 0.9643 loss_val: 0.6751 acc_val: 0.7933 time: 0.0131s\n",
      "Epoch: 0177 loss_train: 0.1332 acc_train: 0.9786 loss_val: 0.6934 acc_val: 0.7933 time: 0.0120s\n",
      "Epoch: 0178 loss_train: 0.1467 acc_train: 0.9714 loss_val: 0.7187 acc_val: 0.7800 time: 0.0130s\n",
      "Epoch: 0179 loss_train: 0.1485 acc_train: 0.9786 loss_val: 0.7455 acc_val: 0.7767 time: 0.0149s\n",
      "Epoch: 0180 loss_train: 0.1640 acc_train: 0.9786 loss_val: 0.7637 acc_val: 0.7833 time: 0.0130s\n",
      "Epoch: 0181 loss_train: 0.1868 acc_train: 0.9714 loss_val: 0.7481 acc_val: 0.7733 time: 0.0107s\n",
      "Epoch: 0182 loss_train: 0.1376 acc_train: 0.9714 loss_val: 0.7241 acc_val: 0.7733 time: 0.0110s\n",
      "Epoch: 0183 loss_train: 0.1392 acc_train: 0.9714 loss_val: 0.6978 acc_val: 0.7933 time: 0.0122s\n",
      "Epoch: 0184 loss_train: 0.2020 acc_train: 0.9571 loss_val: 0.6882 acc_val: 0.8000 time: 0.0096s\n",
      "Epoch: 0185 loss_train: 0.1476 acc_train: 0.9643 loss_val: 0.6847 acc_val: 0.8033 time: 0.0107s\n",
      "Epoch: 0186 loss_train: 0.1306 acc_train: 0.9714 loss_val: 0.6903 acc_val: 0.8000 time: 0.0104s\n",
      "Epoch: 0187 loss_train: 0.1410 acc_train: 0.9571 loss_val: 0.6946 acc_val: 0.8000 time: 0.0105s\n",
      "Epoch: 0188 loss_train: 0.1514 acc_train: 0.9571 loss_val: 0.7036 acc_val: 0.8033 time: 0.0106s\n",
      "Epoch: 0189 loss_train: 0.1657 acc_train: 0.9571 loss_val: 0.7129 acc_val: 0.8033 time: 0.0115s\n",
      "Epoch: 0190 loss_train: 0.1671 acc_train: 0.9571 loss_val: 0.7245 acc_val: 0.7933 time: 0.0106s\n",
      "Epoch: 0191 loss_train: 0.1602 acc_train: 0.9714 loss_val: 0.7229 acc_val: 0.7900 time: 0.0113s\n",
      "Epoch: 0192 loss_train: 0.1636 acc_train: 0.9643 loss_val: 0.7223 acc_val: 0.7933 time: 0.0111s\n",
      "Epoch: 0193 loss_train: 0.1362 acc_train: 0.9786 loss_val: 0.7134 acc_val: 0.7967 time: 0.0106s\n",
      "Epoch: 0194 loss_train: 0.1514 acc_train: 0.9714 loss_val: 0.7057 acc_val: 0.8033 time: 0.0099s\n",
      "Epoch: 0195 loss_train: 0.1527 acc_train: 0.9714 loss_val: 0.7129 acc_val: 0.7900 time: 0.0116s\n",
      "Epoch: 0196 loss_train: 0.1244 acc_train: 0.9857 loss_val: 0.7167 acc_val: 0.7933 time: 0.0095s\n",
      "Epoch: 0197 loss_train: 0.1315 acc_train: 0.9714 loss_val: 0.7278 acc_val: 0.7833 time: 0.0105s\n",
      "Epoch: 0198 loss_train: 0.1515 acc_train: 0.9571 loss_val: 0.7340 acc_val: 0.7867 time: 0.0106s\n",
      "Epoch: 0199 loss_train: 0.1448 acc_train: 0.9571 loss_val: 0.7214 acc_val: 0.7933 time: 0.0107s\n",
      "Epoch: 0200 loss_train: 0.1368 acc_train: 0.9786 loss_val: 0.7110 acc_val: 0.7867 time: 0.0111s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 3.8974s\n",
      "Test set results: loss= 0.5779 accuracy= 0.8180\n",
      "inference time:  0.0029211044311523438\n"
     ]
    }
   ],
   "source": [
    "run_experiment(num_epochs=num_epochs, model=model1, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unspecified or invalid number of iterations for inference. Treat as the same as training iterations.\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n"
     ]
    }
   ],
   "source": [
    "model2 = ite_GCN(nfeat=features.shape[1],\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0,\n",
    "            train_nite= 3,\n",
    "            eval_nite= 0,\n",
    "            allow_grad=True,\n",
    "            smooth_fac=smooth_fac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runrunrun!\n",
      "Epoch: 0001 loss_train: 1.9457 acc_train: 0.0786 loss_val: 2.0765 acc_val: 0.3500 time: 0.9939s\n",
      "Epoch: 0002 loss_train: 2.1026 acc_train: 0.2929 loss_val: 1.9560 acc_val: 0.0833 time: 0.9755s\n",
      "Epoch: 0003 loss_train: 1.9560 acc_train: 0.0786 loss_val: 1.9375 acc_val: 0.1567 time: 0.8698s\n",
      "Epoch: 0004 loss_train: 1.9371 acc_train: 0.2000 loss_val: 1.9321 acc_val: 0.1667 time: 0.8545s\n",
      "Epoch: 0005 loss_train: 1.9314 acc_train: 0.2143 loss_val: 1.9125 acc_val: 0.1700 time: 0.8433s\n",
      "Epoch: 0006 loss_train: 1.9105 acc_train: 0.2214 loss_val: 1.8603 acc_val: 0.1733 time: 0.8820s\n",
      "Epoch: 0007 loss_train: 1.8570 acc_train: 0.2357 loss_val: 1.8185 acc_val: 0.3500 time: 0.7809s\n",
      "Epoch: 0008 loss_train: 1.8138 acc_train: 0.3000 loss_val: 1.7757 acc_val: 0.3500 time: 0.8151s\n",
      "Epoch: 0009 loss_train: 1.7822 acc_train: 0.2929 loss_val: 1.7334 acc_val: 0.3500 time: 0.8860s\n",
      "Epoch: 0010 loss_train: 1.7343 acc_train: 0.2929 loss_val: 1.7101 acc_val: 0.3500 time: 0.8302s\n",
      "Epoch: 0011 loss_train: 1.7025 acc_train: 0.2929 loss_val: 1.6231 acc_val: 0.3500 time: 0.8119s\n",
      "Epoch: 0012 loss_train: 1.6111 acc_train: 0.2929 loss_val: 1.5111 acc_val: 0.3500 time: 0.8617s\n",
      "Epoch: 0013 loss_train: 1.5090 acc_train: 0.2929 loss_val: 1.4547 acc_val: 0.3600 time: 0.8496s\n",
      "Epoch: 0014 loss_train: 1.4280 acc_train: 0.3000 loss_val: 1.4216 acc_val: 0.4133 time: 0.8553s\n",
      "Epoch: 0015 loss_train: 1.3756 acc_train: 0.3571 loss_val: 1.3677 acc_val: 0.4200 time: 0.8166s\n",
      "Epoch: 0016 loss_train: 1.3119 acc_train: 0.3857 loss_val: 1.2952 acc_val: 0.4300 time: 0.8395s\n",
      "Epoch: 0017 loss_train: 1.2241 acc_train: 0.4071 loss_val: 1.2662 acc_val: 0.5000 time: 0.8984s\n",
      "Epoch: 0018 loss_train: 1.1101 acc_train: 0.5357 loss_val: 1.2522 acc_val: 0.4367 time: 0.7929s\n",
      "Epoch: 0019 loss_train: 1.0523 acc_train: 0.4071 loss_val: 1.2808 acc_val: 0.6200 time: 0.8527s\n",
      "Epoch: 0020 loss_train: 1.0138 acc_train: 0.7357 loss_val: 1.1151 acc_val: 0.5900 time: 0.9037s\n",
      "Epoch: 0021 loss_train: 0.7992 acc_train: 0.6786 loss_val: 1.1929 acc_val: 0.5533 time: 0.9789s\n",
      "Epoch: 0022 loss_train: 0.7976 acc_train: 0.6500 loss_val: 1.0555 acc_val: 0.6467 time: 0.8269s\n",
      "Epoch: 0023 loss_train: 0.6160 acc_train: 0.8000 loss_val: 1.0482 acc_val: 0.6967 time: 0.8777s\n",
      "Epoch: 0024 loss_train: 0.5309 acc_train: 0.8429 loss_val: 1.1596 acc_val: 0.7233 time: 0.8736s\n",
      "Epoch: 0025 loss_train: 0.4079 acc_train: 0.9071 loss_val: 1.1776 acc_val: 0.7133 time: 0.8777s\n",
      "Epoch: 0026 loss_train: 0.4006 acc_train: 0.8714 loss_val: 1.3623 acc_val: 0.6833 time: 0.7903s\n",
      "Epoch: 0027 loss_train: 0.3038 acc_train: 0.9000 loss_val: 1.1646 acc_val: 0.7467 time: 0.8229s\n",
      "Epoch: 0028 loss_train: 0.2389 acc_train: 0.9000 loss_val: 1.3379 acc_val: 0.7400 time: 0.8812s\n",
      "Epoch: 0029 loss_train: 0.2083 acc_train: 0.9429 loss_val: 1.5567 acc_val: 0.7333 time: 0.8137s\n",
      "Epoch: 0030 loss_train: 0.1722 acc_train: 0.9286 loss_val: 1.5806 acc_val: 0.7133 time: 0.9707s\n",
      "Epoch: 0031 loss_train: 0.1910 acc_train: 0.9214 loss_val: 1.6555 acc_val: 0.7467 time: 0.9139s\n",
      "Epoch: 0032 loss_train: 0.1087 acc_train: 0.9571 loss_val: 1.9551 acc_val: 0.7200 time: 0.8847s\n",
      "Epoch: 0033 loss_train: 0.1293 acc_train: 0.9500 loss_val: 1.7858 acc_val: 0.7367 time: 0.8194s\n",
      "Epoch: 0034 loss_train: 0.1057 acc_train: 0.9714 loss_val: 1.8653 acc_val: 0.7467 time: 0.8783s\n",
      "Epoch: 0035 loss_train: 0.0649 acc_train: 0.9786 loss_val: 2.2120 acc_val: 0.7233 time: 0.8899s\n",
      "Epoch: 0036 loss_train: 0.0815 acc_train: 0.9929 loss_val: 1.9956 acc_val: 0.7433 time: 0.8495s\n",
      "Epoch: 0037 loss_train: 0.0421 acc_train: 0.9929 loss_val: 1.9488 acc_val: 0.7600 time: 0.8339s\n",
      "Epoch: 0038 loss_train: 0.0466 acc_train: 0.9786 loss_val: 2.0535 acc_val: 0.7567 time: 0.8266s\n",
      "Epoch: 0039 loss_train: 0.0253 acc_train: 1.0000 loss_val: 2.1878 acc_val: 0.7300 time: 0.9026s\n",
      "Epoch: 0040 loss_train: 0.0459 acc_train: 0.9786 loss_val: 1.9463 acc_val: 0.7600 time: 0.7758s\n",
      "Epoch: 0041 loss_train: 0.0266 acc_train: 1.0000 loss_val: 1.9774 acc_val: 0.7467 time: 0.8407s\n",
      "Epoch: 0042 loss_train: 0.0312 acc_train: 0.9929 loss_val: 2.1072 acc_val: 0.7600 time: 0.8919s\n",
      "Epoch: 0043 loss_train: 0.0157 acc_train: 1.0000 loss_val: 2.2703 acc_val: 0.7467 time: 0.8716s\n",
      "Epoch: 0044 loss_train: 0.0281 acc_train: 1.0000 loss_val: 2.1127 acc_val: 0.7667 time: 0.8245s\n",
      "Epoch: 0045 loss_train: 0.0129 acc_train: 1.0000 loss_val: 2.0594 acc_val: 0.7500 time: 0.8952s\n",
      "Epoch: 0046 loss_train: 0.0191 acc_train: 1.0000 loss_val: 2.0585 acc_val: 0.7567 time: 0.8638s\n",
      "Epoch: 0047 loss_train: 0.0150 acc_train: 1.0000 loss_val: 2.1018 acc_val: 0.7667 time: 0.8034s\n",
      "Epoch: 0048 loss_train: 0.0127 acc_train: 1.0000 loss_val: 2.0699 acc_val: 0.7667 time: 0.8433s\n",
      "Epoch: 0049 loss_train: 0.0134 acc_train: 1.0000 loss_val: 1.9690 acc_val: 0.7833 time: 0.8523s\n",
      "Epoch: 0050 loss_train: 0.0130 acc_train: 1.0000 loss_val: 1.9018 acc_val: 0.7667 time: 0.8732s\n",
      "Epoch: 0051 loss_train: 0.0118 acc_train: 1.0000 loss_val: 1.9042 acc_val: 0.7700 time: 0.8265s\n",
      "Epoch: 0052 loss_train: 0.0123 acc_train: 1.0000 loss_val: 1.9079 acc_val: 0.7700 time: 0.8582s\n",
      "Epoch: 0053 loss_train: 0.0119 acc_train: 1.0000 loss_val: 1.8725 acc_val: 0.7733 time: 0.8767s\n",
      "Epoch: 0054 loss_train: 0.0115 acc_train: 1.0000 loss_val: 1.8203 acc_val: 0.7733 time: 0.8583s\n",
      "Epoch: 0055 loss_train: 0.0137 acc_train: 1.0000 loss_val: 1.7661 acc_val: 0.7767 time: 0.8462s\n",
      "Epoch: 0056 loss_train: 0.0125 acc_train: 1.0000 loss_val: 1.7480 acc_val: 0.7700 time: 0.9145s\n",
      "Epoch: 0057 loss_train: 0.0143 acc_train: 1.0000 loss_val: 1.7125 acc_val: 0.7733 time: 0.8455s\n",
      "Epoch: 0058 loss_train: 0.0139 acc_train: 1.0000 loss_val: 1.6887 acc_val: 0.7767 time: 0.7699s\n",
      "Epoch: 0059 loss_train: 0.0154 acc_train: 1.0000 loss_val: 1.6737 acc_val: 0.7733 time: 0.8491s\n",
      "Epoch: 0060 loss_train: 0.0151 acc_train: 1.0000 loss_val: 1.6551 acc_val: 0.7633 time: 0.8306s\n",
      "Epoch: 0061 loss_train: 0.0166 acc_train: 1.0000 loss_val: 1.6126 acc_val: 0.7800 time: 0.9030s\n",
      "Epoch: 0062 loss_train: 0.0164 acc_train: 1.0000 loss_val: 1.5902 acc_val: 0.7733 time: 0.7944s\n",
      "Epoch: 0063 loss_train: 0.0181 acc_train: 1.0000 loss_val: 1.5829 acc_val: 0.7733 time: 0.8245s\n",
      "Epoch: 0064 loss_train: 0.0183 acc_train: 1.0000 loss_val: 1.5686 acc_val: 0.7733 time: 0.8922s\n",
      "Epoch: 0065 loss_train: 0.0185 acc_train: 1.0000 loss_val: 1.5588 acc_val: 0.7767 time: 0.8507s\n",
      "Epoch: 0066 loss_train: 0.0199 acc_train: 1.0000 loss_val: 1.5529 acc_val: 0.7667 time: 0.7972s\n",
      "Epoch: 0067 loss_train: 0.0200 acc_train: 1.0000 loss_val: 1.5447 acc_val: 0.7767 time: 0.8673s\n",
      "Epoch: 0068 loss_train: 0.0198 acc_train: 1.0000 loss_val: 1.5063 acc_val: 0.7800 time: 0.8803s\n",
      "Epoch: 0069 loss_train: 0.0199 acc_train: 1.0000 loss_val: 1.5267 acc_val: 0.7800 time: 0.7846s\n",
      "Epoch: 0070 loss_train: 0.0191 acc_train: 1.0000 loss_val: 1.5107 acc_val: 0.7800 time: 0.8665s\n",
      "Epoch: 0071 loss_train: 0.0184 acc_train: 1.0000 loss_val: 1.5088 acc_val: 0.7800 time: 0.8215s\n",
      "Epoch: 0072 loss_train: 0.0180 acc_train: 1.0000 loss_val: 1.5259 acc_val: 0.7800 time: 0.8749s\n",
      "Epoch: 0073 loss_train: 0.0178 acc_train: 1.0000 loss_val: 1.5037 acc_val: 0.7800 time: 0.7875s\n",
      "Epoch: 0074 loss_train: 0.0180 acc_train: 1.0000 loss_val: 1.5533 acc_val: 0.7667 time: 0.8318s\n",
      "Epoch: 0075 loss_train: 0.0199 acc_train: 1.0000 loss_val: 1.5681 acc_val: 0.7600 time: 0.8681s\n",
      "Epoch: 0076 loss_train: 0.0297 acc_train: 1.0000 loss_val: 1.8564 acc_val: 0.7033 time: 0.8565s\n",
      "Epoch: 0077 loss_train: 0.1240 acc_train: 0.9571 loss_val: 5.7555 acc_val: 0.2633 time: 0.7685s\n",
      "Epoch: 0078 loss_train: 3.4474 acc_train: 0.3214 loss_val: 6.2639 acc_val: 0.4533 time: 0.8603s\n",
      "Epoch: 0079 loss_train: 4.3515 acc_train: 0.4571 loss_val: 3.1761 acc_val: 0.6100 time: 0.8664s\n",
      "Epoch: 0080 loss_train: 1.8077 acc_train: 0.7071 loss_val: 2.1757 acc_val: 0.5567 time: 0.8359s\n",
      "Epoch: 0081 loss_train: 0.9768 acc_train: 0.6357 loss_val: 2.7942 acc_val: 0.4033 time: 0.7996s\n",
      "Epoch: 0082 loss_train: 1.6923 acc_train: 0.5357 loss_val: 1.7440 acc_val: 0.4800 time: 0.8376s\n",
      "Epoch: 0083 loss_train: 0.8919 acc_train: 0.6286 loss_val: 1.7811 acc_val: 0.4067 time: 0.8541s\n",
      "Epoch: 0084 loss_train: 1.0250 acc_train: 0.5286 loss_val: 1.4678 acc_val: 0.4800 time: 0.7687s\n",
      "Epoch: 0085 loss_train: 0.8477 acc_train: 0.6714 loss_val: 1.2267 acc_val: 0.5767 time: 0.8340s\n",
      "Epoch: 0086 loss_train: 0.7545 acc_train: 0.7000 loss_val: 1.1687 acc_val: 0.5700 time: 0.8137s\n",
      "Epoch: 0087 loss_train: 0.7930 acc_train: 0.6857 loss_val: 1.1559 acc_val: 0.5667 time: 0.8602s\n",
      "Epoch: 0088 loss_train: 0.8071 acc_train: 0.6357 loss_val: 1.0903 acc_val: 0.6067 time: 0.7758s\n",
      "Epoch: 0089 loss_train: 0.7182 acc_train: 0.6857 loss_val: 1.0489 acc_val: 0.6600 time: 0.8042s\n",
      "Epoch: 0090 loss_train: 0.6302 acc_train: 0.7286 loss_val: 1.0476 acc_val: 0.6600 time: 0.8567s\n",
      "Epoch: 0091 loss_train: 0.5872 acc_train: 0.7714 loss_val: 1.0145 acc_val: 0.6433 time: 0.8291s\n",
      "Epoch: 0092 loss_train: 0.5361 acc_train: 0.7571 loss_val: 0.9427 acc_val: 0.6667 time: 0.8850s\n",
      "Epoch: 0093 loss_train: 0.4636 acc_train: 0.8000 loss_val: 0.9010 acc_val: 0.7367 time: 0.8304s\n",
      "Epoch: 0094 loss_train: 0.4144 acc_train: 0.8571 loss_val: 0.8729 acc_val: 0.7600 time: 0.8382s\n",
      "Epoch: 0095 loss_train: 0.3641 acc_train: 0.8786 loss_val: 0.8651 acc_val: 0.7733 time: 0.7476s\n",
      "Epoch: 0096 loss_train: 0.2977 acc_train: 0.9286 loss_val: 0.8828 acc_val: 0.7700 time: 0.8092s\n",
      "Epoch: 0097 loss_train: 0.2255 acc_train: 0.9571 loss_val: 0.9824 acc_val: 0.7700 time: 0.8345s\n",
      "Epoch: 0098 loss_train: 0.1803 acc_train: 0.9571 loss_val: 1.1278 acc_val: 0.7500 time: 0.7474s\n",
      "Epoch: 0099 loss_train: 0.1578 acc_train: 0.9643 loss_val: 1.2088 acc_val: 0.7633 time: 0.8352s\n",
      "Epoch: 0100 loss_train: 0.1252 acc_train: 0.9714 loss_val: 1.2392 acc_val: 0.7767 time: 0.8268s\n",
      "Epoch: 0101 loss_train: 0.0923 acc_train: 0.9786 loss_val: 1.3060 acc_val: 0.7867 time: 0.8422s\n",
      "Epoch: 0102 loss_train: 0.0746 acc_train: 0.9786 loss_val: 1.4200 acc_val: 0.7600 time: 0.7603s\n",
      "Epoch: 0103 loss_train: 0.0642 acc_train: 0.9786 loss_val: 1.5657 acc_val: 0.7700 time: 0.8307s\n",
      "Epoch: 0104 loss_train: 0.0505 acc_train: 0.9786 loss_val: 1.6792 acc_val: 0.7700 time: 0.8805s\n",
      "Epoch: 0105 loss_train: 0.0385 acc_train: 0.9929 loss_val: 1.7857 acc_val: 0.7700 time: 0.7969s\n",
      "Epoch: 0106 loss_train: 0.0322 acc_train: 0.9929 loss_val: 1.8688 acc_val: 0.7633 time: 0.8189s\n",
      "Epoch: 0107 loss_train: 0.0274 acc_train: 0.9929 loss_val: 1.8700 acc_val: 0.7800 time: 0.8691s\n",
      "Epoch: 0108 loss_train: 0.0201 acc_train: 0.9929 loss_val: 1.8647 acc_val: 0.7767 time: 0.8509s\n",
      "Epoch: 0109 loss_train: 0.0169 acc_train: 1.0000 loss_val: 1.9212 acc_val: 0.7633 time: 0.7519s\n",
      "Epoch: 0110 loss_train: 0.0149 acc_train: 1.0000 loss_val: 1.9469 acc_val: 0.7600 time: 0.8153s\n",
      "Epoch: 0111 loss_train: 0.0118 acc_train: 1.0000 loss_val: 1.9465 acc_val: 0.7633 time: 0.8200s\n",
      "Epoch: 0112 loss_train: 0.0091 acc_train: 1.0000 loss_val: 1.9699 acc_val: 0.7667 time: 0.8478s\n",
      "Epoch: 0113 loss_train: 0.0086 acc_train: 1.0000 loss_val: 1.9946 acc_val: 0.7633 time: 0.8579s\n",
      "Epoch: 0114 loss_train: 0.0078 acc_train: 1.0000 loss_val: 2.0041 acc_val: 0.7600 time: 0.8417s\n",
      "Epoch: 0115 loss_train: 0.0072 acc_train: 1.0000 loss_val: 1.9950 acc_val: 0.7733 time: 1.0162s\n",
      "Epoch: 0116 loss_train: 0.0068 acc_train: 1.0000 loss_val: 1.9735 acc_val: 0.7733 time: 0.8754s\n",
      "Epoch: 0117 loss_train: 0.0071 acc_train: 1.0000 loss_val: 1.9424 acc_val: 0.7733 time: 0.8735s\n",
      "Epoch: 0118 loss_train: 0.0071 acc_train: 1.0000 loss_val: 1.9210 acc_val: 0.7667 time: 0.9537s\n",
      "Epoch: 0119 loss_train: 0.0071 acc_train: 1.0000 loss_val: 1.8935 acc_val: 0.7733 time: 0.9227s\n",
      "Epoch: 0120 loss_train: 0.0074 acc_train: 1.0000 loss_val: 1.8550 acc_val: 0.7700 time: 0.8790s\n",
      "Epoch: 0121 loss_train: 0.0079 acc_train: 1.0000 loss_val: 1.8285 acc_val: 0.7733 time: 0.8623s\n",
      "Epoch: 0122 loss_train: 0.0083 acc_train: 1.0000 loss_val: 1.8201 acc_val: 0.7767 time: 0.8553s\n",
      "Epoch: 0123 loss_train: 0.0085 acc_train: 1.0000 loss_val: 1.8028 acc_val: 0.7767 time: 0.8752s\n",
      "Epoch: 0124 loss_train: 0.0090 acc_train: 1.0000 loss_val: 1.7714 acc_val: 0.7733 time: 0.7827s\n",
      "Epoch: 0125 loss_train: 0.0096 acc_train: 1.0000 loss_val: 1.7588 acc_val: 0.7867 time: 0.8385s\n",
      "Epoch: 0126 loss_train: 0.0100 acc_train: 1.0000 loss_val: 1.7475 acc_val: 0.7833 time: 0.8433s\n",
      "Epoch: 0127 loss_train: 0.0104 acc_train: 1.0000 loss_val: 1.7158 acc_val: 0.7800 time: 0.9846s\n",
      "Epoch: 0128 loss_train: 0.0108 acc_train: 1.0000 loss_val: 1.7006 acc_val: 0.7833 time: 0.7679s\n",
      "Epoch: 0129 loss_train: 0.0112 acc_train: 1.0000 loss_val: 1.6902 acc_val: 0.7833 time: 0.8421s\n",
      "Epoch: 0130 loss_train: 0.0115 acc_train: 1.0000 loss_val: 1.6586 acc_val: 0.7800 time: 0.8016s\n",
      "Epoch: 0131 loss_train: 0.0119 acc_train: 1.0000 loss_val: 1.6597 acc_val: 0.7833 time: 0.8483s\n",
      "Epoch: 0132 loss_train: 0.0118 acc_train: 1.0000 loss_val: 1.6538 acc_val: 0.7900 time: 0.9514s\n",
      "Epoch: 0133 loss_train: 0.0120 acc_train: 1.0000 loss_val: 1.6312 acc_val: 0.7767 time: 0.8586s\n",
      "Epoch: 0134 loss_train: 0.0122 acc_train: 1.0000 loss_val: 1.6421 acc_val: 0.7867 time: 0.8917s\n",
      "Epoch: 0135 loss_train: 0.0123 acc_train: 1.0000 loss_val: 1.6190 acc_val: 0.7833 time: 0.7758s\n",
      "Epoch: 0136 loss_train: 0.0124 acc_train: 1.0000 loss_val: 1.6159 acc_val: 0.7867 time: 0.9430s\n",
      "Epoch: 0137 loss_train: 0.0123 acc_train: 1.0000 loss_val: 1.6348 acc_val: 0.7900 time: 0.8977s\n",
      "Epoch: 0138 loss_train: 0.0131 acc_train: 1.0000 loss_val: 1.5933 acc_val: 0.7900 time: 0.8610s\n",
      "Epoch: 0139 loss_train: 0.0161 acc_train: 1.0000 loss_val: 1.7438 acc_val: 0.7533 time: 0.8434s\n",
      "Epoch: 0140 loss_train: 0.0265 acc_train: 1.0000 loss_val: 1.6978 acc_val: 0.7367 time: 0.8353s\n",
      "Epoch: 0141 loss_train: 0.0737 acc_train: 0.9643 loss_val: 2.0803 acc_val: 0.7433 time: 0.8801s\n",
      "Epoch: 0142 loss_train: 0.1924 acc_train: 0.9143 loss_val: 1.7752 acc_val: 0.7400 time: 0.7641s\n",
      "Epoch: 0143 loss_train: 0.1299 acc_train: 0.9500 loss_val: 1.6639 acc_val: 0.7467 time: 0.9524s\n",
      "Epoch: 0144 loss_train: 0.0906 acc_train: 0.9643 loss_val: 1.5366 acc_val: 0.7633 time: 0.9089s\n",
      "Epoch: 0145 loss_train: 0.0238 acc_train: 1.0000 loss_val: 1.8568 acc_val: 0.7300 time: 0.9060s\n",
      "Epoch: 0146 loss_train: 0.1232 acc_train: 0.9500 loss_val: 1.5788 acc_val: 0.7567 time: 0.8895s\n",
      "Epoch: 0147 loss_train: 0.0513 acc_train: 0.9857 loss_val: 1.4399 acc_val: 0.7733 time: 0.8770s\n",
      "Epoch: 0148 loss_train: 0.0211 acc_train: 1.0000 loss_val: 1.5898 acc_val: 0.7467 time: 0.8913s\n",
      "Epoch: 0149 loss_train: 0.0592 acc_train: 0.9786 loss_val: 1.5722 acc_val: 0.7667 time: 1.0421s\n",
      "Epoch: 0150 loss_train: 0.0484 acc_train: 0.9857 loss_val: 1.5036 acc_val: 0.7800 time: 1.0640s\n",
      "Epoch: 0151 loss_train: 0.0290 acc_train: 1.0000 loss_val: 1.4810 acc_val: 0.7667 time: 0.9533s\n",
      "Epoch: 0152 loss_train: 0.0206 acc_train: 1.0000 loss_val: 1.5638 acc_val: 0.7633 time: 0.9812s\n",
      "Epoch: 0153 loss_train: 0.0279 acc_train: 1.0000 loss_val: 1.6056 acc_val: 0.7533 time: 0.8758s\n",
      "Epoch: 0154 loss_train: 0.0339 acc_train: 1.0000 loss_val: 1.5305 acc_val: 0.7600 time: 0.9500s\n",
      "Epoch: 0155 loss_train: 0.0238 acc_train: 1.0000 loss_val: 1.4803 acc_val: 0.7700 time: 0.9549s\n",
      "Epoch: 0156 loss_train: 0.0210 acc_train: 1.0000 loss_val: 1.4701 acc_val: 0.7767 time: 0.9571s\n",
      "Epoch: 0157 loss_train: 0.0148 acc_train: 1.0000 loss_val: 1.5170 acc_val: 0.7767 time: 0.8011s\n",
      "Epoch: 0158 loss_train: 0.0148 acc_train: 1.0000 loss_val: 1.5736 acc_val: 0.7633 time: 1.1823s\n",
      "Epoch: 0159 loss_train: 0.0196 acc_train: 1.0000 loss_val: 1.5688 acc_val: 0.7667 time: 1.2818s\n",
      "Epoch: 0160 loss_train: 0.0171 acc_train: 1.0000 loss_val: 1.5485 acc_val: 0.7767 time: 1.3268s\n",
      "Epoch: 0161 loss_train: 0.0140 acc_train: 1.0000 loss_val: 1.5501 acc_val: 0.7800 time: 0.8987s\n",
      "Epoch: 0162 loss_train: 0.0129 acc_train: 1.0000 loss_val: 1.5698 acc_val: 0.7800 time: 1.1486s\n",
      "Epoch: 0163 loss_train: 0.0105 acc_train: 1.0000 loss_val: 1.6224 acc_val: 0.7633 time: 0.9541s\n",
      "Epoch: 0164 loss_train: 0.0118 acc_train: 1.0000 loss_val: 1.6358 acc_val: 0.7567 time: 0.9343s\n",
      "Epoch: 0165 loss_train: 0.0124 acc_train: 1.0000 loss_val: 1.6050 acc_val: 0.7700 time: 1.0615s\n",
      "Epoch: 0166 loss_train: 0.0105 acc_train: 1.0000 loss_val: 1.5777 acc_val: 0.7767 time: 1.0321s\n",
      "Epoch: 0167 loss_train: 0.0100 acc_train: 1.0000 loss_val: 1.5619 acc_val: 0.7733 time: 1.2163s\n",
      "Epoch: 0168 loss_train: 0.0096 acc_train: 1.0000 loss_val: 1.5584 acc_val: 0.7867 time: 1.2371s\n",
      "Epoch: 0169 loss_train: 0.0089 acc_train: 1.0000 loss_val: 1.5679 acc_val: 0.7767 time: 0.9092s\n",
      "Epoch: 0170 loss_train: 0.0093 acc_train: 1.0000 loss_val: 1.5743 acc_val: 0.7767 time: 1.2798s\n",
      "Epoch: 0171 loss_train: 0.0101 acc_train: 1.0000 loss_val: 1.5671 acc_val: 0.7833 time: 1.2606s\n",
      "Epoch: 0172 loss_train: 0.0099 acc_train: 1.0000 loss_val: 1.5588 acc_val: 0.7833 time: 0.9154s\n",
      "Epoch: 0173 loss_train: 0.0097 acc_train: 1.0000 loss_val: 1.5619 acc_val: 0.7833 time: 1.2319s\n",
      "Epoch: 0174 loss_train: 0.0099 acc_train: 1.0000 loss_val: 1.5758 acc_val: 0.7833 time: 0.9606s\n",
      "Epoch: 0175 loss_train: 0.0098 acc_train: 1.0000 loss_val: 1.5957 acc_val: 0.7800 time: 1.1535s\n",
      "Epoch: 0176 loss_train: 0.0102 acc_train: 1.0000 loss_val: 1.6007 acc_val: 0.7767 time: 1.1834s\n",
      "Epoch: 0177 loss_train: 0.0107 acc_train: 1.0000 loss_val: 1.5854 acc_val: 0.7800 time: 0.9189s\n",
      "Epoch: 0178 loss_train: 0.0104 acc_train: 1.0000 loss_val: 1.5653 acc_val: 0.7867 time: 1.2281s\n",
      "Epoch: 0179 loss_train: 0.0105 acc_train: 1.0000 loss_val: 1.5523 acc_val: 0.7867 time: 1.1135s\n",
      "Epoch: 0180 loss_train: 0.0108 acc_train: 1.0000 loss_val: 1.5466 acc_val: 0.7800 time: 0.8453s\n",
      "Epoch: 0181 loss_train: 0.0109 acc_train: 1.0000 loss_val: 1.5463 acc_val: 0.7833 time: 1.0897s\n",
      "Epoch: 0182 loss_train: 0.0112 acc_train: 1.0000 loss_val: 1.5436 acc_val: 0.7800 time: 1.0462s\n",
      "Epoch: 0183 loss_train: 0.0113 acc_train: 1.0000 loss_val: 1.5369 acc_val: 0.7833 time: 0.8208s\n",
      "Epoch: 0184 loss_train: 0.0113 acc_train: 1.0000 loss_val: 1.5320 acc_val: 0.7800 time: 1.4154s\n",
      "Epoch: 0185 loss_train: 0.0116 acc_train: 1.0000 loss_val: 1.5315 acc_val: 0.7800 time: 0.9287s\n",
      "Epoch: 0186 loss_train: 0.0118 acc_train: 1.0000 loss_val: 1.5329 acc_val: 0.7800 time: 0.8790s\n",
      "Epoch: 0187 loss_train: 0.0118 acc_train: 1.0000 loss_val: 1.5303 acc_val: 0.7767 time: 0.8901s\n",
      "Epoch: 0188 loss_train: 0.0119 acc_train: 1.0000 loss_val: 1.5234 acc_val: 0.7833 time: 0.9633s\n",
      "Epoch: 0189 loss_train: 0.0120 acc_train: 1.0000 loss_val: 1.5161 acc_val: 0.7833 time: 0.7883s\n",
      "Epoch: 0190 loss_train: 0.0121 acc_train: 1.0000 loss_val: 1.5134 acc_val: 0.7867 time: 0.8279s\n",
      "Epoch: 0191 loss_train: 0.0122 acc_train: 1.0000 loss_val: 1.5154 acc_val: 0.7800 time: 0.8591s\n",
      "Epoch: 0192 loss_train: 0.0122 acc_train: 1.0000 loss_val: 1.5139 acc_val: 0.7800 time: 0.8367s\n",
      "Epoch: 0193 loss_train: 0.0123 acc_train: 1.0000 loss_val: 1.5042 acc_val: 0.7800 time: 0.7832s\n",
      "Epoch: 0194 loss_train: 0.0123 acc_train: 1.0000 loss_val: 1.5017 acc_val: 0.7800 time: 0.8185s\n",
      "Epoch: 0195 loss_train: 0.0124 acc_train: 1.0000 loss_val: 1.5098 acc_val: 0.7833 time: 0.8952s\n",
      "Epoch: 0196 loss_train: 0.0124 acc_train: 1.0000 loss_val: 1.5022 acc_val: 0.7800 time: 0.8598s\n",
      "Epoch: 0197 loss_train: 0.0124 acc_train: 1.0000 loss_val: 1.5026 acc_val: 0.7800 time: 0.7895s\n",
      "Epoch: 0198 loss_train: 0.0124 acc_train: 1.0000 loss_val: 1.5087 acc_val: 0.7800 time: 0.8204s\n",
      "Epoch: 0199 loss_train: 0.0124 acc_train: 1.0000 loss_val: 1.5021 acc_val: 0.7833 time: 0.8685s\n",
      "Epoch: 0200 loss_train: 0.0124 acc_train: 1.0000 loss_val: 1.5031 acc_val: 0.7833 time: 0.8177s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 177.9773s\n",
      "Test set results: loss= 1.1974 accuracy= 0.7650\n",
      "inference time:  0.24875998497009277\n"
     ]
    }
   ],
   "source": [
    "run_experiment(num_epochs=num_epochs, model=model2, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = ite_GCN(nfeat=features.shape[1],\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout,\n",
    "            nite = 3,\n",
    "            allow_grad=False,\n",
    "            smooth_fac=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(num_epochs=num_epochs, model=model3, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['gc.weight', 'gc.bias', 'linear_no_bias.weight', 'linear_no_bias.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(model2.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a model with the same weights as model2, to try inference with more iterations\n",
    "model22 = ite_GCN(nfeat=features.shape[1],\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0,\n",
    "            train_nite= 3,\n",
    "            eval_nite= 500,\n",
    "            allow_grad=True,\n",
    "            smooth_fac=smooth_fac)\n",
    "model22.load_state_dict(model2.state_dict().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= nan accuracy= 0.1180\n",
      "inference time:  37.327991008758545\n"
     ]
    }
   ],
   "source": [
    "test(model22, features, adj, idx_test, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iterativeENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
