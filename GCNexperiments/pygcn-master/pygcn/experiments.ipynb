{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import load_data, accuracy, run_experiment\n",
    "from models import GCN_2, GCN_3, ite_GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(path=\"../data/cora/\", dataset=\"cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 16\n",
    "dropout = 0.5\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "num_epochs = 200\n",
    "smooth_fac = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intialize a 2-layer GCN\n",
      "runrunrun!\n",
      "Epoch: 0001 loss_train: 1.9845 acc_train: 0.1214 loss_val: 1.9948 acc_val: 0.1133 time: 0.0464s\n",
      "Epoch: 0002 loss_train: 1.9745 acc_train: 0.1214 loss_val: 1.9846 acc_val: 0.1133 time: 0.0075s\n",
      "Epoch: 0003 loss_train: 1.9643 acc_train: 0.1214 loss_val: 1.9744 acc_val: 0.1133 time: 0.0083s\n",
      "Epoch: 0004 loss_train: 1.9572 acc_train: 0.1214 loss_val: 1.9641 acc_val: 0.1133 time: 0.0078s\n",
      "Epoch: 0005 loss_train: 1.9449 acc_train: 0.1214 loss_val: 1.9536 acc_val: 0.1133 time: 0.0080s\n",
      "Epoch: 0006 loss_train: 1.9374 acc_train: 0.1214 loss_val: 1.9430 acc_val: 0.1133 time: 0.0072s\n",
      "Epoch: 0007 loss_train: 1.9219 acc_train: 0.1429 loss_val: 1.9321 acc_val: 0.1133 time: 0.0078s\n",
      "Epoch: 0008 loss_train: 1.9134 acc_train: 0.1643 loss_val: 1.9209 acc_val: 0.1167 time: 0.0089s\n",
      "Epoch: 0009 loss_train: 1.9045 acc_train: 0.1786 loss_val: 1.9093 acc_val: 0.1633 time: 0.0081s\n",
      "Epoch: 0010 loss_train: 1.8997 acc_train: 0.1714 loss_val: 1.8976 acc_val: 0.2233 time: 0.0076s\n",
      "Epoch: 0011 loss_train: 1.8808 acc_train: 0.2500 loss_val: 1.8857 acc_val: 0.2367 time: 0.0070s\n",
      "Epoch: 0012 loss_train: 1.8686 acc_train: 0.3357 loss_val: 1.8736 acc_val: 0.4700 time: 0.0091s\n",
      "Epoch: 0013 loss_train: 1.8684 acc_train: 0.2929 loss_val: 1.8614 acc_val: 0.5400 time: 0.0066s\n",
      "Epoch: 0014 loss_train: 1.8361 acc_train: 0.5071 loss_val: 1.8491 acc_val: 0.4200 time: 0.0073s\n",
      "Epoch: 0015 loss_train: 1.8224 acc_train: 0.5286 loss_val: 1.8365 acc_val: 0.3933 time: 0.0077s\n",
      "Epoch: 0016 loss_train: 1.8261 acc_train: 0.3714 loss_val: 1.8240 acc_val: 0.3667 time: 0.0083s\n",
      "Epoch: 0017 loss_train: 1.7995 acc_train: 0.3857 loss_val: 1.8116 acc_val: 0.3633 time: 0.0070s\n",
      "Epoch: 0018 loss_train: 1.7839 acc_train: 0.4357 loss_val: 1.7993 acc_val: 0.3633 time: 0.0082s\n",
      "Epoch: 0019 loss_train: 1.7851 acc_train: 0.3143 loss_val: 1.7875 acc_val: 0.3600 time: 0.0068s\n",
      "Epoch: 0020 loss_train: 1.7914 acc_train: 0.3143 loss_val: 1.7761 acc_val: 0.3533 time: 0.0071s\n",
      "Epoch: 0021 loss_train: 1.7722 acc_train: 0.3286 loss_val: 1.7653 acc_val: 0.3533 time: 0.0085s\n",
      "Epoch: 0022 loss_train: 1.7595 acc_train: 0.3500 loss_val: 1.7548 acc_val: 0.3533 time: 0.0064s\n",
      "Epoch: 0023 loss_train: 1.7520 acc_train: 0.3429 loss_val: 1.7447 acc_val: 0.3533 time: 0.0106s\n",
      "Epoch: 0024 loss_train: 1.7629 acc_train: 0.2929 loss_val: 1.7352 acc_val: 0.3533 time: 0.0084s\n",
      "Epoch: 0025 loss_train: 1.7221 acc_train: 0.3143 loss_val: 1.7262 acc_val: 0.3533 time: 0.0112s\n",
      "Epoch: 0026 loss_train: 1.7546 acc_train: 0.3000 loss_val: 1.7178 acc_val: 0.3533 time: 0.0085s\n",
      "Epoch: 0027 loss_train: 1.7051 acc_train: 0.3214 loss_val: 1.7097 acc_val: 0.3533 time: 0.0077s\n",
      "Epoch: 0028 loss_train: 1.6688 acc_train: 0.3429 loss_val: 1.7017 acc_val: 0.3533 time: 0.0088s\n",
      "Epoch: 0029 loss_train: 1.6777 acc_train: 0.3143 loss_val: 1.6939 acc_val: 0.3533 time: 0.0083s\n",
      "Epoch: 0030 loss_train: 1.7084 acc_train: 0.3357 loss_val: 1.6863 acc_val: 0.3567 time: 0.0079s\n",
      "Epoch: 0031 loss_train: 1.6377 acc_train: 0.3429 loss_val: 1.6788 acc_val: 0.3633 time: 0.0075s\n",
      "Epoch: 0032 loss_train: 1.6527 acc_train: 0.3286 loss_val: 1.6714 acc_val: 0.3633 time: 0.0076s\n",
      "Epoch: 0033 loss_train: 1.6418 acc_train: 0.3857 loss_val: 1.6639 acc_val: 0.3633 time: 0.0078s\n",
      "Epoch: 0034 loss_train: 1.6151 acc_train: 0.3786 loss_val: 1.6563 acc_val: 0.3667 time: 0.0071s\n",
      "Epoch: 0035 loss_train: 1.6126 acc_train: 0.3929 loss_val: 1.6485 acc_val: 0.3667 time: 0.0069s\n",
      "Epoch: 0036 loss_train: 1.6102 acc_train: 0.4143 loss_val: 1.6405 acc_val: 0.3700 time: 0.0079s\n",
      "Epoch: 0037 loss_train: 1.5762 acc_train: 0.4214 loss_val: 1.6322 acc_val: 0.3767 time: 0.0075s\n",
      "Epoch: 0038 loss_train: 1.5963 acc_train: 0.4357 loss_val: 1.6239 acc_val: 0.3800 time: 0.0072s\n",
      "Epoch: 0039 loss_train: 1.5815 acc_train: 0.4500 loss_val: 1.6154 acc_val: 0.3933 time: 0.0068s\n",
      "Epoch: 0040 loss_train: 1.5389 acc_train: 0.4500 loss_val: 1.6068 acc_val: 0.4033 time: 0.0072s\n",
      "Epoch: 0041 loss_train: 1.5459 acc_train: 0.4500 loss_val: 1.5980 acc_val: 0.4133 time: 0.0071s\n",
      "Epoch: 0042 loss_train: 1.5257 acc_train: 0.4429 loss_val: 1.5890 acc_val: 0.4133 time: 0.0067s\n",
      "Epoch: 0043 loss_train: 1.5076 acc_train: 0.4643 loss_val: 1.5793 acc_val: 0.4167 time: 0.0069s\n",
      "Epoch: 0044 loss_train: 1.4744 acc_train: 0.4786 loss_val: 1.5693 acc_val: 0.4200 time: 0.0069s\n",
      "Epoch: 0045 loss_train: 1.5060 acc_train: 0.4643 loss_val: 1.5588 acc_val: 0.4200 time: 0.0071s\n",
      "Epoch: 0046 loss_train: 1.4757 acc_train: 0.5071 loss_val: 1.5478 acc_val: 0.4200 time: 0.0070s\n",
      "Epoch: 0047 loss_train: 1.4618 acc_train: 0.5143 loss_val: 1.5363 acc_val: 0.4233 time: 0.0078s\n",
      "Epoch: 0048 loss_train: 1.4654 acc_train: 0.4714 loss_val: 1.5246 acc_val: 0.4333 time: 0.0075s\n",
      "Epoch: 0049 loss_train: 1.4260 acc_train: 0.5143 loss_val: 1.5128 acc_val: 0.4500 time: 0.0089s\n",
      "Epoch: 0050 loss_train: 1.4215 acc_train: 0.5000 loss_val: 1.5010 acc_val: 0.4500 time: 0.0114s\n",
      "Epoch: 0051 loss_train: 1.4348 acc_train: 0.5071 loss_val: 1.4891 acc_val: 0.4600 time: 0.0101s\n",
      "Epoch: 0052 loss_train: 1.3627 acc_train: 0.5571 loss_val: 1.4770 acc_val: 0.4900 time: 0.0085s\n",
      "Epoch: 0053 loss_train: 1.3597 acc_train: 0.5286 loss_val: 1.4649 acc_val: 0.5100 time: 0.0084s\n",
      "Epoch: 0054 loss_train: 1.3326 acc_train: 0.5643 loss_val: 1.4529 acc_val: 0.5433 time: 0.0066s\n",
      "Epoch: 0055 loss_train: 1.3590 acc_train: 0.5929 loss_val: 1.4407 acc_val: 0.5533 time: 0.0072s\n",
      "Epoch: 0056 loss_train: 1.3218 acc_train: 0.5929 loss_val: 1.4281 acc_val: 0.5567 time: 0.0079s\n",
      "Epoch: 0057 loss_train: 1.3002 acc_train: 0.6143 loss_val: 1.4150 acc_val: 0.5700 time: 0.0082s\n",
      "Epoch: 0058 loss_train: 1.2797 acc_train: 0.6000 loss_val: 1.4015 acc_val: 0.5700 time: 0.0089s\n",
      "Epoch: 0059 loss_train: 1.2725 acc_train: 0.6214 loss_val: 1.3875 acc_val: 0.5767 time: 0.0078s\n",
      "Epoch: 0060 loss_train: 1.2583 acc_train: 0.6357 loss_val: 1.3736 acc_val: 0.5867 time: 0.0091s\n",
      "Epoch: 0061 loss_train: 1.2078 acc_train: 0.6286 loss_val: 1.3598 acc_val: 0.5900 time: 0.0091s\n",
      "Epoch: 0062 loss_train: 1.2297 acc_train: 0.6071 loss_val: 1.3463 acc_val: 0.5967 time: 0.0088s\n",
      "Epoch: 0063 loss_train: 1.2326 acc_train: 0.6214 loss_val: 1.3326 acc_val: 0.6167 time: 0.0081s\n",
      "Epoch: 0064 loss_train: 1.2194 acc_train: 0.6571 loss_val: 1.3190 acc_val: 0.6167 time: 0.0091s\n",
      "Epoch: 0065 loss_train: 1.1656 acc_train: 0.7000 loss_val: 1.3052 acc_val: 0.6167 time: 0.0099s\n",
      "Epoch: 0066 loss_train: 1.1627 acc_train: 0.7071 loss_val: 1.2918 acc_val: 0.6300 time: 0.0087s\n",
      "Epoch: 0067 loss_train: 1.1328 acc_train: 0.6786 loss_val: 1.2786 acc_val: 0.6467 time: 0.0083s\n",
      "Epoch: 0068 loss_train: 1.0987 acc_train: 0.7500 loss_val: 1.2651 acc_val: 0.6467 time: 0.0087s\n",
      "Epoch: 0069 loss_train: 1.0668 acc_train: 0.6857 loss_val: 1.2519 acc_val: 0.6567 time: 0.0088s\n",
      "Epoch: 0070 loss_train: 1.0913 acc_train: 0.7643 loss_val: 1.2388 acc_val: 0.6700 time: 0.0063s\n",
      "Epoch: 0071 loss_train: 1.0634 acc_train: 0.7214 loss_val: 1.2260 acc_val: 0.6967 time: 0.0083s\n",
      "Epoch: 0072 loss_train: 1.0633 acc_train: 0.7571 loss_val: 1.2134 acc_val: 0.7067 time: 0.0069s\n",
      "Epoch: 0073 loss_train: 1.0356 acc_train: 0.7500 loss_val: 1.2013 acc_val: 0.7233 time: 0.0115s\n",
      "Epoch: 0074 loss_train: 1.0435 acc_train: 0.7000 loss_val: 1.1899 acc_val: 0.7333 time: 0.0075s\n",
      "Epoch: 0075 loss_train: 0.9882 acc_train: 0.7429 loss_val: 1.1788 acc_val: 0.7333 time: 0.0086s\n",
      "Epoch: 0076 loss_train: 1.0189 acc_train: 0.7357 loss_val: 1.1677 acc_val: 0.7400 time: 0.0066s\n",
      "Epoch: 0077 loss_train: 1.0183 acc_train: 0.7357 loss_val: 1.1567 acc_val: 0.7400 time: 0.0078s\n",
      "Epoch: 0078 loss_train: 0.9788 acc_train: 0.7786 loss_val: 1.1451 acc_val: 0.7433 time: 0.0068s\n",
      "Epoch: 0079 loss_train: 1.0088 acc_train: 0.7429 loss_val: 1.1332 acc_val: 0.7500 time: 0.0070s\n",
      "Epoch: 0080 loss_train: 0.9708 acc_train: 0.7857 loss_val: 1.1219 acc_val: 0.7533 time: 0.0070s\n",
      "Epoch: 0081 loss_train: 0.9279 acc_train: 0.7857 loss_val: 1.1113 acc_val: 0.7600 time: 0.0073s\n",
      "Epoch: 0082 loss_train: 0.9611 acc_train: 0.7857 loss_val: 1.1013 acc_val: 0.7600 time: 0.0071s\n",
      "Epoch: 0083 loss_train: 0.9365 acc_train: 0.7857 loss_val: 1.0919 acc_val: 0.7733 time: 0.0063s\n",
      "Epoch: 0084 loss_train: 0.9271 acc_train: 0.7929 loss_val: 1.0831 acc_val: 0.7767 time: 0.0071s\n",
      "Epoch: 0085 loss_train: 0.9222 acc_train: 0.8000 loss_val: 1.0747 acc_val: 0.7800 time: 0.0070s\n",
      "Epoch: 0086 loss_train: 0.8915 acc_train: 0.8357 loss_val: 1.0666 acc_val: 0.7900 time: 0.0074s\n",
      "Epoch: 0087 loss_train: 0.8791 acc_train: 0.8143 loss_val: 1.0587 acc_val: 0.7933 time: 0.0075s\n",
      "Epoch: 0088 loss_train: 0.8928 acc_train: 0.8000 loss_val: 1.0511 acc_val: 0.7933 time: 0.0112s\n",
      "Epoch: 0089 loss_train: 0.8573 acc_train: 0.8143 loss_val: 1.0438 acc_val: 0.7933 time: 0.0085s\n",
      "Epoch: 0090 loss_train: 0.8013 acc_train: 0.8714 loss_val: 1.0366 acc_val: 0.7967 time: 0.0086s\n",
      "Epoch: 0091 loss_train: 0.8424 acc_train: 0.8214 loss_val: 1.0293 acc_val: 0.7933 time: 0.0097s\n",
      "Epoch: 0092 loss_train: 0.8544 acc_train: 0.8286 loss_val: 1.0223 acc_val: 0.8000 time: 0.0081s\n",
      "Epoch: 0093 loss_train: 0.8506 acc_train: 0.8357 loss_val: 1.0150 acc_val: 0.8000 time: 0.0081s\n",
      "Epoch: 0094 loss_train: 0.7691 acc_train: 0.8643 loss_val: 1.0075 acc_val: 0.8000 time: 0.0085s\n",
      "Epoch: 0095 loss_train: 0.8052 acc_train: 0.8500 loss_val: 0.9998 acc_val: 0.8033 time: 0.0077s\n",
      "Epoch: 0096 loss_train: 0.7746 acc_train: 0.8500 loss_val: 0.9924 acc_val: 0.8033 time: 0.0081s\n",
      "Epoch: 0097 loss_train: 0.8386 acc_train: 0.8143 loss_val: 0.9851 acc_val: 0.8033 time: 0.0087s\n",
      "Epoch: 0098 loss_train: 0.8006 acc_train: 0.8286 loss_val: 0.9779 acc_val: 0.8067 time: 0.0096s\n",
      "Epoch: 0099 loss_train: 0.7912 acc_train: 0.8214 loss_val: 0.9715 acc_val: 0.8033 time: 0.0089s\n",
      "Epoch: 0100 loss_train: 0.7185 acc_train: 0.8357 loss_val: 0.9653 acc_val: 0.7933 time: 0.0084s\n",
      "Epoch: 0101 loss_train: 0.7881 acc_train: 0.8143 loss_val: 0.9594 acc_val: 0.8033 time: 0.0077s\n",
      "Epoch: 0102 loss_train: 0.7598 acc_train: 0.8286 loss_val: 0.9539 acc_val: 0.8000 time: 0.0090s\n",
      "Epoch: 0103 loss_train: 0.7310 acc_train: 0.8500 loss_val: 0.9484 acc_val: 0.8000 time: 0.0075s\n",
      "Epoch: 0104 loss_train: 0.7538 acc_train: 0.8714 loss_val: 0.9433 acc_val: 0.7900 time: 0.0092s\n",
      "Epoch: 0105 loss_train: 0.7738 acc_train: 0.8429 loss_val: 0.9385 acc_val: 0.7833 time: 0.0097s\n",
      "Epoch: 0106 loss_train: 0.7400 acc_train: 0.8500 loss_val: 0.9337 acc_val: 0.7867 time: 0.0108s\n",
      "Epoch: 0107 loss_train: 0.7426 acc_train: 0.8643 loss_val: 0.9290 acc_val: 0.7833 time: 0.0113s\n",
      "Epoch: 0108 loss_train: 0.6818 acc_train: 0.8714 loss_val: 0.9243 acc_val: 0.7833 time: 0.0087s\n",
      "Epoch: 0109 loss_train: 0.6775 acc_train: 0.8929 loss_val: 0.9192 acc_val: 0.7833 time: 0.0105s\n",
      "Epoch: 0110 loss_train: 0.6644 acc_train: 0.8857 loss_val: 0.9136 acc_val: 0.7867 time: 0.0094s\n",
      "Epoch: 0111 loss_train: 0.7220 acc_train: 0.8643 loss_val: 0.9085 acc_val: 0.7867 time: 0.0126s\n",
      "Epoch: 0112 loss_train: 0.7192 acc_train: 0.8643 loss_val: 0.9031 acc_val: 0.7800 time: 0.0135s\n",
      "Epoch: 0113 loss_train: 0.6645 acc_train: 0.8857 loss_val: 0.8981 acc_val: 0.7800 time: 0.0115s\n",
      "Epoch: 0114 loss_train: 0.6499 acc_train: 0.8857 loss_val: 0.8933 acc_val: 0.7833 time: 0.0140s\n",
      "Epoch: 0115 loss_train: 0.6753 acc_train: 0.8786 loss_val: 0.8884 acc_val: 0.7833 time: 0.0165s\n",
      "Epoch: 0116 loss_train: 0.6602 acc_train: 0.8714 loss_val: 0.8842 acc_val: 0.7900 time: 0.0152s\n",
      "Epoch: 0117 loss_train: 0.6928 acc_train: 0.8643 loss_val: 0.8799 acc_val: 0.7900 time: 0.0126s\n",
      "Epoch: 0118 loss_train: 0.6971 acc_train: 0.8571 loss_val: 0.8756 acc_val: 0.7900 time: 0.0106s\n",
      "Epoch: 0119 loss_train: 0.6512 acc_train: 0.8857 loss_val: 0.8718 acc_val: 0.7900 time: 0.0103s\n",
      "Epoch: 0120 loss_train: 0.6448 acc_train: 0.8429 loss_val: 0.8691 acc_val: 0.7867 time: 0.0119s\n",
      "Epoch: 0121 loss_train: 0.5840 acc_train: 0.8786 loss_val: 0.8664 acc_val: 0.7933 time: 0.0132s\n",
      "Epoch: 0122 loss_train: 0.6337 acc_train: 0.9000 loss_val: 0.8635 acc_val: 0.7933 time: 0.0114s\n",
      "Epoch: 0123 loss_train: 0.6410 acc_train: 0.8643 loss_val: 0.8598 acc_val: 0.7933 time: 0.0105s\n",
      "Epoch: 0124 loss_train: 0.6274 acc_train: 0.8714 loss_val: 0.8554 acc_val: 0.7933 time: 0.0099s\n",
      "Epoch: 0125 loss_train: 0.6034 acc_train: 0.9143 loss_val: 0.8510 acc_val: 0.7933 time: 0.0099s\n",
      "Epoch: 0126 loss_train: 0.5874 acc_train: 0.9143 loss_val: 0.8459 acc_val: 0.8000 time: 0.0108s\n",
      "Epoch: 0127 loss_train: 0.5712 acc_train: 0.8929 loss_val: 0.8403 acc_val: 0.8000 time: 0.0106s\n",
      "Epoch: 0128 loss_train: 0.6129 acc_train: 0.8857 loss_val: 0.8355 acc_val: 0.8000 time: 0.0101s\n",
      "Epoch: 0129 loss_train: 0.6044 acc_train: 0.8786 loss_val: 0.8312 acc_val: 0.8000 time: 0.0099s\n",
      "Epoch: 0130 loss_train: 0.5979 acc_train: 0.8714 loss_val: 0.8273 acc_val: 0.8000 time: 0.0106s\n",
      "Epoch: 0131 loss_train: 0.5435 acc_train: 0.8929 loss_val: 0.8236 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0132 loss_train: 0.5707 acc_train: 0.9143 loss_val: 0.8204 acc_val: 0.7967 time: 0.0106s\n",
      "Epoch: 0133 loss_train: 0.5758 acc_train: 0.8643 loss_val: 0.8175 acc_val: 0.8000 time: 0.0104s\n",
      "Epoch: 0134 loss_train: 0.5396 acc_train: 0.9143 loss_val: 0.8151 acc_val: 0.7967 time: 0.0112s\n",
      "Epoch: 0135 loss_train: 0.5693 acc_train: 0.9000 loss_val: 0.8121 acc_val: 0.7967 time: 0.0108s\n",
      "Epoch: 0136 loss_train: 0.5809 acc_train: 0.9071 loss_val: 0.8072 acc_val: 0.8033 time: 0.0104s\n",
      "Epoch: 0137 loss_train: 0.5475 acc_train: 0.9214 loss_val: 0.8021 acc_val: 0.8033 time: 0.0106s\n",
      "Epoch: 0138 loss_train: 0.5241 acc_train: 0.9143 loss_val: 0.7978 acc_val: 0.8067 time: 0.0088s\n",
      "Epoch: 0139 loss_train: 0.5613 acc_train: 0.8929 loss_val: 0.7928 acc_val: 0.8067 time: 0.0084s\n",
      "Epoch: 0140 loss_train: 0.5396 acc_train: 0.9071 loss_val: 0.7873 acc_val: 0.8133 time: 0.0088s\n",
      "Epoch: 0141 loss_train: 0.5419 acc_train: 0.9143 loss_val: 0.7833 acc_val: 0.8133 time: 0.0084s\n",
      "Epoch: 0142 loss_train: 0.5070 acc_train: 0.9071 loss_val: 0.7801 acc_val: 0.8167 time: 0.0085s\n",
      "Epoch: 0143 loss_train: 0.5517 acc_train: 0.9143 loss_val: 0.7782 acc_val: 0.8100 time: 0.0076s\n",
      "Epoch: 0144 loss_train: 0.5232 acc_train: 0.8929 loss_val: 0.7776 acc_val: 0.8067 time: 0.0094s\n",
      "Epoch: 0145 loss_train: 0.5193 acc_train: 0.8786 loss_val: 0.7799 acc_val: 0.8000 time: 0.0089s\n",
      "Epoch: 0146 loss_train: 0.5241 acc_train: 0.8714 loss_val: 0.7817 acc_val: 0.7900 time: 0.0098s\n",
      "Epoch: 0147 loss_train: 0.4817 acc_train: 0.9286 loss_val: 0.7823 acc_val: 0.7867 time: 0.0093s\n",
      "Epoch: 0148 loss_train: 0.5158 acc_train: 0.9214 loss_val: 0.7786 acc_val: 0.7867 time: 0.0102s\n",
      "Epoch: 0149 loss_train: 0.5121 acc_train: 0.9286 loss_val: 0.7734 acc_val: 0.7967 time: 0.0089s\n",
      "Epoch: 0150 loss_train: 0.5016 acc_train: 0.8929 loss_val: 0.7672 acc_val: 0.8033 time: 0.0098s\n",
      "Epoch: 0151 loss_train: 0.5344 acc_train: 0.9000 loss_val: 0.7622 acc_val: 0.8033 time: 0.0098s\n",
      "Epoch: 0152 loss_train: 0.4836 acc_train: 0.9143 loss_val: 0.7570 acc_val: 0.8033 time: 0.0085s\n",
      "Epoch: 0153 loss_train: 0.4787 acc_train: 0.9214 loss_val: 0.7536 acc_val: 0.8133 time: 0.0104s\n",
      "Epoch: 0154 loss_train: 0.4506 acc_train: 0.9357 loss_val: 0.7514 acc_val: 0.8133 time: 0.0084s\n",
      "Epoch: 0155 loss_train: 0.4679 acc_train: 0.9214 loss_val: 0.7500 acc_val: 0.8133 time: 0.0102s\n",
      "Epoch: 0156 loss_train: 0.4644 acc_train: 0.9000 loss_val: 0.7497 acc_val: 0.8100 time: 0.0078s\n",
      "Epoch: 0157 loss_train: 0.4980 acc_train: 0.9214 loss_val: 0.7493 acc_val: 0.8133 time: 0.0103s\n",
      "Epoch: 0158 loss_train: 0.5124 acc_train: 0.9071 loss_val: 0.7474 acc_val: 0.8100 time: 0.0085s\n",
      "Epoch: 0159 loss_train: 0.4470 acc_train: 0.9429 loss_val: 0.7456 acc_val: 0.8033 time: 0.0097s\n",
      "Epoch: 0160 loss_train: 0.4802 acc_train: 0.9000 loss_val: 0.7425 acc_val: 0.8100 time: 0.0088s\n",
      "Epoch: 0161 loss_train: 0.5002 acc_train: 0.9071 loss_val: 0.7403 acc_val: 0.8100 time: 0.0075s\n",
      "Epoch: 0162 loss_train: 0.5141 acc_train: 0.9000 loss_val: 0.7382 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0163 loss_train: 0.4449 acc_train: 0.9357 loss_val: 0.7356 acc_val: 0.8200 time: 0.0088s\n",
      "Epoch: 0164 loss_train: 0.4345 acc_train: 0.9357 loss_val: 0.7340 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0165 loss_train: 0.4266 acc_train: 0.9286 loss_val: 0.7329 acc_val: 0.8200 time: 0.0083s\n",
      "Epoch: 0166 loss_train: 0.4636 acc_train: 0.9286 loss_val: 0.7315 acc_val: 0.8200 time: 0.0082s\n",
      "Epoch: 0167 loss_train: 0.5151 acc_train: 0.9143 loss_val: 0.7303 acc_val: 0.8133 time: 0.0085s\n",
      "Epoch: 0168 loss_train: 0.4317 acc_train: 0.9571 loss_val: 0.7287 acc_val: 0.8133 time: 0.0081s\n",
      "Epoch: 0169 loss_train: 0.4197 acc_train: 0.9500 loss_val: 0.7273 acc_val: 0.8133 time: 0.0083s\n",
      "Epoch: 0170 loss_train: 0.4141 acc_train: 0.9500 loss_val: 0.7269 acc_val: 0.8133 time: 0.0085s\n",
      "Epoch: 0171 loss_train: 0.4149 acc_train: 0.9429 loss_val: 0.7270 acc_val: 0.8067 time: 0.0088s\n",
      "Epoch: 0172 loss_train: 0.4678 acc_train: 0.9286 loss_val: 0.7277 acc_val: 0.8033 time: 0.0086s\n",
      "Epoch: 0173 loss_train: 0.4644 acc_train: 0.9429 loss_val: 0.7266 acc_val: 0.8067 time: 0.0082s\n",
      "Epoch: 0174 loss_train: 0.4439 acc_train: 0.9071 loss_val: 0.7242 acc_val: 0.8067 time: 0.0081s\n",
      "Epoch: 0175 loss_train: 0.4494 acc_train: 0.9143 loss_val: 0.7217 acc_val: 0.8067 time: 0.0075s\n",
      "Epoch: 0176 loss_train: 0.4439 acc_train: 0.9429 loss_val: 0.7191 acc_val: 0.8067 time: 0.0077s\n",
      "Epoch: 0177 loss_train: 0.4305 acc_train: 0.9571 loss_val: 0.7171 acc_val: 0.8133 time: 0.0071s\n",
      "Epoch: 0178 loss_train: 0.4611 acc_train: 0.9429 loss_val: 0.7145 acc_val: 0.8133 time: 0.0099s\n",
      "Epoch: 0179 loss_train: 0.4401 acc_train: 0.9143 loss_val: 0.7125 acc_val: 0.8133 time: 0.0097s\n",
      "Epoch: 0180 loss_train: 0.4198 acc_train: 0.9429 loss_val: 0.7111 acc_val: 0.8133 time: 0.0084s\n",
      "Epoch: 0181 loss_train: 0.4037 acc_train: 0.9357 loss_val: 0.7095 acc_val: 0.8133 time: 0.0099s\n",
      "Epoch: 0182 loss_train: 0.4116 acc_train: 0.9357 loss_val: 0.7084 acc_val: 0.8133 time: 0.0107s\n",
      "Epoch: 0183 loss_train: 0.4192 acc_train: 0.9571 loss_val: 0.7096 acc_val: 0.8133 time: 0.0101s\n",
      "Epoch: 0184 loss_train: 0.3859 acc_train: 0.9429 loss_val: 0.7088 acc_val: 0.8100 time: 0.0146s\n",
      "Epoch: 0185 loss_train: 0.4015 acc_train: 0.9429 loss_val: 0.7075 acc_val: 0.8100 time: 0.0090s\n",
      "Epoch: 0186 loss_train: 0.3824 acc_train: 0.9500 loss_val: 0.7070 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0187 loss_train: 0.3588 acc_train: 0.9357 loss_val: 0.7068 acc_val: 0.8100 time: 0.0088s\n",
      "Epoch: 0188 loss_train: 0.4097 acc_train: 0.9071 loss_val: 0.7067 acc_val: 0.8100 time: 0.0114s\n",
      "Epoch: 0189 loss_train: 0.4021 acc_train: 0.9214 loss_val: 0.7057 acc_val: 0.8100 time: 0.0135s\n",
      "Epoch: 0190 loss_train: 0.3915 acc_train: 0.9643 loss_val: 0.7046 acc_val: 0.8100 time: 0.0088s\n",
      "Epoch: 0191 loss_train: 0.4123 acc_train: 0.9429 loss_val: 0.7053 acc_val: 0.8100 time: 0.0146s\n",
      "Epoch: 0192 loss_train: 0.4304 acc_train: 0.9357 loss_val: 0.7060 acc_val: 0.8033 time: 0.0107s\n",
      "Epoch: 0193 loss_train: 0.3790 acc_train: 0.9500 loss_val: 0.7066 acc_val: 0.8067 time: 0.0152s\n",
      "Epoch: 0194 loss_train: 0.4007 acc_train: 0.9500 loss_val: 0.7046 acc_val: 0.8067 time: 0.0079s\n",
      "Epoch: 0195 loss_train: 0.3649 acc_train: 0.9571 loss_val: 0.7017 acc_val: 0.8067 time: 0.0095s\n",
      "Epoch: 0196 loss_train: 0.3823 acc_train: 0.9571 loss_val: 0.7002 acc_val: 0.8067 time: 0.0108s\n",
      "Epoch: 0197 loss_train: 0.3954 acc_train: 0.9357 loss_val: 0.6988 acc_val: 0.8067 time: 0.0146s\n",
      "Epoch: 0198 loss_train: 0.4362 acc_train: 0.9643 loss_val: 0.6978 acc_val: 0.8033 time: 0.0156s\n",
      "Epoch: 0199 loss_train: 0.4072 acc_train: 0.9429 loss_val: 0.6967 acc_val: 0.8033 time: 0.0114s\n",
      "Epoch: 0200 loss_train: 0.4054 acc_train: 0.9429 loss_val: 0.6948 acc_val: 0.8067 time: 0.0116s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.8582s\n",
      "Test set results: loss= 0.7096 accuracy= 0.8370\n",
      "inference time:  0.0023469924926757812\n"
     ]
    }
   ],
   "source": [
    "model0 = GCN_2(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout\n",
    ")\n",
    "run_experiment(num_epochs=num_epochs, model=model0, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intialize a 3-layer GCN\n"
     ]
    }
   ],
   "source": [
    "model1 = GCN_3(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "run_experiment(num_epochs=num_epochs, model=model1, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ite_GCN(nfeat=features.shape[1],\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0,\n",
    "            train_nite= 2,\n",
    "            eval_nite= 0,\n",
    "            allow_grad=True,\n",
    "            smooth_fac=smooth_fac)\n",
    "run_experiment(num_epochs=num_epochs, model=model2, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unspecified or invalid number of iterations for inference. Treat as the same as training iterations.\n",
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n"
     ]
    }
   ],
   "source": [
    "model3 = ite_GCN(nfeat=features.shape[1],\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0,\n",
    "            train_nite= 3,\n",
    "            eval_nite= 0,\n",
    "            allow_grad=True,\n",
    "            smooth_fac=smooth_fac)\n",
    "run_experiment(num_epochs=num_epochs, model=model3, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model3 = ite_GCN(nfeat=features.shape[1],\n",
    "#             nclass=labels.max().item() + 1,\n",
    "#             dropout=dropout,\n",
    "#             nite = 3,\n",
    "#             allow_grad=False,\n",
    "#             smooth_fac=0.3)\n",
    "# run_experiment(num_epochs=num_epochs, model=model3, lr=lr, weight_decay=weight_decay, features=features, adj=adj, idx_train=idx_train, idx_val=idx_val, idx_test=idx_test, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['gc.weight', 'gc.bias', 'linear_no_bias.weight', 'linear_no_bias.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(model2.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize a 1-layer GCN with  3 iterations\n",
      "Gradient flows to all iterations:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a model with the same weights as model2, to try inference with more iterations\n",
    "model22 = ite_GCN(nfeat=features.shape[1],\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0,\n",
    "            train_nite= 3,\n",
    "            eval_nite= 500,\n",
    "            allow_grad=True,\n",
    "            smooth_fac=smooth_fac)\n",
    "model22.load_state_dict(model2.state_dict().copy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iterativeENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
